<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.230,0:00:02.200<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.200,0:00:04.200<br>
各位同學大家好，我們來上課吧！<br>
<br>
0:00:11.110,0:00:15.290<br>
好，那接下來呢，我們來講一下<br>
Recurrent Neural Network<br>
<br>
0:00:15.880,0:00:20.140<br>
那 Recurrent Neural Network呢<br>
它其實也可以做到我們在前一堂課裡面<br>
<br>
0:00:20.180,0:00:25.000<br>
講的 sequence labeling 的 task<br>
那最後呢，我們再來說他們有<br>
<br>
0:00:25.240,0:00:27.240<br>
甚麼樣的不同<br>
<br>
0:00:28.570,0:00:33.520<br>
那我們這邊要舉的例子呢<br>
是 Slot Filling，那我們知道說<br>
<br>
0:00:33.750,0:00:38.300<br>
現在很流行呢，做一些智慧客服之類的東西<br>
<br>
0:00:38.390,0:00:43.230<br>
譬如說，做一些智慧的訂票系統<br>
那這種智慧客服或智慧訂票系統裡面呢<br>
<br>
0:00:43.700,0:00:48.140<br>
你往往需要 Slot Filling 這個技術<br>
Slot Filling 指的是甚麼呢<br>
<br>
0:00:48.760,0:00:53.600<br>
Slot Filling 指的是說<br>
比如說假設有一個人對你的訂票系統說<br>
<br>
0:00:53.860,0:00:58.800<br>
I would like to arrive Taipei on November 2nd<br>
那你的系統要自動知道說<br>
<br>
0:00:59.220,0:01:03.480<br>
你的系統裡面有一些 slot<br>
比如說，在這個<br>
<br>
0:01:03.540,0:01:07.980<br>
訂票系統裡面呢<br>
它會有一個 slot叫做 Destination<br>
<br>
0:01:08.300,0:01:13.280<br>
它會有一個 slot 叫做 Time of Arrival<br>
那你的系統要自動知道說呢<br>
<br>
0:01:13.600,0:01:18.060<br>
這邊的每一個詞彙，它屬於哪一個 slot<br>
<br>
0:01:18.320,0:01:23.270<br>
那你的系統要知道說<br>
Taipei 屬於 Destination 這個 slot<br>
<br>
0:01:23.360,0:01:28.180<br>
那你的系統要知道說<br>
November 2nd 屬於 Time of Arrival 這個 slot<br>
<br>
0:01:28.240,0:01:32.850<br>
那其他的詞彙呢，就不屬於任何 slot 裡面<br>
<br>
0:01:33.200,0:01:37.890<br>
那這個問題要怎麼解呢，其實這個問題<br>
<br>
0:01:37.950,0:01:42.740<br>
你當然也可以用一個 Feedforward 的 <br>
Neural Network 來解。也就是說，我疊一個<br>
<br>
0:01:43.630,0:01:45.630<br>
Feedforward  的 Neural Network<br>
<br>
0:01:45.630,0:01:48.020<br>
然後它的 input 就是一個詞彙<br>
<br>
0:01:48.390,0:01:52.600<br>
比如說，你把 Taipei 變成一個 vector <br>
丟到這個 Neural Network裡面去<br>
<br>
0:01:53.720,0:01:57.540<br>
但是你要把一個詞彙丟到Neural Network 裡面去<br>
你必須要先把它<br>
<br>
0:01:58.000,0:02:02.000<br>
用一個向量 vector 來表示<br>
<br>
0:02:02.000,0:02:04.960<br>
那怎麼把一個詞彙用一個向量來表示呢<br>
<br>
0:02:04.960,0:02:07.310<br>
方法實在太多了，最 naive的方法呢<br>
<br>
0:02:07.370,0:02:11.940<br>
就是 1-of-N encoding<br>
我想這個呢，應該就不需要再細講<br>
<br>
0:02:12.690,0:02:17.330<br>
當然你用 word vector來表示一個詞彙呢<br>
<br>
0:02:17.780,0:02:22.110<br>
也是可以的，那或者是呢，有一些<br>
<br>
0:02:22.180,0:02:27.120<br>
Beyond 1-of-N encoding 的方法，比如說<br>
<br>
0:02:27.380,0:02:32.130<br>
有時候，如果你只是用 1-of-N encoding<br>
來描述一個詞彙的話，你會遇到一些問題<br>
<br>
0:02:32.340,0:02:37.180<br>
因為有很多詞彙，你可能從來都沒有見過<br>
<br>
0:02:37.240,0:02:39.240<br>
所以你會需要在1-of-N encoding 裡面<br>
<br>
0:02:39.240,0:02:42.240<br>
多加一個 dimension，這個 dimension 代表<br>
<br>
0:02:42.340,0:02:47.300<br>
代表 other，然後所有的詞彙如果它不是在我們的<br>
<br>
0:02:47.390,0:02:52.320<br>
辭典裡有的詞彙，就歸類到 other 裡面去<br>
<br>
0:02:52.440,0:02:57.340<br>
比如說， Gandalf 不在我們的 vocabulary 裡面<br>
<br>
0:02:57.740,0:02:59.740<br>
它就歸類到 other<br>
<br>
0:02:59.750,0:03:00.320<br>
或者是<br>
<br>
0:03:02.270,0:03:07.170<br>
Sauron 不在這個 vocabulary 裡面，它就歸類到這個 vector<br>
<br>
0:03:07.710,0:03:12.650<br>
那你也可以用某一個詞彙的字母，來表示<br>
<br>
0:03:13.130,0:03:18.040<br>
它的 vector ，那如果你用某一個詞彙的字母的 n-gram 來表示那個 vector 的話呢<br>
<br>
0:03:18.180,0:03:23.060<br>
你就不會有某一個詞彙不在辭典中的問題<br>
比如說，你有一個詞彙叫做 "apple"<br>
<br>
0:03:23.960,0:03:27.680<br>
那 apple 呢，它裡面有出現 "app"<br>
<br>
0:03:27.800,0:03:31.660<br>
有出現 "ppl", 有出現 "ple"<br>
<br>
0:03:32.060,0:03:37.400<br>
那在這個 vector 裡面，對應到 "app" 的<br>
那個 dimension 就是 1<br>
<br>
0:03:37.400,0:03:42.400<br>
對應到 "ppl" 的 dimension 就是 1<br>
那其他的都是 0，無論如何<br>
<br>
0:03:42.620,0:03:46.420<br>
假設我們可以把一個詞彙表示成一個 vector<br>
<br>
0:03:46.980,0:03:51.400<br>
那就可以把這個 vector 丟到一個 Feedforward 的 Network 裡面去，那在 slot filling 這個 task 裡面<br>
<br>
0:03:51.440,0:03:55.560<br>
你就會希望你的 output 是一個 probability distribution<br>
<br>
0:03:55.880,0:04:00.840<br>
這個 probability distribution <br>
代表說我們現在 input 的這個詞彙<br>
<br>
0:04:00.900,0:04:05.100<br>
屬於 哪一個 slot 的機率，舉例來說<br>
<br>
0:04:05.200,0:04:10.110<br>
Taipei 屬於 destination 的機率，<br>
還有 Taipei 屬於 time of departure 的機率，等等<br>
<br>
0:04:13.260,0:04:18.140<br>
但是呢，光只有這樣是不夠的<br>
Feedforward Network 呢<br>
<br>
0:04:18.180,0:04:23.040<br>
沒有辦法 solve 這個 problem<br>
為甚麼呢？假設現在，有一個使用者說<br>
<br>
0:04:23.160,0:04:26.700<br>
要 arrive Taipei on November 2nd<br>
<br>
0:04:26.700,0:04:29.300<br>
那 arrive 是other , Taipei 是 destination ，on 是 other<br>
<br>
0:04:29.300,0:04:33.160<br>
November 2nd 都是時間，但是如果另外一個<br>
<br>
0:04:33.340,0:04:37.620<br>
使用者說 leave Taipei on November 2nd<br>
那 Taipei 這個時候<br>
<br>
0:04:37.680,0:04:43.400<br>
它應該是 place of departure<br>
它應該是出發地而不是目的地<br>
<br>
0:04:43.480,0:04:47.600<br>
但是對 neural network 來說，input 一樣的東西<br>
<br>
0:04:48.000,0:04:51.700<br>
output 就是一樣的東西，而你 input 台北這個詞彙<br>
<br>
0:04:52.200,0:04:55.180<br>
output 要馬都是 destination 的機率最高<br>
<br>
0:04:55.180,0:04:57.990<br>
要馬就都是 place of departure<br>
<br>
0:04:58.160,0:05:03.120<br>
要馬就都是目的地的機率最高<br>
要馬就都是出發地的機率最高<br>
<br>
0:05:03.290,0:05:07.900<br>
你沒有辦法讓它有時候出發地的機率最高<br>
有時候目的地的機率高<br>
<br>
0:05:09.000,0:05:12.060<br>
那怎麼辦呢？這個時候呢，我們就希望<br>
<br>
0:05:12.060,0:05:16.100<br>
我們的neural network 它是有記憶力的<br>
<br>
0:05:16.500,0:05:18.690<br>
如果今天 neural network 是有記憶力的<br>
<br>
0:05:18.760,0:05:21.980<br>
它記得它在看過這個紅色的台北之前<br>
<br>
0:05:21.980,0:05:25.000<br>
它就已經看過 arrive 這個詞彙<br>
<br>
0:05:25.080,0:05:28.420<br>
它記得它在看過這個綠色的 Taipei 之前<br>
<br>
0:05:28.500,0:05:33.450<br>
它就已經看過 leave 這個詞彙<br>
它就可以根據一段話的上下文<br>
<br>
0:05:33.940,0:05:38.160<br>
產生不同的 output <br>
所以如果我們讓我們的 neural network<br>
<br>
0:05:39.270,0:05:44.230<br>
它是有記憶力的話，它就可以解決input 同樣的詞彙<br>
<br>
0:05:44.350,0:05:49.010<br>
但是 output 必須是不同的這個問題<br>
<br>
0:05:49.330,0:05:51.580<br>
好那這種有記憶力的 neural network 呢<br>
<br>
0:05:51.580,0:05:53.990<br>
就叫做 Recurrent Neural Network<br>
<br>
0:05:54.100,0:05:58.860<br>
它的縮寫，是 RNN<br>
那在 Recurrent Neural Network 裡面呢<br>
<br>
0:06:00.000,0:06:04.790<br>
每一次我們的 hidden layer<br>
<br>
0:06:04.990,0:06:09.690<br>
每一次我們的 hidden layer <br>
裡面的 neuron 產生 output 的時候<br>
<br>
0:06:09.760,0:06:14.730<br>
這個 output 都會被存到 memory 裡面去<br>
<br>
0:06:14.850,0:06:19.800<br>
這邊呢，用藍色的方塊來表示 memory<br>
<br>
0:06:19.860,0:06:24.090<br>
當這些 hidden layer 裡面的 neuron 有 output 的時候<br>
<br>
0:06:25.710,0:06:28.940<br>
它就會被存到這個藍色的方塊裡面去，那下一次<br>
<br>
0:06:29.520,0:06:33.610<br>
如果當有 input 的時候，這個 hidden layer<br>
<br>
0:06:35.210,0:06:39.550<br>
這些 neuron 它不是只會考慮<br>
<br>
0:06:39.670,0:06:43.330<br>
input 的這個 x1 跟 x2，它還會考慮<br>
<br>
0:06:43.400,0:06:45.560<br>
存在這些 memory 裡面的值。<br>
<br>
0:06:45.560,0:06:48.300<br>
對它來說，除了 x1 跟 x2 以外<br>
<br>
0:06:49.100,0:06:51.080<br>
這些存在 memory 裡面的值呢<br>
<br>
0:06:51.080,0:06:56.300<br>
a1、a2 也會影響它的 output<br>
<br>
0:06:58.580,0:07:03.120<br>
那我想直接舉個例子，大家可能會比較清楚<br>
<br>
0:07:03.450,0:07:08.220<br>
假設我們現在圖上的這個 network<br>
它所有的 weight 都是 1<br>
<br>
0:07:08.400,0:07:13.000<br>
然後所有的 neuron 都沒有任何的 bias 值<br>
<br>
0:07:13.170,0:07:17.850<br>
然後假設所有的 activation function 都是 linear<br>
<br>
0:07:17.930,0:07:20.620<br>
這樣可以不要讓計算太複雜<br>
<br>
0:07:20.900,0:07:24.400<br>
那現在假設我們的 input 是一個 sequence<br>
<br>
0:07:24.440,0:07:27.930<br>
我們的 input 是 [11] [11] [22]<br>
<br>
0:07:28.000,0:07:32.160<br>
那我們把[11] [11] [22] 這個 sequence<br>
<br>
0:07:32.810,0:07:35.860<br>
input 到這個  Recurrent Neural Network 裡面去<br>
<br>
0:07:35.860,0:07:37.200<br>
會發生甚麼事呢？<br>
<br>
0:07:37.780,0:07:39.780<br>
那首先呢，在你開始要<br>
<br>
0:07:42.100,0:07:46.120<br>
使用這個 Recurrent Neural Network的時候呢<br>
<br>
0:07:46.120,0:07:49.600<br>
你必須要給 memory 起始值<br>
<br>
0:07:49.680,0:07:52.960<br>
你必須要給 memory<br>
<br>
0:07:53.560,0:07:58.410<br>
在還沒有放進任何東西的時候，<br>
你必須要給它一個初始值，比如說這邊呢<br>
<br>
0:07:58.580,0:08:02.600<br>
我們在假設還沒有放進任何東西之前<br>
memory 裡面的值是 0<br>
<br>
0:08:03.510,0:08:08.490<br>
那現在輸入第一個輸入 [1 1]<br>
<br>
0:08:08.700,0:08:12.280<br>
那接下來會發生甚麼事呢<br>
<br>
0:08:12.280,0:08:13.690<br>
對這個 neuron 來說<br>
<br>
0:08:13.780,0:08:18.680<br>
它除了接到 input 的 [1 1] 以外<br>
<br>
0:08:18.780,0:08:23.420<br>
它還接到 memory 的 [0 0]，<br>
那因為我們說所有的 weight 都是 1<br>
<br>
0:08:23.750,0:08:26.760<br>
所以它的 output  就是 2，<br>
那這個 neuron 的 output 也一樣是 2<br>
<br>
0:08:28.510,0:08:30.510<br>
那接下來呢<br>
<br>
0:08:33.730,0:08:37.940<br>
接下來呢，因為所有的 weight 都是 1<br>
<br>
0:08:38.100,0:08:41.500<br>
所以紅色這兩個 neuron 呢，它們的 output 就是 4<br>
<br>
0:08:41.500,0:08:46.500<br>
所以 input [1 1] 的時候，它的 output 就是 [4 4]<br>
<br>
0:08:48.330,0:08:52.580<br>
接下來呢，Recurrent Neural Network 會把這些<br>
<br>
0:08:52.930,0:08:56.280<br>
綠色的 neuron 他的 output 存到 memory 裡面去<br>
<br>
0:08:56.280,0:08:57.880<br>
所以呢<br>
<br>
0:08:57.950,0:09:02.720<br>
memory 裡面的值就被 update 成 2<br>
這個2呢，會被寫進來；這個2，會被寫進來<br>
<br>
0:09:03.140,0:09:07.940<br>
所以 memory 裡面的值就 update 變成 2，接下來呢<br>
<br>
0:09:07.960,0:09:11.860<br>
再輸入 1 跟 1，這時候綠色的 neuron會有什麼樣的輸出呢？<br>
<br>
0:09:11.860,0:09:13.800<br>
它的輸入有 4 個<br>
<br>
0:09:14.040,0:09:18.900<br>
[1 1] 跟 [2 2]，然後 weight 都是 1，<br>
所以你把 2 + 2 + 1 + 1<br>
<br>
0:09:19.350,0:09:21.500<br>
得到的結果呢，是 6<br>
<br>
0:09:21.500,0:09:22.800<br>
那最後呢<br>
<br>
0:09:23.500,0:09:28.920<br>
紅色 neuron 的輸出就是 6 + 6 是 12<br>
所以當輸入 [1 1] 的時候<br>
<br>
0:09:29.000,0:09:31.340<br>
第二次再輸 [1 1] 的時候，輸出就是 [12 12]<br>
<br>
0:09:31.500,0:09:33.980<br>
所以對 Recurrent Neural Network 來說<br>
<br>
0:09:33.980,0:09:38.440<br>
你就算是輸入一樣的東西，你就算給它一模一樣的 input<br>
<br>
0:09:38.500,0:09:42.780<br>
在這個 case 裡面都是 1 跟 1<br>
<br>
0:09:43.560,0:09:48.320<br>
你就算給它一模一樣的 input<br>
在這個 case 裡面都是 1 跟 1，<br>
它的 output 是有可能會不一樣的<br>
<br>
0:09:48.490,0:09:50.490<br>
因為存在 memory 裡面的值呢，是不一樣的<br>
<br>
0:09:52.530,0:09:57.220<br>
那原來的值呢，在這個綠色的 neuron 的 output 是 6 跟 6<br>
<br>
0:09:57.320,0:10:01.860<br>
那接下來 6 跟 6 呢，就會被存到 memory 裡面去，<br>
就會被存到 memory 裡面去<br>
<br>
0:10:01.990,0:10:04.100<br>
所以 2 就被洗掉，變成 6<br>
<br>
0:10:04.100,0:10:06.700<br>
那接下來呢，我們的 input 是<br>
<br>
0:10:07.040,0:10:11.000<br>
2 跟 2，假設 input 是 [2 2]，這邊每一個綠色的 neuron<br>
<br>
0:10:11.320,0:10:15.360<br>
它考慮的 4 個 input ：2 跟 2 跟 6 跟 6<br>
<br>
0:10:15.360,0:10:19.300<br>
所以 6 + 6 + 2 + 2 是多少呢，得到的值是 16<br>
<br>
0:10:19.300,0:10:22.580<br>
那紅色 neuron 的 output 就是 32<br>
<br>
0:10:22.580,0:10:25.140<br>
所以 input 2 跟 2 的時候呢，output 是 32<br>
<br>
0:10:26.580,0:10:30.460<br>
那今天在做 Recurrent neural network 的時候呢<br>
有一件很重要的事情就是<br>
<br>
0:10:31.390,0:10:36.180<br>
這個 input 的 sequence<br>
<br>
0:10:36.380,0:10:40.540<br>
Recurrent neural network 在<br>
考慮它的時候，並不是 independent<br>
<br>
0:10:41.230,0:10:45.150<br>
今天如果你任意調換 input sequence 的順序<br>
比如說把 2 跟 2 挪到最前面來<br>
<br>
0:10:46.220,0:10:49.140<br>
那 output 呢，是會完全不一樣的<br>
<br>
0:10:49.140,0:10:51.080<br>
所以在 Recurrent neural network 裡面，<br>
<br>
0:10:51.640,0:10:53.980<br>
它會考慮 input 這個 sequence 的 order<br>
<br>
0:10:59.450,0:11:04.270<br>
所以今天如果我們要用 Recurrent neural network 來處理 slot filling 這個問題的話<br>
<br>
0:11:04.530,0:11:09.500<br>
它看起來就像是這樣，有一個使用者說<br>
arrive Taipei on November 2nd<br>
<br>
0:11:09.950,0:11:14.740<br>
那 arrive 就變成一個 vector，丟到 neural network裡面去<br>
<br>
0:11:14.840,0:11:19.520<br>
neural network 的 hidden layer<br>
它的 output 這邊寫成 a1<br>
<br>
0:11:19.870,0:11:24.820<br>
這個 a1 是一排 neuron 的 output，所以它其實是一個 vector<br>
<br>
0:11:24.900,0:11:26.440<br>
然後根據這個 a1，我們產生 y1<br>
<br>
0:11:26.440,0:11:31.400<br>
這個 y1 就是 arrive 屬於哪一個 slot 的機率<br>
<br>
0:11:31.400,0:11:34.500<br>
接下來呢，a1 會被存到 memory 裡面去<br>
<br>
0:11:34.930,0:11:37.700<br>
接下來呢，Taipei 會變成 input<br>
<br>
0:11:39.810,0:11:44.540<br>
那這個 hidden layer 會同時考慮 Taipei 這個 input<br>
<br>
0:11:44.630,0:11:49.300<br>
跟存在 memory 裡面的 a1，得到 a2<br>
<br>
0:11:49.480,0:11:54.350<br>
然後再根據 a2 產生 y2<br>
y2 是 Taipei 屬於哪一個 slot 的機率<br>
<br>
0:11:54.930,0:11:57.180<br>
這個 process 呢，就以此類推<br>
<br>
0:11:57.180,0:11:59.140<br>
我們再把 a2 存到 memory 裡面<br>
<br>
0:11:59.140,0:12:00.680<br>
再把 on 丟進去<br>
<br>
0:12:00.900,0:12:04.920<br>
那 hidden layer 同時考慮 input "on" 這個詞彙的 vector<br>
<br>
0:12:04.980,0:12:09.500<br>
跟存在 memory 裡面的 a2，得到 a3，然後a3 再得到 y3<br>
<br>
0:12:09.680,0:12:14.470<br>
它代表 on 屬於哪一個 slot 的機率，<br>
那這邊要注意的事情是<br>
<br>
0:12:14.540,0:12:19.160<br>
有人看到這個圖就說，這邊有3個 network<br>
<br>
0:12:19.420,0:12:23.300<br>
這個不是3個 network，這個是同一個 network<br>
<br>
0:12:24.460,0:12:28.830<br>
在三個不同的時間點，被使用了3次<br>
<br>
0:12:29.390,0:12:34.350<br>
我這邊呢，特別把同樣的 weight，用同樣的顏色來表示<br>
<br>
0:12:34.730,0:12:38.170<br>
同樣的 weight，就用同樣的顏色來表示，<br>
希望大家看得出來<br>
<br>
0:12:40.920,0:12:45.690<br>
那，所以如果我們有了 memory 以後<br>
<br>
0:12:45.750,0:12:50.610<br>
剛才我們講的輸入同一個詞彙，<br>
我們希望它 output 不同的這個問題<br>
<br>
0:12:50.740,0:12:55.640<br>
就有可能被解決，比如說，<br>
如果同樣是輸入 Taipei 這個詞彙<br>
<br>
0:12:56.060,0:13:00.960<br>
但是因為紅色 Taipei 前面接的是 leave，<br>
綠色 Taipei 前面接的是 arrive<br>
<br>
0:13:01.080,0:13:03.420<br>
因為 leave 跟 arrive 它們的 vector 不一樣<br>
<br>
0:13:03.420,0:13:05.680<br>
所以 hidden layer 的 output 也會不同<br>
<br>
0:13:06.240,0:13:11.100<br>
所以存在 memory 裡面的值呢，也會不同<br>
<br>
0:13:11.200,0:13:16.080<br>
雖然現在 x2 是一模一樣的<br>
但是因為存在 memory 裡面的值不同<br>
<br>
0:13:16.290,0:13:20.800<br>
所以 hidden layer 的 output 也會不一樣<br>
所以最後的 output 也就會不一樣<br>
<br>
0:13:22.530,0:13:27.380<br>
好那這個是 Recurrent Neural Network 的基本觀念<br>
<br>
0:13:29.160,0:13:34.040<br>
當然 Recurrent Neural Network 的架構<br>
你是可以任意設計的<br>
<br>
0:13:34.180,0:13:39.100<br>
比如說，它當然可以是 deep，我們剛才看到<br>
 Recurrent Neural Network 它只有一個 hidden layer<br>
<br>
0:13:39.860,0:13:44.600<br>
當然它可以是 deep 的 Recurrent Neural Network<br>
<br>
0:13:44.890,0:13:46.800<br>
比如說，我們把 x1 丟進去以後<br>
<br>
0:13:46.800,0:13:49.790<br>
它可以通過一個 hidden layer，在通過第二個 hidden layer<br>
<br>
0:13:50.110,0:13:54.780<br>
以此類推，通過很多個 hidden layer 以後，<br>
才得到最後的 output<br>
<br>
0:13:55.130,0:13:59.420<br>
那每一個 hidden layer 的 output <br>
都會被存在 memory 裡面<br>
<br>
0:13:59.510,0:14:04.190<br>
在下一個時間點的時候呢，每一個 hidden layer<br>
<br>
0:14:04.360,0:14:08.010<br>
會再把前一個時間點存的值，再讀出來<br>
<br>
0:14:08.240,0:14:13.200<br>
再把前一個時間點存的值呢，再讀出來，最後得到<br>
<br>
0:14:13.280,0:14:15.280<br>
最後的 output，這個 process 就一直持續下去<br>
<br>
0:14:15.280,0:14:17.670<br>
這個 deep 你要疊幾層呢<br>
<br>
0:14:18.320,0:14:20.320<br>
都是可以的<br>
<br>
0:14:21.400,0:14:26.060<br>
那 Recurrent Neural Network 有不同的變形<br>
我們剛才講的呢<br>
<br>
0:14:26.600,0:14:31.400<br>
叫做 Elman Network，<br>
如果我們今天是把 hidden layer的值<br>
<br>
0:14:31.530,0:14:34.800<br>
存起來，在下一個時間點再讀出來<br>
<br>
0:14:35.200,0:14:39.920<br>
那麼這個叫做 Elman Network，那有另外一種呢<br>
<br>
0:14:40.210,0:14:41.440<br>
叫做 Jordan Network<br>
<br>
0:14:41.440,0:14:46.700<br>
Jordan Network 它存的是整個 network 的 output 的值<br>
<br>
0:14:46.760,0:14:50.050<br>
那它再把 output 的值，在下一個時間點<br>
<br>
0:14:50.520,0:14:55.510<br>
再讀進來，它是把 output 的值存在 memory 裡面<br>
<br>
0:14:55.730,0:14:59.500<br>
那傳說呢，Jordan Network<br>
可以得到比較好的 performance<br>
<br>
0:14:59.560,0:15:00.420<br>
因為<br>
<br>
0:15:01.600,0:15:05.200<br>
這邊的 hidden layer，它是沒有 target 的<br>
<br>
0:15:05.200,0:15:08.920<br>
所以有點難控制說<br>
它學到什麼樣的 hidden 的 information<br>
<br>
0:15:08.920,0:15:11.300<br>
它學到把甚麼東西放到 memory 裡面<br>
<br>
0:15:11.300,0:15:16.280<br>
但是這個 y ，它是有 target 的<br>
所以我們今天可以比較清楚我們<br>
<br>
0:15:16.330,0:15:18.460<br>
放在 memory 裡面的，是甚麼樣的東西<br>
<br>
0:15:21.800,0:15:25.840<br>
這個 Recurrent Neural Network ，<br>
它還可以是雙向的<br>
<br>
0:15:26.470,0:15:29.060<br>
甚麼意思呢<br>
<br>
0:15:29.060,0:15:31.230<br>
我們剛才看到 Recurrent Neural Network<br>
<br>
0:15:31.390,0:15:36.260<br>
你 input 一個句子的話，它就是從句首一直<br>
<br>
0:15:36.440,0:15:40.700<br>
讀到句尾，假設句子裡面的每一個詞彙<br>
我們都用 x^t 來表示它的話<br>
<br>
0:15:41.540,0:15:45.950<br>
它就是先讀 x^t，再讀 x^(t+1)，再讀 x^(t+2)<br>
<br>
0:15:47.440,0:15:52.100<br>
但是，其實它的讀取方向也可以是反過來<br>
<br>
0:15:52.910,0:15:57.860<br>
它可以先讀 x^(t+2)，再讀 x^(t+1)<br>
<br>
0:15:57.930,0:16:00.320<br>
再讀 x^t<br>
<br>
0:16:00.320,0:16:05.600<br>
你可以同時 train 一個正向的 Recurrent Neural Network<br>
<br>
0:16:05.660,0:16:09.500<br>
又同時 train 一個逆向的 Recurrent Neural Network<br>
<br>
0:16:09.660,0:16:12.330<br>
然後把這兩個 Recurrent Neural Network<br>
<br>
0:16:13.900,0:16:17.680<br>
的 hidden layer 拿出來<br>
<br>
0:16:17.780,0:16:21.700<br>
把這兩個 Recurrent Neural Network 的 <br>
hidden layer 拿出來，都接給一個 output layer<br>
<br>
0:16:21.700,0:16:22.900<br>
得到最後的 y<br>
<br>
0:16:23.140,0:16:29.000<br>
所以你把正向的 network，在 input x^t 的時候的 output<br>
<br>
0:16:29.180,0:16:33.180<br>
跟逆向的 network，在 input x^t 的時候的 output<br>
<br>
0:16:33.740,0:16:38.070<br>
都丟給 output layer<br>
<br>
0:16:38.290,0:16:42.500<br>
讓 output layer 產生 y^t<br>
然後產生 y^(t+1) 完，產生 y^(t+2)<br>
<br>
0:16:43.560,0:16:48.380<br>
那用 Bidirectional RNN 的好處呢<br>
<br>
0:16:49.250,0:16:54.080<br>
你的 network 呢，它在產生 output 的時候，它看的範圍<br>
<br>
0:16:54.230,0:16:58.620<br>
是比較廣的，如果今天你只有正向的 network<br>
<br>
0:17:00.700,0:17:04.370<br>
在產生 y^t , 跟 y^(t+1) 的時候<br>
<br>
0:17:04.480,0:17:10.600<br>
你的 network 只看過 x1 一直到 x^(t+1) 的部分<br>
<br>
0:17:10.660,0:17:14.380<br>
但是如果我們今天是 Bidirectional 的 RNN<br>
<br>
0:17:15.210,0:17:19.660<br>
在產生 y^(t+1) 的時候，你的 network 不只是看了<br>
<br>
0:17:19.890,0:17:24.830<br>
x1 到 x^(t+1) 所有的 input，它也看了從句尾<br>
<br>
0:17:25.090,0:17:27.090<br>
一直到 x^(t+1) 的 input<br>
<br>
0:17:27.090,0:17:31.600<br>
那你的 network 等於是看了整個 input 的 sequence<br>
<br>
0:17:31.620,0:17:34.600<br>
假設你今天考慮的是 slot filling 的話<br>
<br>
0:17:34.800,0:17:40.010<br>
你的 network 等於是看了整個 sentence 以後，<br>
才決定每一個詞彙的 slot 應該是甚麼<br>
<br>
0:17:40.880,0:17:45.550<br>
那這樣當然會比只看句子的一半，<br>
有更好的 performance<br>
<br>
0:17:48.060,0:17:52.680<br>
那我們剛才講的 Recurrent Neural Network，其實只是<br>
<br>
0:17:52.770,0:17:57.430<br>
Recurrent Neural Network 一個最 simple 的版本<br>
<br>
0:17:57.780,0:18:02.520<br>
那其實只是一個最 simple 的版本<br>
<br>
0:18:02.740,0:18:06.340<br>
那我們剛才講的 memory 呢，是最單純的<br>
<br>
0:18:06.340,0:18:10.100<br>
就是我們隨時都可以把值存到 memory 裡面去<br>
<br>
0:18:10.180,0:18:14.300<br>
也可以隨時把值從 memory 裡面讀出來<br>
<br>
0:18:14.380,0:18:17.100<br>
但現在比較常用的 memory 呢<br>
<br>
0:18:17.160,0:18:21.220<br>
稱之為 Long Short-term 的 memory<br>
<br>
0:18:22.150,0:18:27.150<br>
這種 Long Short-term 的 memory 呢<br>
它的簡寫是 LSTM<br>
<br>
0:18:27.230,0:18:29.480<br>
這種 Long Short-term 的 memory 呢<br>
它是比較複雜的<br>
<br>
0:18:29.480,0:18:32.060<br>
這個 Long Short-term 的 memory 呢<br>
它有 3 個 gate<br>
<br>
0:18:32.800,0:18:37.590<br>
當外界，當 neural network 的其他部分<br>
<br>
0:18:37.900,0:18:42.130<br>
某個 neuron 的 output 想要被寫到 memory cell 裡面的時候<br>
<br>
0:18:42.770,0:18:47.590<br>
它必須先通過一個閘門，通過一個 input gate<br>
<br>
0:18:47.860,0:18:51.560<br>
那這個 input gate 呢，它要被打開的時候<br>
<br>
0:18:51.560,0:18:55.700<br>
你才能夠把值寫到 memory cell 裡面去<br>
<br>
0:18:55.700,0:18:57.920<br>
如果它被關起來的時候，<br>
<br>
0:18:58.040,0:19:01.920<br>
其他 neuron 就沒有辦法把值寫進去<br>
<br>
0:19:01.920,0:19:05.800<br>
那至於這個 input gate 它是打開還是關起來<br>
<br>
0:19:06.480,0:19:09.100<br>
這個是 neural network 自己學<br>
<br>
0:19:09.160,0:19:12.330<br>
所以它可以自己學說甚麼時候要把 input gate 打開<br>
<br>
0:19:12.450,0:19:15.000<br>
甚麼時候要把 input gate 關起來<br>
<br>
0:19:16.800,0:19:18.400<br>
那輸出的地方呢<br>
<br>
0:19:18.400,0:19:20.900<br>
輸出的地方也有一個 output gate<br>
<br>
0:19:20.900,0:19:22.550<br>
那這個 output gate 會決定說<br>
<br>
0:19:22.870,0:19:27.610<br>
外界、其他的 neuron <br>
可不可以從 memory 裡面把值讀出來<br>
<br>
0:19:28.900,0:19:33.710<br>
那當 output gate 關閉的時候，就沒有辦法把值讀出來<br>
<br>
0:19:33.940,0:19:36.260<br>
只有 output gate 被打開的時候，才可以把值讀出來<br>
<br>
0:19:36.260,0:19:38.440<br>
那跟 input gate 一樣，output gate 甚麼時候是打開<br>
<br>
0:19:38.760,0:19:42.500<br>
甚麼時候是關起來，network 也是自己學到<br>
<br>
0:19:45.720,0:19:50.120<br>
那還有第三個 gate 呢，叫做 forget gate<br>
<br>
0:19:50.280,0:19:54.470<br>
那 forget gate 是決定說，甚麼時候 memory<br>
<br>
0:19:54.970,0:19:58.460<br>
要把過去記得的東西忘掉<br>
<br>
0:19:58.460,0:20:02.800<br>
或是它甚麼時候要把過去記得的東西做一下 format<br>
<br>
0:20:02.900,0:20:04.650<br>
把它 format 掉<br>
<br>
0:20:05.020,0:20:09.820<br>
那這個 forget gate 甚麼時候<br>
會把存在 memory 裡面的值 format 掉<br>
<br>
0:20:10.070,0:20:12.460<br>
甚麼時候會把存在 memory 裡面的值繼續保留下來<br>
<br>
0:20:12.460,0:20:14.830<br>
這個也是 network 自己學到的<br>
<br>
0:20:15.950,0:20:20.790<br>
那整個 LSTM 呢，可以看成它有4個 input<br>
<br>
0:20:21.350,0:20:26.120<br>
1 個 output，這 4 個input 是甚麼呢，一個是<br>
<br>
0:20:26.350,0:20:28.320<br>
想要被存到 memory cell 裡面的值<br>
<br>
0:20:28.320,0:20:33.000<br>
然後它不一定存得進去，這 depend on <br>
input gate 要不要讓這個 information 過去<br>
<br>
0:20:33.100,0:20:35.140<br>
跟操控 input gate 的這個訊號<br>
<br>
0:20:36.250,0:20:39.340<br>
操控 output gate 的這個訊號<br>
和操控 forget gate 的訊號<br>
<br>
0:20:39.340,0:20:41.200<br>
所以一個 LSTM 的 cell 呢<br>
<br>
0:20:41.260,0:20:47.000<br>
它有 4 個 input，那有這 4 個 input<br>
但它只會得到一個 output<br>
<br>
0:20:57.190,0:21:01.770<br>
那這邊有一個小小的冷知識<br>
<br>
0:21:02.200,0:21:07.040<br>
就是這個 dash，你覺得它應該被放在哪裡<br>
<br>
0:21:08.570,0:21:13.430<br>
我投影片把它放在這邊，但並不代表我投影片就是對的<br>
<br>
0:21:13.510,0:21:16.220<br>
我有可能只是突然發現我寫錯了想要改一下這樣子<br>
<br>
0:21:16.220,0:21:18.470<br>
好你覺得這個 dash<br>
<br>
0:21:18.800,0:21:22.360<br>
應該放在 long 和 short 之間的舉手一下<br>
<br>
0:21:22.360,0:21:23.690<br>
沒有<br>
<br>
0:21:23.800,0:21:26.500<br>
那你覺得它應該放在 short 跟 term 之間的同學舉手一下<br>
<br>
0:21:27.670,0:21:31.000<br>
好，手放下，沒錯它應該放在 short 跟 term 之間<br>
<br>
0:21:31.000,0:21:33.540<br>
有時候我會看到有人放在 long 和 short 之間<br>
<br>
0:21:33.540,0:21:37.040<br>
那其實這是比較不 make sense 的<br>
應該放在 short 跟 term 之間<br>
<br>
0:21:37.140,0:21:41.530<br>
因為它其實呢，還是一個 short-term 的 memory<br>
<br>
0:21:42.630,0:21:47.200<br>
它只是比較長的 short-term memory<br>
<br>
0:21:47.480,0:21:51.390<br>
所以按照這個字面意思，<br>
它是比較長的 short-term memory<br>
<br>
0:21:54.820,0:21:57.420<br>
因為之前我們看那個 Recurrent neural network 阿<br>
<br>
0:21:57.420,0:21:59.770<br>
它的 memory 在每一個時間點<br>
<br>
0:21:59.900,0:22:04.650<br>
都會被洗掉<br>
<br>
0:22:04.720,0:22:08.980<br>
只要每次有新的 input 進來，在每一個時間點呢<br>
<br>
0:22:09.090,0:22:13.660<br>
Recurrent neural network 都會把 memory 洗掉<br>
所以這個 short-term 是非常 short<br>
<br>
0:22:14.380,0:22:17.160<br>
它只記得前一個時間點的事情<br>
<br>
0:22:17.160,0:22:19.240<br>
但是如果是 long short-term 的話，它可以記得<br>
<br>
0:22:20.160,0:22:25.150<br>
比較長一點，只要 forget gate不要決定要 format 的話<br>
<br>
0:22:25.430,0:22:27.430<br>
它的值就會被存起來<br>
<br>
0:22:28.520,0:22:32.660<br>
好那這個 memory 的 cell 呢，如果<br>
<br>
0:22:33.550,0:22:38.120<br>
更仔細地來看它的 formulation的話，它長得像這樣<br>
<br>
0:22:38.670,0:22:43.230<br>
這個是外界的 input，<br>
外界要存到 cell 裡面的 input，這個是 input gate<br>
<br>
0:22:43.750,0:22:45.750<br>
這個是 forget gate, 這個是 output gate<br>
<br>
0:22:47.810,0:22:52.700<br>
那我們假設呢，現在讓要被存到 cell 裡面的 input 叫做 z<br>
<br>
0:22:53.440,0:22:56.860<br>
操控 input gate 的 signal 叫做 zi<br>
<br>
0:22:56.860,0:23:01.000<br>
好這個所謂操控 input gate 的 signal，<br>
它也就是一個 scalar<br>
<br>
0:23:01.200,0:23:03.110<br>
也是一個數值<br>
<br>
0:23:03.490,0:23:05.460<br>
那我們等一下會講說這個數值它是從哪裡來的<br>
<br>
0:23:07.000,0:23:08.380<br>
反正這邊就是有一個數值<br>
<br>
0:23:08.550,0:23:13.320<br>
被當作這個 cell 的 input，好那這個 forget gate<br>
<br>
0:23:13.530,0:23:17.780<br>
有一個操控它的數值是 zf，<br>
output gate 有一個操控它的數值是 zo<br>
<br>
0:23:18.460,0:23:23.100<br>
綜合這些東西以後，最後會得到一個 output<br>
<br>
0:23:23.550,0:23:25.000<br>
這邊寫作 a<br>
<br>
0:23:25.400,0:23:28.490<br>
好，假設我們現在 cell 裡面呢<br>
<br>
0:23:29.080,0:23:33.870<br>
在輸入這些，在有這 4 個輸入之前<br>
<br>
0:23:34.370,0:23:36.740<br>
它裡面已經存了值 c<br>
<br>
0:23:36.740,0:23:39.270<br>
那現在呢，假設要輸入<br>
<br>
0:23:39.460,0:23:41.100<br>
輸入的部分呢，輸入z<br>
<br>
0:23:41.100,0:23:44.440<br>
那三個 gate 呢，分別是由zi, zo, zf 所操控的<br>
<br>
0:23:44.660,0:23:46.860<br>
那 output a 會長甚麼樣子呢？<br>
<br>
0:23:46.860,0:23:49.570<br>
我們把 z 呢，通過一個 activation function<br>
<br>
0:23:49.910,0:23:53.860<br>
得到 g(z)，然後把 zi 通過<br>
<br>
0:23:54.220,0:23:57.200<br>
另外一個 activation function，得到 f(zi)<br>
<br>
0:23:57.300,0:23:58.260<br>
那這邊呢<br>
<br>
0:23:59.940,0:24:04.440<br>
這3個 zi, zf, zo 他們通過的這3個 activation function f 阿<br>
<br>
0:24:04.700,0:24:09.560<br>
通常我們會選擇 sigmoid function<br>
那選擇 sigmoid function 它的意義就是<br>
<br>
0:24:09.660,0:24:14.600<br>
sigmoid 的值是介在 0~1 之間的，而這個 0~1 之間的值<br>
<br>
0:24:14.970,0:24:17.140<br>
代表了這個 gate 被打開的程度<br>
<br>
0:24:17.140,0:24:19.760<br>
如果這個 f 的 output<br>
<br>
0:24:20.160,0:24:24.920<br>
這個 activation function 的 output 是 1<br>
代表這個 gate 是處於被打開的狀態<br>
<br>
0:24:24.980,0:24:29.460<br>
反之呢，代表這個 gate 是被關起來的<br>
<br>
0:24:33.800,0:24:38.720<br>
接下來呢，我們就把 g(z) 乘上這個 input gate 的值<br>
<br>
0:24:38.860,0:24:43.440<br>
f(zi) 得到 g(z)*f(zi)<br>
<br>
0:24:44.770,0:24:47.040<br>
那這個 forget gate 的 zf 呢<br>
<br>
0:24:47.040,0:24:50.900<br>
zf 這個 signal 也通過這個 sigmoid activation function<br>
<br>
0:24:51.000,0:24:54.740<br>
得到 f(zf)，接下來呢<br>
<br>
0:24:55.690,0:25:00.550<br>
我們把存在 memory 裡面的值<br>
<br>
0:25:01.000,0:25:05.820<br>
c 乘上 f(zf)<br>
<br>
0:25:05.890,0:25:09.900<br>
得到 c*f(zf)，然後接下來<br>
<br>
0:25:11.190,0:25:16.160<br>
把 c*f(zf) 加上 g(z)*f(zi)<br>
<br>
0:25:16.250,0:25:21.110<br>
把這兩項加起來，得到 c'<br>
<br>
0:25:21.260,0:25:25.610<br>
c' 就是新的存在 memory 裡面的值<br>
<br>
0:25:25.830,0:25:30.530<br>
新的存在 memory 裡面的值就是 c'，所以<br>
<br>
0:25:30.630,0:25:35.270<br>
根據到目前為止的運算可以發現說呢<br>
這個 f(zi) 就是 control 這個 g(z)<br>
<br>
0:25:35.640,0:25:40.580<br>
可不可以輸入的一個關卡，因為假設 f(zi) = 0<br>
<br>
0:25:41.360,0:25:46.050<br>
那 g(z) * f(zi) 就等於 0，就好像是沒有輸入一樣<br>
<br>
0:25:46.150,0:25:50.680<br>
若 f(zi) 是等於 1，那就等於是直接把 g(z) 當作輸入<br>
<br>
0:25:51.780,0:25:56.490<br>
那這個 f(zf)，就是決定說我們要不要<br>
<br>
0:25:57.520,0:26:01.640<br>
把存在 memory 裡面的值洗掉，假設 f(zf) 是  1<br>
<br>
0:26:06.000,0:26:07.650<br>
假設 f(zf) 是 1<br>
<br>
0:26:07.850,0:26:09.260<br>
也就是 forget gate 是被開啟的時候<br>
<br>
0:26:09.260,0:26:12.780<br>
forget gate 被開啟的時候呢，這個時候<br>
<br>
0:26:12.970,0:26:17.860<br>
c 會直接通過，就等於是<br>
<br>
0:26:18.080,0:26:23.050<br>
把之前存的值，還是記得，<br>
那如果是 f(zf) = 0<br>
<br>
0:26:23.150,0:26:28.150<br>
也就是 forget gate 被關閉的時候， 0 乘上 c <br>
過去存在 memory  裡面的值呢，就會變成 0<br>
<br>
0:26:29.640,0:26:34.630<br>
然後把這兩個值呢，加起來當作<br>
<br>
0:26:34.850,0:26:39.830<br>
我們把這兩個值加起來，寫到 memory 裡面得到 c'<br>
<br>
0:26:39.900,0:26:44.480<br>
那 forget gate 它的開關呢，跟我們直覺的想法<br>
<br>
0:26:44.850,0:26:49.720<br>
是相反的，這個 forget gate 它打開的時候代表<br>
<br>
0:26:49.970,0:26:54.920<br>
是記得，它被關閉的時候，代表的是遺忘<br>
<br>
0:26:54.990,0:26:59.690<br>
所以我覺得它的名字取的有點怪，<br>
或許不該叫它 forget gate<br>
<br>
0:26:59.880,0:27:03.940<br>
不過反正，習慣上呢，就是這麼做<br>
<br>
0:27:05.010,0:27:09.760<br>
好那把這個 c' 通過 h，得到 h(c')<br>
<br>
0:27:10.090,0:27:13.440<br>
然後接下來呢，這邊有一個 output gate<br>
<br>
0:27:13.440,0:27:15.800<br>
這個 output gate 受 zo 所操控<br>
<br>
0:27:15.820,0:27:19.560<br>
zo 通過 f 得到 f(zo)<br>
<br>
0:27:19.760,0:27:24.580<br>
f(zo) 如果是 1 的話，那這邊我們會把 f(zo)<br>
<br>
0:27:24.640,0:27:29.300<br>
跟這個 h(c') 乘起來，如果 f(zo)是 1，就等於是<br>
<br>
0:27:29.360,0:27:33.370<br>
h(c') 可以通過這個 output gate，如果 f(zo) 是 0<br>
<br>
0:27:34.320,0:27:39.180<br>
就等於這個 output 會變成0<br>
就代表存在 memory 的值呢<br>
<br>
0:27:39.290,0:27:44.020<br>
沒有辦法通過 output gate，被讀取出來<br>
<br>
0:27:44.220,0:27:48.850<br>
也許這樣你還是沒有很清楚，<br>
所以後面呢，我打算做一個韌體 LSTM<br>
<br>
0:27:49.500,0:27:54.100<br>
我從來沒有在其他地方看過韌體 LSTM<br>
<br>
0:27:54.100,0:27:56.920<br>
所以你可以想到我這個投影片是做很久<br>
<br>
0:27:59.940,0:28:02.900<br>
那我們先講一下我們要舉的例子<br>
<br>
0:28:02.900,0:28:04.600<br>
等一下我們要舉的例子是這樣子<br>
<br>
0:28:04.940,0:28:09.630<br>
在 network 裡面，只有一個 LSTM 的 cell<br>
<br>
0:28:10.160,0:28:12.640<br>
那我們的 input 都是三維的 vector<br>
<br>
0:28:12.640,0:28:16.200<br>
output 都是一維的 vector<br>
<br>
0:28:19.390,0:28:24.110<br>
那這個三維的 vector，它跟 output 還有 <br>
memory 裡面的值的關係是甚麼呢<br>
<br>
0:28:24.580,0:28:26.060<br>
這個關係是這樣子<br>
<br>
0:28:28.000,0:28:29.190<br>
假設<br>
<br>
0:28:29.280,0:28:33.010<br>
第二個 dimension x2 的值是 1 的時候<br>
<br>
0:28:33.680,0:28:37.160<br>
x1 的值就會被寫到 memory 裡面去<br>
<br>
0:28:37.160,0:28:41.200<br>
那x2 是1的時候，x1的值就會被存到 memory 裡面去<br>
<br>
0:28:41.300,0:28:44.070<br>
假設 x2 的值是 -1 的時候<br>
<br>
0:28:44.130,0:28:46.760<br>
memory 就會被 reset<br>
<br>
0:28:48.290,0:28:51.080<br>
memory 存的值就會被遺忘<br>
<br>
0:28:51.080,0:28:53.230<br>
假設 x3 等於 1 的時候<br>
<br>
0:28:53.290,0:28:57.200<br>
你才會把 output gate 打開，才能夠看到輸出<br>
<br>
0:28:58.000,0:28:59.000<br>
所以呢<br>
<br>
0:29:01.500,0:29:04.420<br>
假設我們原來存在 memory 裡面的值是 0<br>
<br>
0:29:05.340,0:29:09.670<br>
那當這邊是1的時候<br>
<br>
0:29:10.020,0:29:14.780<br>
當 x2 = 1 的時候，3會被存到 memory 裡面去<br>
<br>
0:29:15.020,0:29:16.520<br>
這裡得到的值呢，就變成 3<br>
<br>
0:29:16.520,0:29:19.860<br>
那這邊又出現一次 1<br>
<br>
0:29:19.940,0:29:24.160<br>
所以 4 會被存到 memory 裡面去，所以就得到 7<br>
<br>
0:29:25.390,0:29:30.100<br>
x3 = 1，所以 7 會被輸出<br>
<br>
0:29:30.220,0:29:35.030<br>
所以得到 7，那這邊是 -1，如果是 -1 的話呢<br>
<br>
0:29:35.250,0:29:40.200<br>
就會把 memory 裡面的值洗掉<br>
所以看到 -1，下一個時間點的值就變成 0<br>
<br>
0:29:40.310,0:29:44.890<br>
然後看到 1 就會把 6 存進去，所以得到的值是 6<br>
<br>
0:29:44.950,0:29:48.290<br>
這邊 1呢是輸出，所以得到的值是 6<br>
<br>
0:29:49.670,0:29:54.490<br>
那我們就來實際做一下運算<br>
<br>
0:29:54.950,0:29:59.050<br>
那這個是一個 memory cell，一個 LSTM 的 memory cell<br>
<br>
0:29:59.980,0:30:04.320<br>
那我們知道 LSTM 的 memory cell 呢，總共有 4 個 input<br>
<br>
0:30:04.710,0:30:06.720<br>
這 4 個 input 都是 scalar<br>
<br>
0:30:06.720,0:30:09.480<br>
這 4 個 input 的 scalar 是怎麼來的呢<br>
<br>
0:30:09.780,0:30:14.600<br>
這 4 個 scalar是我們 input 的三維的 vector 乘上一個<br>
<br>
0:30:16.690,0:30:20.600<br>
linear 的 transform 以後，所得到的結果<br>
<br>
0:30:20.600,0:30:23.600<br>
你就把這 3 個 vector 乘上這 3 個值<br>
<br>
0:30:23.740,0:30:28.460<br>
再加上 bias，就得到這邊的 input，這三個值<br>
<br>
0:30:28.960,0:30:33.650<br>
再乘上三個 weight，再加上 bias，就得到它的 input<br>
<br>
0:30:34.040,0:30:36.040<br>
以此類推<br>
<br>
0:30:38.920,0:30:43.840<br>
那這些值，就是 input 的 x1, x2, x3<br>
<br>
0:30:43.930,0:30:48.760<br>
要乘上哪些值，還有 bias 的值應該要是多少這件事情呢<br>
<br>
0:30:48.980,0:30:53.930<br>
是透過 training data，透過 gradient descent 去學到的<br>
<br>
0:30:54.380,0:30:59.300<br>
那我們今天只是假設說，我已經知道這些值是多少<br>
<br>
0:30:59.480,0:31:04.270<br>
然後我現在用這樣的輸入，它會得到怎麼樣的輸出<br>
<br>
0:31:04.380,0:31:08.830<br>
那我們就來實際地運算一下，<br>
不過在實際運算之前<br>
<br>
0:31:09.200,0:31:13.830<br>
我們先根據它的 input，根據這些參數呢<br>
<br>
0:31:13.920,0:31:18.000<br>
來分析一下我們可能會得到的結果，<br>
那你看，在這個地方<br>
<br>
0:31:18.900,0:31:23.620<br>
x1 乘 1，其他都是乘 0，所以呢<br>
<br>
0:31:23.680,0:31:28.210<br>
這邊呢，就是直接把 x1 當作輸入<br>
好，那我們看 input gate 的地方<br>
<br>
0:31:28.320,0:31:32.330<br>
它是 x2 * 100<br>
<br>
0:31:33.760,0:31:38.720<br>
bias 是 -10，也就是說呢，假設 x2 沒有值的時候<br>
<br>
0:31:38.810,0:31:43.130<br>
因為 bias 是 -10，所以通常input gate 呢，是被關閉的<br>
<br>
0:31:44.300,0:31:48.880<br>
如果 bias 是 -10的話，那通過 activation function 以後呢<br>
<br>
0:31:49.300,0:31:54.040<br>
通過 sigmoid 的 activation function 之後<br>
它的值會接近 0<br>
<br>
0:31:54.220,0:31:56.220<br>
所以呢，代表它是被關閉的<br>
<br>
0:31:57.840,0:32:02.640<br>
那只有在 x2 有值的時候，如果 x2 有值<br>
它就會比 bias 的這個 -10 還要大<br>
<br>
0:32:03.140,0:32:08.060<br>
如果 x2 代 1的話呢，它就會比 bias大<br>
這個時候呢，input 就會是很大的正值<br>
<br>
0:32:08.210,0:32:10.540<br>
代表 input gate 被打開<br>
<br>
0:32:10.540,0:32:13.110<br>
那 forget gate 呢<br>
<br>
0:32:13.190,0:32:18.080<br>
forget gate 平常都是被打開的，你會發現說，因為它 bias 是 10<br>
<br>
0:32:18.370,0:32:22.990<br>
所以平常呢，它都是被打開的<br>
所以平常都會一直記得東西，只有在 x2<br>
<br>
0:32:23.270,0:32:27.560<br>
給它一個很大的負值的時候，它會壓過這個 bias<br>
才會把 forget gate 關起來<br>
<br>
0:32:27.560,0:32:29.000<br>
那 output gate 呢<br>
<br>
0:32:30.940,0:32:34.980<br>
output gate 平常也都是被關閉的，<br>
因為它的 bias 是很大的負值<br>
<br>
0:32:34.980,0:32:37.200<br>
但是如果今天 x3 有一個很大的正值的話<br>
<br>
0:32:37.840,0:32:40.000<br>
它就可以壓過 bias，把 output gate 打開<br>
<br>
0:32:40.440,0:32:44.700<br>
所以我們就實際地來 input 一下看看<br>
<br>
0:32:44.850,0:32:49.600<br>
我們假設 g 跟 h 都是 linear 的，這樣計算比較方便<br>
<br>
0:32:50.670,0:32:54.890<br>
假設存在 memory 裡面的初始值是 0<br>
<br>
0:32:55.130,0:32:59.680<br>
好那我們現在 input 第一個 vector [3 1 0]<br>
<br>
0:32:59.940,0:33:05.180<br>
那 input [3 1 0] 會發生甚麼事呢，<br>
3 乘上 1，所以這邊進來的值是 3<br>
<br>
0:33:05.180,0:33:11.440<br>
然後 1 乘 100 減 100，所以這邊的 input gate 約等於 1<br>
<br>
0:33:11.440,0:33:13.820<br>
所以它是被打開的，那 forget gate 呢<br>
<br>
0:33:14.440,0:33:19.440<br>
1*3，通過 input gate 以後得到的值是 3<br>
<br>
0:33:19.520,0:33:24.180<br>
那 forget gate 呢，input 是 [3 1 0]，forget gate 呢<br>
<br>
0:33:25.070,0:33:29.040<br>
是被打開的，所以 forget gate 是被打開的<br>
<br>
0:33:29.130,0:33:33.890<br>
把 0 乘上 1 加上 3，所以 forget gate 是被打開的<br>
不過裡面本來就沒有存值<br>
<br>
0:33:34.020,0:33:38.810<br>
也沒有甚麼影響，0 * 1 + 3<br>
所以存在 memory 裡面的值變成 3<br>
<br>
0:33:39.240,0:33:42.990<br>
然後接下來呢，看 output gate，[3 1 0]<br>
<br>
0:33:44.240,0:33:49.020<br>
output gate 還是被關起來的，3無法通過，所以輸出就是 0<br>
<br>
0:33:49.300,0:33:51.300<br>
好，接下來呢 input<br>
<br>
0:33:53.300,0:33:58.240<br>
input [4 1 0]，這個 input 的地方還是 4<br>
<br>
0:33:58.480,0:34:03.320<br>
然後這個 [4 1 0] 會把 input gate 打開<br>
<br>
0:34:03.380,0:34:08.360<br>
forget gate 也會被打開<br>
<br>
0:34:08.450,0:34:13.350<br>
forget gate 被打開的關係，所以 3 * 1 + 4<br>
所以 memory 裡面存的值會變成 7<br>
<br>
0:34:13.790,0:34:18.720<br>
那 output 仍然是被關閉的<br>
<br>
0:34:18.960,0:34:23.600<br>
所以 7 呢，仍然無法被輸出，<br>
所以整個 memory 的輸出仍然是 0<br>
<br>
0:34:26.950,0:34:30.660<br>
那接下來呢，input [2 0 0]，會發生甚麼事呢？<br>
<br>
0:34:30.940,0:34:35.300<br>
input [2 0 0 ]，所以現在 input 變成 2<br>
<br>
0:34:36.190,0:34:40.690<br>
這個 input gate 會怎樣呢，input gate 現在是 [2 0 0]<br>
<br>
0:34:41.030,0:34:45.910<br>
所以它 activation function 的 input 是 -10<br>
<br>
0:34:45.980,0:34:50.770<br>
所以 output 是趨近於0，0 * 2 = 0<br>
<br>
0:34:51.760,0:34:56.690<br>
等於 input 的 2 被 input gate 擋住了，那 forget gate 呢<br>
<br>
0:34:57.140,0:35:01.570<br>
[2 0 0] 得到的 forget gate，得到<br>
activation function 的 input 是 10<br>
<br>
0:35:01.900,0:35:06.530<br>
所以 forget gate 還是打開的，所以 7 * 1 + 0<br>
<br>
0:35:06.690,0:35:11.540<br>
原來存在 memory 裡面的值是不動的，還是 7<br>
<br>
0:35:11.660,0:35:16.000<br>
那這個 7 它沒有辦法被輸出，因為 output gate<br>
仍然是關閉的，所以 output 仍然是 0<br>
<br>
0:35:16.560,0:35:20.910<br>
好接下來呢，input 是 [1 0 1]<br>
<br>
0:35:21.620,0:35:25.980<br>
那 input [1 0 1]會發生甚麼事呢？這邊 input 仍然是 1<br>
<br>
0:35:26.850,0:35:31.580<br>
那這個 input gate 是被關閉的，那 forget gate 呢<br>
<br>
0:35:31.710,0:35:36.050<br>
forget gate 這個時候仍然跟原來一樣，它是被打開的<br>
<br>
0:35:36.370,0:35:39.140<br>
所以 memory 裡面存的值是不變的<br>
<br>
0:35:39.140,0:35:42.120<br>
那 output gate 呢，當你 input [1 0 1] 的時候<br>
<br>
0:35:42.120,0:35:46.360<br>
你會打開 output gate，這時候 activation function <br>
的 input 變成 90<br>
<br>
0:35:46.670,0:35:49.740<br>
通過 activation function 以後呢，得到 1<br>
<br>
0:35:51.560,0:35:56.120<br>
那 1 * 7 = 7 這樣子<br>
<br>
0:35:56.290,0:36:01.220<br>
所以 output 的地方會變成是有值的，<br>
memory 裡面的值呢<br>
<br>
0:36:01.480,0:36:06.450<br>
存在 memory 裡面的值 7 呢，會被讀取出來<br>
<br>
0:36:06.840,0:36:10.700<br>
最後，讓我們試一下 [3 -1 0]<br>
<br>
0:36:12.360,0:36:15.510<br>
[3 -1 0] 這個 3 呢，就被讀進來<br>
<br>
0:36:16.900,0:36:21.490<br>
input gate 會被關起來，那 forget gate 呢？<br>
<br>
0:36:23.120,0:36:29.320<br>
因為這個值是 -1，所以 forget gate 的<br>
activation function input 是 -90<br>
<br>
0:36:29.320,0:36:32.440<br>
activation output 就是 0<br>
<br>
0:36:32.550,0:36:36.150<br>
所以呢， memory 裡面存的值會被洗掉<br>
<br>
0:36:36.320,0:36:40.320<br>
memory 裡面存的值會乘上 forget gate 的 output，<br>
會被洗掉變成 0<br>
<br>
0:36:40.460,0:36:46.360<br>
那 output gate 呢，這時候仍然是關起來的，不過它有開沒開也沒差，因為反正現在存在 memory 裡面的值變成 0<br>
<br>
0:36:46.420,0:36:49.820<br>
那它讀出來的值也是 0<br>
<br>
0:36:51.900,0:36:56.560<br>
好那你看到這邊你可能會有一個問題，這個東西<br>
<br>
0:36:56.690,0:37:01.410<br>
跟我們原來看到的 neural network 感覺很不像阿<br>
<br>
0:37:01.490,0:37:05.840<br>
它跟原來的 neural network 到底有甚麼樣的關係呢，<br>
你可以這樣想<br>
<br>
0:37:07.910,0:37:12.210<br>
在我們原來的 neural network 裡面會有很多的 neuron<br>
<br>
0:37:12.800,0:37:19.320<br>
我們會把 input 乘上很多不同的 weight<br>
<br>
0:37:19.500,0:37:22.600<br>
然後當作是不同 neuron 的輸入<br>
<br>
0:37:22.720,0:37:25.460<br>
然後每一個 neuron 它都是一個 function<br>
<br>
0:37:25.460,0:37:28.920<br>
它輸入一個 scalar，output 另外一個 scalar<br>
<br>
0:37:28.920,0:37:32.020<br>
但是如果是 LSTM 的話呢<br>
<br>
0:37:32.250,0:37:37.130<br>
你其實只要把那個 LSTM 的那個 memory cell <br>
想成是一個 neuron 就好<br>
<br>
0:37:37.570,0:37:41.690<br>
所以如果我們今天要用一個 LSTM 的 network<br>
<br>
0:37:42.720,0:37:47.610<br>
你做的事情只是把原來一個簡單的 neuron<br>
<br>
0:37:47.740,0:37:51.660<br>
換成一個 LSTM 的 cell<br>
<br>
0:37:52.980,0:37:57.910<br>
而現在的 input x1, x2 它會乘上不同的 weight<br>
<br>
0:37:57.970,0:38:02.890<br>
當作 LSTM 的不同的輸入<br>
<br>
0:38:02.960,0:38:06.790<br>
也就是說 x1, x2 乘上某一組 weight，變成<br>
<br>
0:38:07.850,0:38:11.330<br>
假設我們現在這個 hidden layer 只有兩個 neuron<br>
<br>
0:38:12.550,0:38:16.700<br>
也就是只有兩個 LSTM，但實際上你不會只有兩個 neuron<br>
<br>
0:38:17.150,0:38:21.780<br>
你可能會有比如說 1000 個 neuron，<br>
1000 個 LSTM 的 memory cell<br>
<br>
0:38:22.010,0:38:26.630<br>
現在假設只有兩個neuron，那 x1, x2乘上某一組 weight<br>
<br>
0:38:26.840,0:38:31.800<br>
會去操控第一個 LSTM 的 output gate，乘上另外一組 weight，操控第一個 LSTM 的input gate<br>
<br>
0:38:31.980,0:38:38.960<br>
乘上一組 weight，當作第一個 LSTM的input，乘上另外一組 weight，當作另外一個 LSTM的forget gate的 input<br>
<br>
0:38:38.960,0:38:42.000<br>
第二個 LSTM 也是一樣<br>
<br>
0:38:42.000,0:38:46.600<br>
x1, x2 乘上某一組 weight 操控它的 output，它會操控它的 input<br>
<br>
0:38:47.270,0:38:51.350<br>
操控它的 output gate，操控它的 input gate，<br>
操控它的 input，操控它的 forget gate 等等<br>
<br>
0:38:52.160,0:38:57.660<br>
所以我們剛才講過說 LSTM 它就是有 4 個 input ，1 個 output<br>
<br>
0:38:57.720,0:39:01.560<br>
而對一個 LSTM 來說<br>
<br>
0:39:02.200,0:39:07.160<br>
它的這 4 個 input 是不一樣的，這 4 個 input 都是不一樣的<br>
<br>
0:39:07.190,0:39:12.070<br>
這 4 個 input 都是不一樣的，在原來的 neural network 裡面<br>
<br>
0:39:12.130,0:39:17.030<br>
一個 neuron 就是一個 input，一個 output，<br>
在 LSTM 裡面它需要 4 個 input<br>
<br>
0:39:17.920,0:39:21.960<br>
它才能夠產生一個 output<br>
<br>
0:39:21.960,0:39:27.440<br>
就好像說有的機器呢，他只要插一個電源線它就可以跑，那像 LSTM 呢，它就要插 4 個電源線才能跑<br>
<br>
0:39:28.540,0:39:31.020<br>
那所以 LSTM 因為它需要 4 個 input<br>
<br>
0:39:31.020,0:39:33.050<br>
而這 4 個 input 都是不一樣的<br>
<br>
0:39:33.160,0:39:37.490<br>
所以 LSTM 它需要的參數量，假設你現在用的 neuron 的數目<br>
<br>
0:39:37.720,0:39:42.630<br>
假設 LSTM 的 network 跟原來的<br>
<br>
0:39:42.850,0:39:47.150<br>
neuron 的 network，他們的 neuron數目是一樣的時候<br>
<br>
0:39:47.440,0:39:52.500<br>
LSTM 需要的參數量會是一般的 neural network 的 4 倍<br>
<br>
0:39:53.360,0:39:57.720<br>
那從這個圖上，你可以很明顯地<br>
<br>
0:39:58.000,0:40:02.650<br>
看出來，一般的 neural network 只需要這個部分的參數，只需要這個部分的參數<br>
<br>
0:40:03.040,0:40:07.990<br>
但 LSTM 還要操控另外三個 gate，所以他需要 4 倍的參數<br>
<br>
0:40:08.120,0:40:12.810<br>
不過這樣講你可能還是沒有辦法很了解，你沒有辦法體會的可能是跟<br>
<br>
0:40:13.400,0:40:17.730<br>
Recurrent Neural Network 的關係是甚麼<br>
<br>
0:40:18.550,0:40:23.050<br>
這個好像看起來不太像 Recurrent Neural Network<br>
<br>
0:40:23.220,0:40:28.200<br>
所以呢，我們要畫另外一個圖呢來表示它，<br>
你可以想像這個圖呢<br>
<br>
0:40:28.260,0:40:32.970<br>
也是要畫非常久，假設我們現在有一整排的<br>
<br>
0:40:33.340,0:40:37.760<br>
neuron，假設有一整排的 LSTM<br>
<br>
0:40:38.190,0:40:43.110<br>
那這一整排的 LSTM 裡面，<br>
它們每一個人的 memory 裡面<br>
<br>
0:40:43.290,0:40:48.150<br>
都存了一個值，每一個 LSTM 的 cell<br>
<br>
0:40:48.520,0:40:52.690<br>
它裡面都存了一個 scalar<br>
<br>
0:40:53.220,0:40:58.000<br>
把所有的 scalar 接起來，把這些 scalar 接起來<br>
<br>
0:40:58.060,0:41:01.720<br>
它就變成一個 vector，這邊寫成 c^(t-1)<br>
<br>
0:41:03.870,0:41:08.200<br>
那你可以想乘這邊每一個 memory 它裡面存的 scalar<br>
<br>
0:41:08.470,0:41:12.670<br>
就是代表這個 vector 裡面的一個 dimension<br>
<br>
0:41:13.270,0:41:17.850<br>
現在，在時間點 t<br>
<br>
0:41:18.420,0:41:23.170<br>
input 一個 vector, x^t<br>
<br>
0:41:23.360,0:41:27.700<br>
這個 vector，它會先乘上一個 linear 的 transform<br>
<br>
0:41:27.810,0:41:32.250<br>
乘上一個 matrix，變成另外一個 vector z<br>
<br>
0:41:32.380,0:41:36.510<br>
你把 x^t 乘上一個 matrix 變成 z<br>
<br>
0:41:37.490,0:41:41.650<br>
那這個 z，也是一個 vector，<br>
那 z 這個 vector 代表甚麼呢<br>
<br>
0:41:42.200,0:41:46.700<br>
z 這個 vector 的每一個 dimension 呢<br>
<br>
0:41:47.230,0:41:51.740<br>
z 這個 vector 的每一個 dimension 呢，就代表了操控<br>
<br>
0:41:52.270,0:41:56.300<br>
每一個 LSTM 的 input<br>
<br>
0:41:57.100,0:42:02.030<br>
所以 z 它的 dimension 就正好是 LSTM 的<br>
<br>
0:42:02.100,0:42:06.510<br>
memory cell 的數目，那正好就是它的數目<br>
<br>
0:42:06.620,0:42:12.920<br>
那這個 z 的第一維就丟給第一個 cell，<br>
第二維就丟給第二個 cell，以此類推<br>
<br>
0:42:12.920,0:42:14.760<br>
希望大家知道我的意思<br>
<br>
0:42:15.880,0:42:18.840<br>
好那這個 x^t 會再乘上另外一個 transform<br>
<br>
0:42:18.840,0:42:21.620<br>
得到 z^i<br>
<br>
0:42:21.620,0:42:25.500<br>
然後這個 z^i 呢，它的 dimension 也跟 cell 的數目一樣<br>
<br>
0:42:26.080,0:42:30.950<br>
z^i 的每一個 dimension<br>
<br>
0:42:31.210,0:42:36.020<br>
都會去操控一個 memory，所以 z^i 的第一維就是<br>
<br>
0:42:36.120,0:42:41.040<br>
去操控第一個 cell 的 input gate，第二維<br>
就是操控第二個 cell 的 input gate，最後一維<br>
<br>
0:42:41.120,0:42:45.350<br>
就是操控最後一個 cell 的 input gate，<br>
那 forget gate 呢？<br>
<br>
0:42:47.060,0:42:51.580<br>
跟 output gate 也是一樣，這邊就不再贅述<br>
<br>
0:42:51.580,0:42:56.680<br>
把 x^t 乘上一個 transform，得到 z^f<br>
z^f 會去操控每一個 forget gate<br>
<br>
0:42:56.800,0:43:04.220<br>
然後 x^t 乘上另外一個 transform，得到 z^o<br>
z^o 會去操控每一個 cell 的 output gate<br>
<br>
0:43:04.220,0:43:08.740<br>
好，所以我們把 x^t 乘上 4 個不同的 transform<br>
<br>
0:43:08.740,0:43:12.040<br>
得到 4 個不同的 vector，這 4 個 vector 的 dimension<br>
<br>
0:43:12.340,0:43:17.270<br>
都跟 cell 的數目是一樣的，那這 4 個 vector 合起來<br>
<br>
0:43:17.470,0:43:21.270<br>
就會去操控<br>
<br>
0:43:23.130,0:43:25.570<br>
這些 memory cell 的運作<br>
<br>
0:43:28.100,0:43:32.080<br>
好那我們知道一個 memory cell 就是長這樣<br>
<br>
0:43:32.100,0:43:37.180<br>
那現在 input 分別是 z, z^i, z^f, z^o,  那注意一下就是這 4 個<br>
<br>
0:43:37.710,0:43:42.680<br>
這 4 個 z 其實都是 vector<br>
<br>
0:43:42.820,0:43:47.630<br>
丟到 cell 裡面的值呢，<br>
其實只是每一個 vector 的一個 dimension<br>
<br>
0:43:47.880,0:43:52.540<br>
那因為每一個 cell 他們 input 的 dimension 都是不一樣的<br>
<br>
0:43:52.700,0:43:57.160<br>
所以他們 input 的值都會是不一樣的，但是<br>
<br>
0:43:57.160,0:44:02.300<br>
所有的 cell 是可以共同一起被運算的<br>
怎麼一起共同被運算呢<br>
<br>
0:44:02.590,0:44:06.300<br>
我們說 z 要乘上 z^i<br>
<br>
0:44:06.430,0:44:10.920<br>
要把 z^i 先通過 activation function，<br>
然後把它跟 z 相乘<br>
<br>
0:44:11.850,0:44:16.560<br>
所以我們就把 z^i 先通過 activation function，跟 z 相乘<br>
<br>
0:44:16.740,0:44:21.450<br>
這個乘呢，是這個 element-wise 的 product 的意思<br>
<br>
0:44:21.640,0:44:26.300<br>
element-wise 的相乘，好那這個 z^f 也要通過<br>
<br>
0:44:26.360,0:44:30.790<br>
forget gate 的 activation function，z^f 通過這個 activation function<br>
<br>
0:44:31.010,0:44:34.570<br>
它跟之前已經存在 cell 裡面的值呢，相乘這件事情<br>
<br>
0:44:36.420,0:44:41.290<br>
它跟原來存在 memory cell 裡面的值相乘，它把它跟它相乘<br>
<br>
0:44:41.810,0:44:46.750<br>
然後接下來呢，也要把這兩個值加起來<br>
<br>
0:44:47.900,0:44:51.700<br>
你就是把 z^i 跟 z 相乘的值加上 z^f<br>
<br>
0:44:52.080,0:44:55.660<br>
跟 c^(t-1) 相乘的值，把他們加起來<br>
<br>
0:44:56.480,0:45:01.320<br>
好那 output gate 呢，<br>
z^o 通過 activation function，然後呢<br>
<br>
0:45:01.430,0:45:06.360<br>
把這個 output 跟相加以後的結果呢，<br>
<br>
0:45:06.700,0:45:11.080<br>
再相乘，最後就得到最後的 output 的 y<br>
<br>
0:45:13.100,0:45:18.000<br>
這個時候相加以後的結果，就是 memory 裡面存的值<br>
<br>
0:45:18.940,0:45:23.720<br>
相加以後的結果，也就是 memory 裡面存的值<br>
也就是 c^t<br>
<br>
0:45:23.990,0:45:26.820<br>
那這 process 呢，就反覆地繼續下去<br>
<br>
0:45:26.820,0:45:30.800<br>
在下一個時間點，input x^(t+1)<br>
<br>
0:45:30.800,0:45:34.100<br>
然後呢，你把 z 跟 input gate 相乘<br>
<br>
0:45:34.810,0:45:39.600<br>
你把 forget gate 跟存在 memory 裡面的值相乘<br>
<br>
0:45:39.760,0:45:44.140<br>
然後再把這個值跟這個值加起來，<br>
再乘上 output gate 的值<br>
<br>
0:45:44.820,0:45:47.680<br>
然後得到下一個時間點的輸出這樣子<br>
<br>
0:45:47.680,0:45:51.860<br>
那你可能覺得說這已經很複雜了，如果你自己<br>
做投影片的話，顯然是要做非常久<br>
<br>
0:45:56.380,0:45:59.700<br>
這個不是 LSTM 的最終型態<br>
<br>
0:45:59.700,0:46:02.880<br>
這個只是一個 simplified 的 version<br>
<br>
0:46:02.900,0:46:05.840<br>
真正的 LSTM 會怎麼做呢<br>
<br>
0:46:05.970,0:46:08.060<br>
它會把這個地方<br>
<br>
0:46:10.520,0:46:14.400<br>
這個地方的輸出呢，把它接進來<br>
<br>
0:46:18.360,0:46:23.960<br>
它會把這個 hidden layer 的輸出把它接進來<br>
當作下一個時間點的 input<br>
<br>
0:46:24.080,0:46:28.320<br>
也就是說，下一個時間點操控這些 gate 的值，不是只看<br>
<br>
0:46:28.520,0:46:33.160<br>
那個時間點的 input x，也看前一個時間點的 output h<br>
<br>
0:46:33.440,0:46:37.480<br>
然後其實還不只這樣，<br>
還會加一個東西呢，叫做 "peephole"<br>
<br>
0:46:37.780,0:46:42.920<br>
這個 peephole 是甚麼呢？<br>
這個 peephole 就是把存在 memory cell 裡面的值呢<br>
<br>
0:46:43.730,0:46:48.690<br>
也拉過來，所以在操縱 LSTM 的 4個 gate 的時候<br>
<br>
0:46:48.850,0:46:53.320<br>
你是同時考慮了 x, 同時考慮了 h, 同時考慮了 c<br>
<br>
0:46:53.560,0:46:58.560<br>
你把這 3 個 vector 並在一起，乘上4個<br>
不同的 transform，得到這4個不同的 vector<br>
<br>
0:46:58.680,0:47:00.940<br>
再去操控 LSTM<br>
<br>
0:47:00.940,0:47:03.300<br>
那 LSTM 通常不會只有一層<br>
<br>
0:47:03.300,0:47:06.320<br>
現在胡亂都要疊個五、六層才爽這樣<br>
<br>
0:47:06.320,0:47:08.440<br>
所以他就長的大概是這個樣子<br>
<br>
0:47:08.800,0:47:13.500<br>
然後每一個第一次看到這個東西的人啊<br>
<br>
0:47:13.750,0:47:15.750<br>
他的反應都是這樣子<br>
<br>
0:47:16.470,0:47:21.370<br>
就我記得，大家知道 sequence 有 sequence model 嗎<br>
<br>
0:47:21.530,0:47:26.340<br>
Google Brain 的 proposed，然後我有聽過他的這個 talk<br>
<br>
0:47:26.860,0:47:31.330<br>
他說他第一次看到 LSTM 的時候，<br>
他的想法呢，就跟這個圖上是一樣的<br>
<br>
0:47:31.820,0:47:36.650<br>
這個太複雜了，這應該不 work 吧，我認識的每一個人第一次看到 LSTM 都覺得說<br>
<br>
0:47:36.930,0:47:41.840<br>
這個應該不 work 這樣，<br>
但是他其實現在還 quite standard 這樣<br>
<br>
0:47:42.200,0:47:46.880<br>
當有一個人告訴你說，當有一個人說<br>
<br>
0:47:47.060,0:47:51.820<br>
我用 RNN 做了甚麼事情的時候，<br>
就是你不要去問他說為甚麼你不用 LSTM<br>
<br>
0:47:51.880,0:47:56.770<br>
因為他其實就是用 LSTM ，<br>
因為現在當你說你在做 RNN 的時候<br>
<br>
0:47:57.540,0:47:59.340<br>
其實你指的就是用 LSTM<br>
<br>
0:47:59.340,0:48:02.480<br>
所以呢，他其實是比較 standard 的<br>
<br>
0:48:02.550,0:48:07.380<br>
其實 Keras 裡面有支援 LSTM 啦，所以就算是<br>
<br>
0:48:07.620,0:48:12.420<br>
剛才講得這麼複雜的東西你沒聽懂就算了<br>
在 Keras 裡面就是打 LSTM 這4個字母<br>
<br>
0:48:12.480,0:48:14.480<br>
然後就結束了<br>
<br>
0:48:17.030,0:48:20.460<br>
那 Keras 它其實支援三種 Recurrent neural networks，一個是 LSTM<br>
<br>
0:48:20.460,0:48:21.990<br>
一個是 GRU<br>
<br>
0:48:22.050,0:48:25.120<br>
GRU 是 LSTM 的一個稍微簡化的版本<br>
<br>
0:48:25.120,0:48:27.040<br>
它只有兩個 gate<br>
<br>
0:48:27.140,0:48:32.340<br>
據說少了一個 gate，但是 performance 跟 LSTM<br>
差不多，而且少了 1/3 的參數<br>
<br>
0:48:32.480,0:48:34.400<br>
所以比較不容易 over-fitting<br>
<br>
0:48:34.400,0:48:40.120<br>
如果你要用一般的、我們這堂課最一開始講的那種<br>
最簡單的 RNN 的話<br>
<br>
0:48:40.120,0:48:43.960<br>
也要說是 SimpleRNN 才行這樣子<br>
<br>
0:48:45.390,0:48:49.720<br>
好那我想我們今天就上到這邊好了，那我們就下課<br>
<br>
0:48:50.500,0:48:51.300<br>
謝謝<br>
<br>
0:48:51.300,0:48:56.980<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
