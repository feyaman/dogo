<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.000,0:00:04.000<br>
我們今天要講的是 Regression<br>
<br>
0:00:04.000,0:00:06.000<br>
等一下我會舉一個例子，<br>
<br>
0:00:06.000,0:00:08.000<br>
來講 Regression 是怎麼做的。<br>
<br>
0:00:10.000,0:00:12.000<br>
順便引出一些 machine learning 裡面<br>
<br>
0:00:12.000,0:00:14.000<br>
常見的重要觀念。<br>
<br>
0:00:16.000,0:00:18.000<br>
那regression可以做甚麼?<br>
<br>
0:00:18.000,0:00:18.640<br>
除了我們作業裡面要大家做的<br>
<br>
0:00:18.640,0:00:20.000<br>
除了我們作業裡面要大家做的<br>
<br>
0:00:20.000,0:00:20.700<br>
預測PM2.5這個任務以外，<br>
<br>
0:00:20.700,0:00:22.000<br>
預測PM2.5這個任務以外，<br>
<br>
0:00:22.000,0:00:24.000<br>
還有很多其他非常有用的task。<br>
<br>
0:00:24.860,0:00:26.000<br>
舉例來說，<br>
<br>
0:00:26.000,0:00:28.000<br>
如果你可以做一個股票預測的系統，<br>
<br>
0:00:28.000,0:00:30.000<br>
如果你可以做一個股票預測的系統，<br>
<br>
0:00:30.000,0:00:32.000<br>
如果你可以做一個股票預測的系統，<br>
<br>
0:00:32.000,0:00:34.000<br>
你要做的事情就是 :<br>
<br>
0:00:34.000,0:00:36.000<br>
找一個function。<br>
<br>
0:00:36.000,0:00:36.640<br>
這個function的input可能是，<br>
<br>
0:00:40.000,0:00:42.000<br>
過去十年，<br>
<br>
0:00:42.000,0:00:44.000<br>
各種股票起伏的資料。<br>
<br>
0:00:44.000,0:00:44.700<br>
或者是，<br>
<br>
0:00:46.000,0:00:48.000<br>
A公司併購B公司，<br>
<br>
0:00:48.000,0:00:50.000<br>
B公司併購C公司等等的資料。<br>
<br>
0:00:50.000,0:00:52.000<br>
你希望這個function在input這些資料以後，<br>
<br>
0:00:52.000,0:00:54.000<br>
它的output是，明天的道瓊工業指數的點數。<br>
<br>
0:00:54.000,0:00:56.000<br>
它的output是，明天的道瓊工業指數的點數。<br>
<br>
0:00:56.000,0:00:58.000<br>
它的output是，明天的道瓊工業指數的點數。<br>
<br>
0:00:58.000,0:01:00.000<br>
如果你可以預測這個的話，你就發了<br>
<br>
0:01:02.000,0:01:04.000<br>
還有別的task，<br>
<br>
0:01:04.000,0:01:06.000<br>
比如說現在很熱門的<br>
<br>
0:01:06.000,0:01:08.000<br>
無人車，自動車。<br>
<br>
0:01:08.000,0:01:10.000<br>
這個自動車也可以想成是一個regression的problem，<br>
<br>
0:01:10.000,0:01:12.000<br>
在這個regression的problem裡面，<br>
<br>
0:01:12.000,0:01:14.000<br>
input就是，你的無人車所看到的各種sensor :<br>
<br>
0:01:14.000,0:01:16.000<br>
input就是，你的無人車所看到的各種sensor :<br>
<br>
0:01:16.000,0:01:18.000<br>
它的紅外線感測的sensor，<br>
<br>
0:01:18.000,0:01:20.000<br>
它的影像的視訊的鏡頭所看到的馬路上的東西等等。<br>
<br>
0:01:20.000,0:01:22.000<br>
它的影像的視訊的鏡頭所看到的馬路上的東西等等。<br>
<br>
0:01:22.000,0:01:24.000<br>
它的影像的視訊的鏡頭所看到的馬路上的東西等等。<br>
<br>
0:01:24.000,0:01:26.000<br>
你的input就是這些information，<br>
<br>
0:01:26.000,0:01:28.000<br>
你的input就是這些information，<br>
<br>
0:01:28.000,0:01:30.000<br>
output就是方向盤的角度。<br>
<br>
0:01:30.000,0:01:32.000<br>
比如說，<br>
<br>
0:01:32.000,0:01:34.000<br>
要左轉50度，<br>
<br>
0:01:34.000,0:01:36.000<br>
還是右轉50度，<br>
<br>
0:01:36.000,0:01:38.000<br>
右轉50度你就當作左轉 負50度，<br>
<br>
0:01:38.000,0:01:40.000<br>
所以output也是一個 scalar。<br>
<br>
0:01:40.000,0:01:42.000<br>
所以無人車駕駛就是一個regression problem<br>
<br>
0:01:42.000,0:01:44.000<br>
input一些information，<br>
<br>
0:01:44.000,0:01:46.000<br>
output就是方向盤的角度，<br>
<br>
0:01:46.000,0:01:48.000<br>
它是一個數值。<br>
<br>
0:01:48.000,0:01:50.000<br>
或者是，<br>
<br>
0:01:50.000,0:01:52.000<br>
你可以做推薦系統。<br>
<br>
0:01:52.000,0:01:54.000<br>
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，<br>
<br>
0:01:54.000,0:01:56.000<br>
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，<br>
<br>
0:01:56.000,0:01:58.000<br>
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，<br>
<br>
0:01:58.000,0:02:00.000<br>
那推薦系統它要做的事情，<br>
<br>
0:02:00.000,0:02:02.000<br>
也可以想成是一個regression的問題。<br>
<br>
0:02:02.000,0:02:04.000<br>
就是找一個 function，<br>
<br>
0:02:04.000,0:02:06.000<br>
它的input是，<br>
<br>
0:02:06.000,0:02:08.000<br>
某一個使用者A和某一個商品B，<br>
<br>
0:02:08.000,0:02:10.000<br>
某一個使用者A和某一個商品B，<br>
<br>
0:02:10.000,0:02:12.000<br>
它的output就是，<br>
<br>
0:02:12.000,0:02:14.000<br>
使用者A購買商品B的可能性。<br>
<br>
0:02:14.000,0:02:16.000<br>
使用者A購買商品B的可能性。<br>
<br>
0:02:16.000,0:02:18.000<br>
如果你可以找到這樣一個function，<br>
<br>
0:02:18.000,0:02:20.000<br>
它可以精確的預測說，<br>
<br>
0:02:20.000,0:02:22.000<br>
使用者A購買商品B的可能性的話，<br>
<br>
0:02:22.000,0:02:24.000<br>
使用者A購買商品B的可能性的話，<br>
<br>
0:02:24.000,0:02:26.000<br>
那Amazon就會推薦使用者A他最有可能購買的商品。<br>
<br>
0:02:26.000,0:02:28.000<br>
那Amazon就會推薦使用者A他最有可能購買的商品。<br>
<br>
0:02:28.000,0:02:30.000<br>
那Amazon就會推薦使用者A他最有可能購買的商品。<br>
<br>
0:02:32.000,0:02:34.000<br>
這個是regression的種種應用，<br>
<br>
0:02:34.000,0:02:36.000<br>
今天我要講的是，<br>
<br>
0:02:36.000,0:02:38.000<br>
另外一個我覺得更實用的應用 :<br>
<br>
0:02:38.000,0:02:40.000<br>
就是預測寶可夢的CP值。<br>
<br>
0:02:40.000,0:02:42.000<br>
就是預測寶可夢的CP值。<br>
<br>
0:02:42.000,0:02:44.000<br>
這個大家知道是甚麼意思嗎?<br>
<br>
0:02:44.000,0:02:46.000<br>
我來說明一下好了:<br>
<br>
0:02:46.000,0:02:48.000<br>
你的CP值就是一隻寶可夢的戰鬥力。<br>
<br>
0:02:48.000,0:02:50.000<br>
你的CP值就是一隻寶可夢的戰鬥力。<br>
<br>
0:02:50.000,0:02:52.000<br>
你抓到一隻寶可夢後，<br>
<br>
0:02:52.000,0:02:54.000<br>
比如說，<br>
<br>
0:02:54.000,0:02:56.000<br>
這個是一隻妙蛙種子，<br>
<br>
0:03:00.000,0:03:02.000<br>
比如說這是一隻妙蛙種子，<br>
<br>
0:03:02.000,0:03:04.000<br>
比如說這是一隻妙蛙種子，<br>
<br>
0:03:04.000,0:03:06.000<br>
然後你給他吃一些星辰或糖果以後，<br>
<br>
0:03:06.000,0:03:08.000<br>
他就會進化成妙蛙草。<br>
<br>
0:03:08.000,0:03:10.000<br>
而如果他進化成妙蛙草以後，<br>
<br>
0:03:10.000,0:03:12.000<br>
他的CP值就變了。<br>
<br>
0:03:12.000,0:03:14.000<br>
他的CP值就變了。<br>
<br>
0:03:16.000,0:03:18.000<br>
為甚麼我們會希望能夠預測寶可夢的CP值呢?<br>
<br>
0:03:18.000,0:03:20.000<br>
為甚麼我們會希望能夠預測寶可夢的CP值呢?<br>
<br>
0:03:20.000,0:03:22.000<br>
為甚麼我們會希望能夠預測寶可夢的CP值呢?<br>
<br>
0:03:24.000,0:03:26.000<br>
因為如果你可以精確的預測一隻寶可夢在進化以後的CP值的話，<br>
<br>
0:03:26.000,0:03:28.000<br>
因為如果你可以精確的預測一隻寶可夢在進化以後的CP值的話，<br>
<br>
0:03:28.000,0:03:30.000<br>
你就可以評估說，<br>
<br>
0:03:30.000,0:03:32.000<br>
你是否要進化這隻寶可夢。<br>
<br>
0:03:32.000,0:03:34.000<br>
你是否要進化這隻寶可夢。<br>
<br>
0:03:34.000,0:03:36.000<br>
如果他是一隻CP值比較低的寶可夢的話，<br>
<br>
0:03:36.000,0:03:38.000<br>
你可能就把他拿去做糖果。<br>
<br>
0:03:40.000,0:03:42.000<br>
你就不進化他，<br>
<br>
0:03:42.000,0:03:44.000<br>
這樣你就可以節省一些你的糖果的資源。<br>
<br>
0:03:44.000,0:03:44.800<br>
你可能就會問說，<br>
<br>
0:03:44.800,0:03:46.000<br>
為甚麼我們要節省糖果的資源?<br>
<br>
0:03:46.000,0:03:48.000<br>
為甚麼我們要節省糖果的資源?<br>
<br>
0:03:48.000,0:03:50.000<br>
因為你這樣可以在比較短時間內，<br>
<br>
0:03:50.000,0:03:52.000<br>
就進化比較多強的神奇寶貝。<br>
<br>
0:04:00.000,0:04:02.000<br>
你就會想說為甚麼我們要比較強的寶可夢?<br>
<br>
0:04:04.000,0:04:06.000<br>
因為他可以去打道館。<br>
<br>
0:04:06.000,0:04:08.000<br>
你問為甚麼我們要去打道館?<br>
<br>
0:04:08.000,0:04:10.000<br>
其實我也不知道這樣。<br>
<br>
0:04:12.000,0:04:14.000<br>
我們今天要做的事情，就是找一個function。<br>
<br>
0:04:14.000,0:04:16.000<br>
這個function的input，<br>
<br>
0:04:18.000,0:04:20.000<br>
就是某一隻寶可夢。<br>
<br>
0:04:20.000,0:04:22.000<br>
它的output就是<br>
<br>
0:04:22.000,0:04:24.000<br>
這隻寶可夢如果我們把它進化以後，<br>
<br>
0:04:24.000,0:04:26.000<br>
這隻寶可夢如果我們把它進化以後，<br>
<br>
0:04:26.000,0:04:28.000<br>
它的CP值的數值是多少。<br>
<br>
0:04:28.000,0:04:30.000<br>
這是一個regression的problem，<br>
<br>
0:04:30.000,0:04:32.000<br>
我們的input就是某一隻寶可夢所有相關的information，<br>
<br>
0:04:32.000,0:04:34.000<br>
我們的input就是某一隻寶可夢所有相關的information，<br>
<br>
0:04:34.000,0:04:36.000<br>
比如說，<br>
<br>
0:04:36.000,0:04:38.000<br>
我們把一隻寶可夢用X表示，<br>
<br>
0:04:38.000,0:04:40.000<br>
我們把一隻寶可夢用Xcp表示。<br>
<br>
0:04:40.000,0:04:42.000<br>
它的CP值我們就用Xcp來表示。<br>
<br>
0:04:42.000,0:04:44.000<br>
它的CP值我們就用Xcp來表示。<br>
<br>
0:04:44.000,0:04:46.000<br>
它的CP值我們就用Xcp來表示。<br>
<br>
0:04:46.000,0:04:48.000<br>
我們用下標來表示某一個，<br>
<br>
0:04:48.000,0:04:50.000<br>
完整的東西裡面的，<br>
<br>
0:04:50.000,0:04:52.000<br>
某一個component，<br>
<br>
0:04:52.000,0:04:54.000<br>
某一個部分，我們用下標來表示。<br>
<br>
0:04:54.000,0:04:56.000<br>
Xcp代表某一隻寶可夢X，<br>
<br>
0:04:56.000,0:04:58.000<br>
它在進化前的CP值。<br>
<br>
0:05:00.000,0:05:02.000<br>
比如說，這個妙蛙種子，<br>
<br>
0:05:02.000,0:05:04.000<br>
它CP值是14，<br>
<br>
0:05:04.000,0:05:04.760<br>
Xs代表這一隻寶可夢X，是屬於哪一個物種。<br>
<br>
0:05:04.760,0:05:06.000<br>
Xs代表這一隻寶可夢X，是屬於哪一個物種。<br>
<br>
0:05:06.000,0:05:08.000<br>
Xs代表這一隻寶可夢X，是屬於哪一個物種。<br>
<br>
0:05:08.000,0:05:10.000<br>
Xs代表這一隻寶可夢X，是屬於哪一個物種。<br>
<br>
0:05:10.000,0:05:12.000<br>
Xs代表這一隻寶可夢X，是屬於哪一個物種。<br>
<br>
0:05:12.000,0:05:14.000<br>
比如說這是妙蛙種子。<br>
<br>
0:05:14.000,0:05:16.000<br>
Xhp代表這一隻表可夢，它的hp值是多少，<br>
<br>
0:05:16.000,0:05:18.000<br>
Xhp代表這一隻表可夢，它的hp值是多少，<br>
<br>
0:05:18.000,0:05:20.000<br>
它的生命值是多少。<br>
<br>
0:05:22.000,0:05:24.000<br>
這個妙蛙種子的生命值是10。<br>
<br>
0:05:24.000,0:05:26.000<br>
Xw代表它的重量，<br>
<br>
0:05:26.000,0:05:28.000<br>
Xh代表它的高度。<br>
<br>
0:05:30.000,0:05:32.000<br>
可以看看你抓的寶可夢是不是特別大隻或特別小隻。<br>
<br>
0:05:36.000,0:05:38.000<br>
那output是進化後的CP。<br>
<br>
0:05:38.000,0:05:40.000<br>
這個進化後的CP值，<br>
<br>
0:05:40.000,0:05:42.000<br>
就是一個數值。<br>
<br>
0:05:42.000,0:05:44.000<br>
就是一個scalar，<br>
<br>
0:05:44.000,0:05:46.000<br>
我們把它用Y來表示。<br>
<br>
0:05:46.000,0:05:46.620<br>
這麼解這個問題呢?<br>
<br>
0:05:50.000,0:05:52.000<br>
我們第一堂課就講過說，做machine learning就是三個步驟，<br>
<br>
0:05:52.000,0:05:54.000<br>
第一個步驟就是，<br>
<br>
0:05:54.000,0:05:56.000<br>
找一個model；<br>
<br>
0:05:56.000,0:05:58.000<br>
第二個步驟是，<br>
<br>
0:05:58.000,0:06:00.000<br>
model就是一個function set，<br>
<br>
0:06:00.000,0:06:02.000<br>
第二個步驟就是，<br>
<br>
0:06:02.000,0:06:04.000<br>
定義function set裡面某一個function，<br>
<br>
0:06:04.000,0:06:06.000<br>
我們拿一個function出來<br>
<br>
0:06:06.000,0:06:08.000<br>
可以要evaluate它的好壞；<br>
<br>
0:06:08.000,0:06:10.000<br>
第三步驟就是找一個最好的function。<br>
<br>
0:06:12.980,0:06:14.000<br>
首先我們就從第一個步驟開始。<br>
<br>
0:06:14.000,0:06:16.000<br>
我們要找一個function set，<br>
<br>
0:06:16.000,0:06:18.000<br>
這個function set就是所謂的model。<br>
<br>
0:06:18.000,0:06:20.000<br>
在這個task裡面，<br>
<br>
0:06:20.000,0:06:22.000<br>
我們的function set，<br>
<br>
0:06:22.000,0:06:24.000<br>
應該長甚麼樣子呢?<br>
<br>
0:06:24.000,0:06:26.000<br>
一個 input一隻寶可夢，<br>
<br>
0:06:28.000,0:06:30.000<br>
output進化後的CP值的function，<br>
<br>
0:06:30.000,0:06:32.000<br>
output進化後的CP值的function，<br>
<br>
0:06:32.000,0:06:34.000<br>
應該長甚麼樣子呢?<br>
<br>
0:06:34.000,0:06:36.000<br>
這邊就先亂寫一個簡單的。<br>
<br>
0:06:36.000,0:06:38.000<br>
比如說我們認為說，<br>
<br>
0:06:38.000,0:06:40.000<br>
進化後的CP值Y，<br>
<br>
0:06:40.000,0:06:42.000<br>
等於某一個常數項B加上某一個數值W，<br>
<br>
0:06:42.000,0:06:44.000<br>
等於某一個常數項b加上某一個數值w，<br>
<br>
0:06:44.000,0:06:46.000<br>
等於某一個常數項b加上某一個數值w，<br>
<br>
0:06:46.000,0:06:48.000<br>
乘上現在輸入的寶可夢的X它在進化前的CP值，<br>
<br>
0:06:48.000,0:06:50.000<br>
乘上現在輸入的寶可夢的X它在進化前的CP值，<br>
<br>
0:06:50.000,0:06:52.000<br>
乘上現在輸入的寶可夢的X它在進化前的CP值，<br>
<br>
0:06:54.000,0:06:56.000<br>
這個Xcp代表進化前的CP值，<br>
<br>
0:06:56.000,0:06:58.000<br>
這個Y是進化後的CP值。<br>
<br>
0:07:00.000,0:07:02.000<br>
這個w和b是參數，<br>
<br>
0:07:06.000,0:07:08.000<br>
w和b可以是任何的數值。<br>
<br>
0:07:08.000,0:07:10.000<br>
在這個model裡面，<br>
<br>
0:07:10.000,0:07:12.000<br>
w和b是未知的，<br>
<br>
0:07:12.000,0:07:14.000<br>
你可以把任何的數字填進去，<br>
<br>
0:07:14.000,0:07:16.000<br>
填進不同的數值，<br>
<br>
0:07:16.000,0:07:18.000<br>
你就得到不同的function。<br>
<br>
0:07:18.000,0:07:20.000<br>
比如說你可以有一個f1，<br>
<br>
0:07:20.000,0:07:22.000<br>
f1是b=10，w=9<br>
<br>
0:07:26.000,0:07:28.000<br>
你可以有另外一個function f2，<br>
<br>
0:07:28.000,0:07:30.000<br>
這個f2是b=9.8，w=9.2<br>
<br>
0:07:30.000,0:07:32.000<br>
這個f2是b=9.8，w=9.2<br>
<br>
0:07:32.000,0:07:34.000<br>
你有一個f3，<br>
<br>
0:07:34.000,0:07:36.000<br>
它是b= -0.8，w=-1.2<br>
<br>
0:07:36.000,0:07:38.000<br>
它是b= -0.8，w=-1.2<br>
<br>
0:07:38.000,0:07:40.000<br>
如果今天你的w和b可以代任何值的話，'<br>
<br>
0:07:44.000,0:07:46.000<br>
其實你這個function set裡面，<br>
<br>
0:07:46.000,0:07:48.000<br>
可以有無窮無盡的function，<br>
<br>
0:07:48.000,0:07:50.000<br>
有無窮多的function。<br>
<br>
0:07:50.000,0:07:52.000<br>
你用這個式子y=b+w × Xcp<br>
<br>
0:07:52.000,0:07:54.000<br>
你用這個式子y=b+w × Xcp<br>
<br>
0:07:54.000,0:07:56.000<br>
代表這些function所成的集合。<br>
<br>
0:07:58.000,0:08:00.000<br>
當然在這些function裡面，<br>
<br>
0:08:00.000,0:08:02.000<br>
比如說f1，f2，f3裡面<br>
<br>
0:08:04.000,0:08:06.000<br>
你會發現有一些function顯然不太可能是正確的。<br>
<br>
0:08:08.000,0:08:10.000<br>
比如說f3不太可能是正確的。<br>
<br>
0:08:10.000,0:08:12.000<br>
因為我們知道說CP值其實是正，<br>
<br>
0:08:12.000,0:08:14.000<br>
乘以-1.2就變成是負的<br>
<br>
0:08:14.000,0:08:16.000<br>
所以進化以後CP值就變成是負的，<br>
<br>
0:08:16.000,0:08:18.000<br>
這樣顯然是說不通的。<br>
<br>
0:08:18.000,0:08:20.000<br>
這個就是我們等一下要用靠training data來告訴我們說，<br>
<br>
0:08:22.000,0:08:24.000<br>
在這個function set裡面，<br>
<br>
0:08:24.000,0:08:26.000<br>
哪一個function才是合理的function。<br>
<br>
0:08:28.000,0:08:30.000<br>
這樣子的model，<br>
<br>
0:08:30.000,0:08:32.000<br>
這個y=b+w × Xcp這樣子的model，<br>
<br>
0:08:36.000,0:08:38.000<br>
它是一種linear的model。<br>
<br>
0:08:38.000,0:08:40.000<br>
所謂的linear的model的意思是，<br>
<br>
0:08:40.000,0:08:42.000<br>
簡單來說，<br>
<br>
0:08:42.000,0:08:44.000<br>
如果我們可以把一個function，<br>
<br>
0:08:44.000,0:08:46.000<br>
我們把現在我們要找的function，<br>
<br>
0:08:48.000,0:08:50.000<br>
寫成y=b+ ∑WiXi<br>
<br>
0:08:50.000,0:08:52.000<br>
寫成y=b+ ∑WiXi<br>
<br>
0:08:54.000,0:08:56.000<br>
那它是一個linear的function。<br>
<br>
0:08:58.000,0:09:00.000<br>
這邊的Xi指的是你input 的X的各種attribute。<br>
<br>
0:09:04.000,0:09:06.000<br>
比如說你input寶可夢的各種不同的屬性，<br>
<br>
0:09:06.000,0:09:08.000<br>
比如說身高或體重等等。<br>
<br>
0:09:08.000,0:09:10.000<br>
這些東西我們叫做feature。<br>
<br>
0:09:12.000,0:09:14.000<br>
從input的object裡面，<br>
<br>
0:09:14.000,0:09:16.000<br>
抽出來的各種數值當作function的input，<br>
<br>
0:09:16.000,0:09:18.000<br>
這些東西叫做feature。<br>
<br>
0:09:20.000,0:09:22.000<br>
這個Wi和b，<br>
<br>
0:09:22.000,0:09:24.000<br>
這個Wi叫做weight，<br>
<br>
0:09:26.000,0:09:28.000<br>
這個b叫做bias。<br>
<br>
0:09:34.000,0:09:36.000<br>
接下來我們要收集training data，才能夠找這個function。<br>
<br>
0:09:38.000,0:09:40.000<br>
這是一個supervised learning 的task，<br>
<br>
0:09:40.000,0:09:42.000<br>
所以我們收集的是function的input，<br>
<br>
0:09:42.000,0:09:44.000<br>
和function的output。<br>
<br>
0:09:44.000,0:09:46.000<br>
因為是regression的task，<br>
<br>
0:09:46.000,0:09:48.000<br>
所以function的output是一個數值。<br>
<br>
0:09:50.000,0:09:52.000<br>
舉例來說，你就抓了一隻，<br>
<br>
0:09:52.000,0:09:54.000<br>
這個是傑尼龜，<br>
<br>
0:09:54.000,0:09:56.000<br>
它進化後這個是卡咪龜，<br>
<br>
0:10:00.000,0:10:02.000<br>
卡咪龜進化是水箭龜。<br>
<br>
0:10:06.000,0:10:08.000<br>
這個function的input在這邊就是這隻傑尼龜。<br>
<br>
0:10:08.000,0:10:10.000<br>
那我們用X1來表示它。<br>
<br>
0:10:10.000,0:10:12.000<br>
我們用上標來表示一個完整的object的編號。<br>
<br>
0:10:12.000,0:10:14.000<br>
我們用上標來表示一個完整的object的編號。<br>
<br>
0:10:14.000,0:10:16.000<br>
我們用上標來表示一個完整的object的編號。<br>
<br>
0:10:16.000,0:10:18.000<br>
我們用上標來表示一個完整的object的編號。<br>
<br>
0:10:18.000,0:10:20.000<br>
剛才我們有看到用下標來表示一個 component，<br>
<br>
0:10:20.000,0:10:22.000<br>
一個完整object裡面的component。<br>
<br>
0:10:24.000,0:10:26.000<br>
我們用上標來表示一個完整object的編號，<br>
<br>
0:10:26.000,0:10:28.000<br>
所以這是第一個X<br>
<br>
0:10:28.000,0:10:30.000<br>
這是一隻傑尼龜<br>
<br>
0:10:30.000,0:10:32.000<br>
那它進化以後的CP是973<br>
<br>
0:10:34.000,0:10:36.000<br>
所以我們function的output，<br>
<br>
0:10:36.000,0:10:38.000<br>
應該看到X1就output數值973。<br>
<br>
0:10:40.000,0:10:42.000<br>
那這個973我們用Y1 hat 來代表，<br>
<br>
0:10:42.000,0:10:44.000<br>
那這個973我們用Y1 hat 來代表，<br>
<br>
0:10:46.000,0:10:48.000<br>
這邊用Y來代表function的output，用上標來代表一個完整的個體。<br>
<br>
0:10:48.000,0:10:50.000<br>
這邊用Y來代表function的output，用上標來代表一個完整的個體。<br>
<br>
0:10:50.000,0:10:52.000<br>
因為我們今天考慮的output是scalar，<br>
<br>
0:10:54.000,0:10:56.000<br>
所以它其實裡面沒有component，<br>
<br>
0:10:56.000,0:10:58.000<br>
它就是一個簡單的數值。<br>
<br>
0:10:58.000,0:11:00.000<br>
但是我們未來如果在考慮structured learning的時候，<br>
<br>
0:11:00.000,0:11:02.000<br>
我們output的object可能是有structure的。<br>
<br>
0:11:02.000,0:11:04.000<br>
我們output的object可能是有structure的。<br>
<br>
0:11:04.000,0:11:06.000<br>
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。<br>
<br>
0:11:06.000,0:11:08.000<br>
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。<br>
<br>
0:11:08.000,0:11:10.000<br>
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。<br>
<br>
0:11:12.000,0:11:14.000<br>
我們用Y hat1來表示這個數值979。<br>
<br>
0:11:18.000,0:11:20.000<br>
只有一隻不夠要收集很多。<br>
<br>
0:11:20.000,0:11:22.000<br>
比如說再抓一隻伊布，<br>
<br>
0:11:22.000,0:11:24.000<br>
那這個伊布就是X2。<br>
<br>
0:11:24.000,0:11:26.000<br>
那它進化以後可以變成雷精靈，<br>
<br>
0:11:26.000,0:11:28.000<br>
那雷精靈的CP值是1420，<br>
<br>
0:11:30.000,0:11:32.000<br>
這個1420就是Y2 hat。<br>
<br>
0:11:32.000,0:11:34.000<br>
我們用hat來代表說這個是一個正確的值，<br>
<br>
0:11:34.000,0:11:36.000<br>
我們用hat來代表說這個是一個正確的值，<br>
<br>
0:11:36.000,0:11:38.000<br>
是我們實際觀察到function該有的output。<br>
<br>
0:11:38.000,0:11:40.000<br>
是我們實際觀察到function該有的output。<br>
<br>
0:11:42.000,0:11:44.000<br>
你可能以為說這只是個例子，<br>
<br>
0:11:44.000,0:11:46.000<br>
這不只是一個例子，<br>
<br>
0:11:46.000,0:11:48.000<br>
我是有真正的data的。<br>
<br>
0:11:50.000,0:11:52.000<br>
今天其實我是想要發表，<br>
<br>
0:11:52.000,0:11:54.000<br>
我在神奇寶貝上面的研究成果這樣。<br>
<br>
0:12:00.000,0:12:02.000<br>
那我們就收集10隻神奇寶貝，<br>
<br>
0:12:06.000,0:12:08.000<br>
這10隻寶可夢就是從編號1到編號10<br>
<br>
0:12:08.000,0:12:10.000<br>
這10隻寶可夢就是從編號1到編號10<br>
<br>
0:12:10.000,0:12:12.000<br>
這10隻寶可夢就是從編號1到編號10<br>
<br>
0:12:14.000,0:12:16.000<br>
每一隻寶可夢我們都讓它進化以後，<br>
<br>
0:12:16.000,0:12:18.000<br>
我們就知道它進化後的CP值<br>
<br>
0:12:18.000,0:12:20.000<br>
就是Y1 hat 到 Y10 hat<br>
<br>
0:12:20.000,0:12:22.000<br>
就是Y1 hat 到 Y10 hat<br>
<br>
0:12:22.000,0:12:24.000<br>
這個是真正的data，<br>
<br>
0:12:24.000,0:12:26.000<br>
你可能會問說怎麼只抓10隻呢?<br>
<br>
0:12:26.000,0:12:28.000<br>
你不知道抓這個很麻煩嗎?<br>
<br>
0:12:30.000,0:12:32.000<br>
其實老實說這也不是我自己抓的，<br>
<br>
0:12:32.000,0:12:34.000<br>
網路上有人分享他抓出來的數據。<br>
<br>
0:12:34.000,0:12:36.000<br>
我拿他的數據來做一下，<br>
<br>
0:12:36.000,0:12:38.000<br>
其實他也沒有抓太多次，<br>
<br>
0:12:38.000,0:12:40.000<br>
因為抓這個其實是很麻煩的。<br>
<br>
0:12:40.000,0:12:42.000<br>
並不是抓來就好，你要把它進化以後，<br>
<br>
0:12:42.000,0:12:44.000<br>
你才知道function的output是多少。<br>
<br>
0:12:44.000,0:12:46.000<br>
所以收集這個data並沒有那麼容易。<br>
<br>
0:12:48.000,0:12:50.000<br>
所以就收集了10隻 神奇寶貝，<br>
<br>
0:12:50.000,0:12:52.000<br>
它進化後的CP值。<br>
<br>
0:12:54.000,0:12:56.000<br>
那如果我們把這十隻神奇寶貝的information畫出來的話，<br>
<br>
0:12:56.000,0:12:58.000<br>
這個圖上每一個藍色的點，<br>
<br>
0:12:58.000,0:13:00.000<br>
這個圖上每一個藍色的點，<br>
<br>
0:13:00.000,0:13:02.000<br>
代表一隻寶可夢。<br>
<br>
0:13:04.000,0:13:06.000<br>
然後他的X軸，<br>
<br>
0:13:06.000,0:13:08.000<br>
X軸代表的是這一隻寶可夢他的CP值，<br>
<br>
0:13:08.000,0:13:10.000<br>
X軸代表的是這一隻寶可夢他的CP值，<br>
<br>
0:13:10.000,0:13:12.000<br>
X軸代表的是這一隻寶可夢他的CP值，<br>
<br>
0:13:12.000,0:13:14.000<br>
這個我們一抓來的時候我們就知道了<br>
<br>
0:13:14.000,0:13:16.000<br>
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。<br>
<br>
0:13:16.000,0:13:18.000<br>
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。<br>
<br>
0:13:18.000,0:13:20.000<br>
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。<br>
<br>
0:13:20.000,0:13:22.000<br>
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。<br>
<br>
0:13:22.000,0:13:24.000<br>
這個用 ŷ 來表示。<br>
<br>
0:13:26.000,0:13:28.000<br>
所以10隻寶可夢<br>
<br>
0:13:28.000,0:13:30.000<br>
這邊我們有10個點<br>
<br>
0:13:34.000,0:13:36.000<br>
這個CP值其實就特別高，<br>
<br>
0:13:36.000,0:13:38.000<br>
這隻其實是伊布<br>
<br>
0:13:40.000,0:13:42.000<br>
伊布其實不是很容易抓的<br>
<br>
0:13:42.000,0:13:44.000<br>
這邊每一個點就是某第n隻寶可夢的CP值，<br>
<br>
0:13:46.000,0:13:48.000<br>
這邊每一個點就是某第n隻寶可夢的CP值，<br>
<br>
0:13:48.000,0:13:50.000<br>
跟它進化以後的 ŷ。<br>
<br>
0:13:50.000,0:13:52.000<br>
那我們用X上標n和下標cp，<br>
<br>
0:13:52.000,0:13:54.000<br>
來代表第n筆data，他的某一個component，也就是他的CP值。<br>
<br>
0:13:54.000,0:13:56.000<br>
來代表第n筆data，他的某一個component，也就是他的CP值。<br>
<br>
0:13:56.000,0:13:58.000<br>
來代表第n筆data，他的某一個component，也就是他的CP值。<br>
<br>
0:13:58.000,0:14:02.000<br>
接下來，<br>
<br>
0:14:02.000,0:14:04.000<br>
有了這些training data以後，<br>
<br>
0:14:04.000,0:14:06.000<br>
我們就可以定義一個function的好壞，<br>
<br>
0:14:08.000,0:14:10.000<br>
我們就可以知道一個function是多好或者是多不好，<br>
<br>
0:14:12.000,0:14:14.000<br>
知道說這裡面每一個function是多好或者是多不好。<br>
<br>
0:14:14.000,0:14:16.000<br>
怎麼做呢?<br>
<br>
0:14:16.000,0:14:18.000<br>
我們要定義一個<br>
<br>
0:14:18.000,0:14:20.000<br>
另外一個function，<br>
<br>
0:14:20.000,0:14:22.000<br>
叫做 loss function，<br>
<br>
0:14:22.000,0:14:24.000<br>
這邊寫作大寫的L<br>
<br>
0:14:24.000,0:14:26.000<br>
我們這裡定了一個function set，<br>
<br>
0:14:26.000,0:14:28.000<br>
這裡面有一大堆的function。<br>
<br>
0:14:28.000,0:14:30.000<br>
這邊我們要再定另外一個function<br>
<br>
0:14:30.000,0:14:32.000<br>
另外一個function叫做loss function，<br>
<br>
0:14:32.000,0:14:34.000<br>
我們寫成大寫的L。<br>
<br>
0:14:34.000,0:14:35.580<br>
這個loss function的input，<br>
<br>
0:14:35.580,0:14:38.000<br>
他是一個很特別function，<br>
<br>
0:14:38.000,0:14:40.000<br>
這個loss function他是function的function，<br>
<br>
0:14:40.000,0:14:42.000<br>
大家知道今天我的意思嗎?<br>
<br>
0:14:44.000,0:14:46.000<br>
他的input就是一個function，<br>
<br>
0:14:46.000,0:14:48.000<br>
他的output 就是一個數值告訴我們說<br>
<br>
0:14:48.000,0:14:50.000<br>
現在input的這個function他有多不好<br>
<br>
0:14:50.000,0:14:52.000<br>
現在input的這個function他有多不好<br>
<br>
0:14:52.000,0:14:54.000<br>
我們這邊是用多不好來表示。<br>
<br>
0:14:56.000,0:14:58.000<br>
所以這個loss function他是一個function的function，<br>
<br>
0:14:58.000,0:15:00.000<br>
他就是吃一個function當作input<br>
<br>
0:15:00.000,0:15:02.000<br>
他的output就是這個function有多不好。<br>
<br>
0:15:02.000,0:15:04.000<br>
他的output就是這個function有多不好。<br>
<br>
0:15:06.000,0:15:08.000<br>
所以你可以寫成這樣，<br>
<br>
0:15:08.000,0:15:10.000<br>
這個L他的input就是某一個function f<br>
<br>
0:15:10.000,0:15:12.000<br>
你知道一個function，<br>
<br>
0:15:12.000,0:15:14.000<br>
他又是由這個function裡面的兩個參數b和w來決定的<br>
<br>
0:15:14.000,0:15:16.000<br>
他又是由這個function裡面的兩個參數b和w來決定的<br>
<br>
0:15:16.000,0:15:18.000<br>
這個f是由b和w來決定的<br>
<br>
0:15:18.000,0:15:20.000<br>
所以input這個f，<br>
<br>
0:15:20.000,0:15:22.000<br>
就等於input這個f裡面的b和w。<br>
<br>
0:15:22.000,0:15:24.000<br>
就等於input這個f裡面的b和w。<br>
<br>
0:15:24.000,0:15:26.000<br>
所以你可以說loss function，<br>
<br>
0:15:26.000,0:15:28.000<br>
他是在衡量一組參數的好壞，<br>
<br>
0:15:28.000,0:15:30.000<br>
他是在衡量一組參數的好壞，<br>
<br>
0:15:30.000,0:15:32.000<br>
衡量一組b和w的好壞。<br>
<br>
0:15:34.000,0:15:36.000<br>
那怎麼定這個loss function呢?<br>
<br>
0:15:38.000,0:15:40.000<br>
loss function你其實可以隨自己的喜好，<br>
<br>
0:15:40.000,0:15:42.000<br>
定義一個你覺得合理的function。<br>
<br>
0:15:44.000,0:15:46.000<br>
不過我們這邊就用比較常見的作法 :<br>
<br>
0:15:46.000,0:15:48.000<br>
怎麼定呢?<br>
<br>
0:15:48.000,0:15:50.000<br>
你就把這個input的w和b，<br>
<br>
0:15:50.000,0:15:52.000<br>
你就把這個input的w和b，<br>
<br>
0:15:52.000,0:15:54.000<br>
實際地代入y=b+w × Xcp 這個function裡面。<br>
<br>
0:15:54.000,0:15:56.000<br>
實際地代入y=b+w × Xcp這個function裡面。<br>
<br>
0:16:00.000,0:16:02.000<br>
你把w乘上第n隻寶可夢的CP值，<br>
<br>
0:16:02.000,0:16:04.000<br>
你把w乘上第n隻寶可夢的CP值，<br>
<br>
0:16:04.000,0:16:06.000<br>
再加上這個constant b，<br>
<br>
0:16:06.000,0:16:08.000<br>
然後你就得到說 :<br>
<br>
0:16:08.000,0:16:10.000<br>
如果我們使用這一組w，<br>
<br>
0:16:10.000,0:16:12.000<br>
如果我們使用這一組w，<br>
<br>
0:16:12.000,0:16:14.000<br>
這一個w和b，<br>
<br>
0:16:14.000,0:16:16.000<br>
來當作我們的function，<br>
<br>
0:16:16.000,0:16:18.000<br>
來預測寶可夢它進化以後的CP值的話，<br>
<br>
0:16:18.000,0:16:20.000<br>
來預測寶可夢它進化以後的CP值的話，<br>
<br>
0:16:20.000,0:16:22.000<br>
這個預測的值Y的數值是多少。<br>
<br>
0:16:22.000,0:16:24.000<br>
這個預測的值Y的數值是多少。<br>
<br>
0:16:26.000,0:16:28.000<br>
這個裡面的括號，比較小的括號，<br>
<br>
0:16:28.000,0:16:30.000<br>
這個裡面的括號，比較小的括號，<br>
<br>
0:16:30.000,0:16:32.000<br>
他輸出的數值是我們用現在的function來預測的話，<br>
<br>
0:16:32.000,0:16:34.000<br>
他輸出的數值是我們用現在的function來預測的話，<br>
<br>
0:16:34.000,0:16:36.000<br>
他輸出的數值是我們用現在的function來預測的話，<br>
<br>
0:16:36.000,0:16:38.000<br>
我們得到的輸出是甚麼。<br>
<br>
0:16:38.000,0:16:40.000<br>
那 ŷ是真正的數值。<br>
<br>
0:16:40.000,0:16:42.000<br>
我們把真正的數值，<br>
<br>
0:16:42.000,0:16:44.000<br>
減掉預測的數值，<br>
<br>
0:16:44.000,0:16:46.000<br>
再取平方，<br>
<br>
0:16:50.000,0:16:52.000<br>
這個就是估測的誤差。<br>
<br>
0:16:54.000,0:16:56.000<br>
我們再把我們手上的10隻寶可夢的估測誤差，<br>
<br>
0:16:56.000,0:16:58.000<br>
都合起來，<br>
<br>
0:17:00.000,0:17:02.000<br>
就得到這個loss function。<br>
<br>
0:17:02.000,0:17:04.000<br>
那這個定義我相信你是不太會有問題的，<br>
<br>
0:17:04.000,0:17:06.000<br>
因為它非常的直覺估測 :<br>
<br>
0:17:06.000,0:17:08.000<br>
如果我使用某一個function，<br>
<br>
0:17:08.000,0:17:10.000<br>
它給我們的估測誤差越大，<br>
<br>
0:17:10.000,0:17:12.000<br>
那當然這個function就越不好。<br>
<br>
0:17:12.000,0:17:14.000<br>
那當然這個function就越不好。<br>
<br>
0:17:14.000,0:17:16.000<br>
所以我們就用估測誤差來定義一個loss function。<br>
<br>
0:17:16.000,0:17:18.000<br>
當然你可以選擇其他可能性。<br>
<br>
0:17:26.000,0:17:28.000<br>
再來我們有了這個loss function以後，<br>
<br>
0:17:30.000,0:17:32.000<br>
如果你還是有一些困惑的話，<br>
<br>
0:17:32.000,0:17:34.000<br>
我們可以把這個loss function的形狀畫出來。<br>
<br>
0:17:36.000,0:17:38.000<br>
這個loss function L(w,b)<br>
<br>
0:17:38.000,0:17:40.000<br>
它input就是兩個參數w和b<br>
<br>
0:17:42.000,0:17:44.000<br>
所以我們可以把這個L(w,b)對w和b把它做圖<br>
<br>
0:17:44.000,0:17:46.000<br>
所以我們可以把這個L(w,b)對w和b把它做圖<br>
<br>
0:17:46.000,0:17:48.000<br>
所以我們可以把這個L(w,b)對w和b把它做圖<br>
<br>
0:17:48.000,0:17:50.860<br>
把它畫出來。<br>
<br>
0:17:50.860,0:17:52.000<br>
在這個圖上的每一個點，<br>
<br>
0:17:52.000,0:17:54.000<br>
就代表著一個組w跟b，<br>
<br>
0:17:54.000,0:17:56.000<br>
也就是代表某一個function。<br>
<br>
0:17:56.000,0:17:58.000<br>
比如說，紅色這個點，<br>
<br>
0:17:58.000,0:18:00.000<br>
紅色這個點就代表著，<br>
<br>
0:18:00.000,0:18:02.000<br>
這個b=-180， 這個w=-2的時候所得到的function。<br>
<br>
0:18:02.000,0:18:04.000<br>
這個b=-180， 這個w=-2時候所得到的function。<br>
<br>
0:18:04.000,0:18:06.000<br>
這個b=-180， 這個w=-2時候所得到的function。<br>
<br>
0:18:08.000,0:18:10.000<br>
就y= -180−2×Xcp這個function。<br>
<br>
0:18:10.000,0:18:12.000<br>
就y= -180−2×Xcp這個function。<br>
<br>
0:18:12.000,0:18:14.000<br>
這圖上每一個點都代表著一個function。<br>
<br>
0:18:16.000,0:18:18.000<br>
顏色代表了，<br>
<br>
0:18:18.000,0:18:20.000<br>
現在如果我們使用這個function，<br>
<br>
0:18:22.000,0:18:24.000<br>
根據我們定義的loss function，<br>
<br>
0:18:24.000,0:18:26.000<br>
它有多糟糕，<br>
<br>
0:18:26.000,0:18:28.000<br>
它有多不好。<br>
<br>
0:18:30.000,0:18:32.000<br>
這個顏色越偏紅色代表數值越大，<br>
<br>
0:18:34.000,0:18:36.000<br>
所以在這一群的function，<br>
<br>
0:18:36.000,0:18:38.000<br>
他們loss非常大，也就是它們是一群不好的function。<br>
<br>
0:18:40.000,0:18:42.000<br>
最好的function落在哪裡呢?<br>
<br>
0:18:42.000,0:18:44.000<br>
越偏藍色代表那一個function越好<br>
<br>
0:18:44.000,0:18:46.000<br>
越偏藍色代表那一個function越好<br>
<br>
0:18:48.000,0:18:50.000<br>
所以最好的function其實落在這個位子<br>
<br>
0:18:50.000,0:18:52.000<br>
如果你選這個function的話，<br>
<br>
0:18:52.000,0:18:54.000<br>
它是可以讓你loss最低的一個function。<br>
<br>
0:18:54.000,0:18:56.000<br>
它是可以讓你loss最低的一個function。<br>
<br>
0:19:00.000,0:19:02.000<br>
接下來，我們已經定好了我們的loss function，<br>
<br>
0:19:02.000,0:19:04.000<br>
接下來，我們已經定好了我們的loss function，<br>
<br>
0:19:06.000,0:19:08.000<br>
可以衡量我們的model裡面每一個function的好壞。<br>
<br>
0:19:10.000,0:19:12.000<br>
接下來我們要做的事情就是，<br>
<br>
0:19:12.000,0:19:14.000<br>
從這個function set裡面，<br>
<br>
0:19:14.000,0:19:16.000<br>
從這個function set裡面，<br>
<br>
0:19:16.000,0:19:18.000<br>
挑選一個最好的function。<br>
<br>
0:19:18.000,0:19:20.000<br>
挑選一個最好的function。<br>
<br>
0:19:22.000,0:19:24.000<br>
所謂挑選最好的function這一件事情，<br>
<br>
0:19:26.000,0:19:28.000<br>
如果你想要把它寫成formulation的話，<br>
<br>
0:19:28.000,0:19:30.000<br>
如果你想要把它寫成equation的話，<br>
<br>
0:19:30.000,0:19:32.000<br>
那寫起來是甚麼樣子呢?<br>
<br>
0:19:32.000,0:19:36.000<br>
它寫起來就是，你要我們定的那個loss function長這樣，<br>
<br>
0:19:38.000,0:19:40.000<br>
那你要找一個f，<br>
<br>
0:19:40.000,0:19:42.000<br>
它可以讓L(f)最小，<br>
<br>
0:19:44.000,0:19:46.000<br>
這個可以讓L(f)最小的function，<br>
<br>
0:19:46.000,0:19:48.000<br>
我們就寫成f*。<br>
<br>
0:19:48.000,0:19:50.000<br>
或者是，<br>
<br>
0:19:50.000,0:19:52.000<br>
我們知道f是由兩個參數w和b表示，<br>
<br>
0:19:52.000,0:19:54.000<br>
我們知道f是由兩個參數w和b表示，<br>
<br>
0:19:54.000,0:19:56.000<br>
今天要做的事情就是，<br>
<br>
0:19:56.000,0:19:58.000<br>
窮舉所有的w和b，<br>
<br>
0:19:58.000,0:20:00.000<br>
看哪一個w和b代入L(w,b)，<br>
<br>
0:20:02.000,0:20:04.000<br>
可以讓這個loss的值最小。<br>
<br>
0:20:06.000,0:20:08.000<br>
那這一個w跟b就是最好的w跟b，<br>
<br>
0:20:08.000,0:20:10.000<br>
那麼寫成w*<br>
<br>
0:20:10.000,0:20:12.000<br>
跟b*。<br>
<br>
0:20:12.000,0:20:14.000<br>
或者是我們把L這個function列出來，<br>
<br>
0:20:16.000,0:20:18.000<br>
L這個function我們知道它長的就是這個樣子，<br>
<br>
0:20:20.000,0:20:22.000<br>
那我們就是把w和b，用各種不同的數值代到這個function裡面，<br>
<br>
0:20:22.000,0:20:24.000<br>
看哪一組w跟b，可以給我們最好的結果。<br>
<br>
0:20:26.840,0:20:28.000<br>
如果你修過線性代數的話，<br>
<br>
0:20:30.000,0:20:32.000<br>
其實這個對你來說應該完全不是問題，對不對?<br>
<br>
0:20:32.000,0:20:34.000<br>
這個是有closed-form solution的<br>
<br>
0:20:34.000,0:20:36.000<br>
像我相信你可能不記得它長甚麼樣子了<br>
<br>
0:20:38.000,0:20:40.000<br>
所謂的closed-form solution意思是說，<br>
<br>
0:20:42.000,0:20:44.000<br>
你只要把10隻寶可夢的CP值<br>
<br>
0:20:44.000,0:20:46.000<br>
跟他們進化後的ŷ<br>
<br>
0:20:48.000,0:20:50.000<br>
你只要把這些數值代到某一個function裡面，<br>
<br>
0:20:52.000,0:20:54.000<br>
它output就可以告訴你最好的w和b是甚麼。<br>
<br>
0:20:54.000,0:20:56.000<br>
如果你修過線代的話，<br>
<br>
0:20:56.000,0:20:58.000<br>
其實你理論上是應該是知道要怎麼做的。<br>
<br>
0:21:00.000,0:21:02.000<br>
我假設你已經忘記了，<br>
<br>
0:21:02.000,0:21:04.000<br>
那我們要教你另外一個做法。<br>
<br>
0:21:04.000,0:21:06.000<br>
這個做法叫做<br>
<br>
0:21:06.000,0:21:08.000<br>
gradient descent。<br>
<br>
0:21:08.000,0:21:10.000<br>
這邊要強調的是，gradient descent<br>
<br>
0:21:10.000,0:21:12.000<br>
不是只適用於解這一個function。<br>
<br>
0:21:12.000,0:21:14.000<br>
不是只適用於解這一個function。<br>
<br>
0:21:14.000,0:21:16.000<br>
解這一個function是比較容易的，<br>
<br>
0:21:16.000,0:21:18.000<br>
你有修過線代你其實就會了。<br>
<br>
0:21:18.000,0:21:20.000<br>
但是gradient descent它厲害的地方是，<br>
<br>
0:21:22.000,0:21:24.000<br>
只要你這個L是可微分的<br>
<br>
0:21:24.000,0:21:26.000<br>
不管它是甚麼function，<br>
<br>
0:21:28.000,0:21:30.000<br>
gradient descent都可以拿來處理這個function，<br>
<br>
0:21:32.000,0:21:34.000<br>
都可以拿來幫你找可能是比較好的function或者是參數。<br>
<br>
0:21:34.000,0:21:36.000<br>
都可以拿來幫你找可能是比較好的function或者是參數。<br>
<br>
0:21:38.000,0:21:40.000<br>
那我們來看一下gradient descent是怎麼做?<br>
<br>
0:21:40.000,0:21:42.000<br>
我們先假設一個比較簡單的task，<br>
<br>
0:21:44.000,0:21:46.000<br>
在這個比較簡單的task裡面，<br>
<br>
0:21:46.000,0:21:48.000<br>
我們的loss function L(w)，<br>
<br>
0:21:48.000,0:21:50.000<br>
我們的loss function L(w)，<br>
<br>
0:21:50.000,0:21:52.000<br>
它只有一個參數w。<br>
<br>
0:21:52.000,0:21:54.000<br>
那這個L(w)必然是不需要是我們之前定出來的那個loss function，<br>
<br>
0:21:54.000,0:21:56.000<br>
那這個L(w)必然是不需要是我們之前定出來的那個loss function，<br>
<br>
0:21:56.000,0:21:58.000<br>
它可以是任何function，<br>
<br>
0:21:58.000,0:22:00.000<br>
只要是可微分的就行了。<br>
<br>
0:22:00.000,0:22:02.000<br>
那我們現在要解的問題是，<br>
<br>
0:22:02.000,0:22:04.000<br>
找一個w，<br>
<br>
0:22:04.000,0:22:06.000<br>
讓這個L(w)最小。<br>
<br>
0:22:06.000,0:22:08.000<br>
這件事情怎麼做呢?<br>
<br>
0:22:08.000,0:22:10.000<br>
那暴力的方法就是，<br>
<br>
0:22:10.000,0:22:12.000<br>
窮舉所有w可能的數字，<br>
<br>
0:22:12.000,0:22:14.000<br>
把所有w可能的數值，<br>
<br>
0:22:14.000,0:22:16.000<br>
從負無限大到無限大，<br>
<br>
0:22:16.000,0:22:18.000<br>
一個一個值都代到loss function裡面去，<br>
<br>
0:22:18.000,0:22:20.000<br>
試一下這個loss function的value，<br>
<br>
0:22:20.000,0:22:22.000<br>
你就會知道說，哪一個w的值，<br>
<br>
0:22:22.000,0:22:24.000<br>
可以讓loss最小。<br>
<br>
0:22:24.000,0:22:26.000<br>
如果你做這件事的話，你就會發現說，<br>
<br>
0:22:26.000,0:22:28.000<br>
比如說這裡這個w的值，<br>
<br>
0:22:28.000,0:22:30.000<br>
可以讓loss最小。<br>
<br>
0:22:30.000,0:22:32.000<br>
但是這樣做是沒有效率的，<br>
<br>
0:22:32.000,0:22:34.000<br>
怎麼做比較有效率呢?<br>
<br>
0:22:34.000,0:22:36.000<br>
這就是gradient descent要告訴我們的。<br>
<br>
0:22:36.000,0:22:38.000<br>
這個作法是這樣子的 :<br>
<br>
0:22:40.000,0:22:40.500<br>
我們首先隨機選取一個初始的點。<br>
<br>
0:22:42.000,0:22:44.000<br>
我們首先隨機選取一個初始的點。<br>
<br>
0:22:46.000,0:22:48.000<br>
比如這邊隨機選取的是，<br>
<br>
0:22:48.000,0:22:50.000<br>
w0<br>
<br>
0:22:54.000,0:22:56.000<br>
其實你也不一定要隨機選取，<br>
<br>
0:22:56.000,0:22:58.000<br>
其實有可能有一些其他的方法，<br>
<br>
0:22:58.000,0:23:00.000<br>
可以讓你找的值是比較好的。<br>
<br>
0:23:00.000,0:23:02.000<br>
這個之後再提，<br>
<br>
0:23:02.000,0:23:04.000<br>
現在就想成是，<br>
<br>
0:23:04.000,0:23:06.000<br>
隨機選取一個初始的點w00<br>
<br>
0:23:06.000,0:23:08.000<br>
接下來，<br>
<br>
0:23:08.000,0:23:10.000<br>
在這個初始的w0這個位置，<br>
<br>
0:23:10.000,0:23:12.000<br>
我們去計算一下，<br>
<br>
0:23:12.000,0:23:14.000<br>
w這個參數<br>
<br>
0:23:24.000,0:23:26.000<br>
我們要計算在w=w0這個位置，<br>
<br>
0:23:26.000,0:23:28.000<br>
我們要計算在w=w0這個位置，<br>
<br>
0:23:28.000,0:23:30.000<br>
參數w對loss function的微分。<br>
<br>
0:23:30.000,0:23:32.000<br>
參數w對loss function的微分。<br>
<br>
0:23:34.000,0:23:36.000<br>
如果你對微分不熟的話，<br>
<br>
0:23:36.000,0:23:38.000<br>
反正我們這邊要找的就是切線斜率。<br>
<br>
0:23:40.000,0:23:42.000<br>
如果今天這個切線斜率是負的的話，<br>
<br>
0:23:42.000,0:23:44.000<br>
如果今天這個切線斜率是負的的話，<br>
<br>
0:23:44.000,0:23:46.000<br>
那顯然就是，<br>
<br>
0:23:46.000,0:23:48.000<br>
所以從這個圖上就可以很明顯地看到<br>
<br>
0:23:48.000,0:23:50.000<br>
如果切線斜率是負的的話，<br>
<br>
0:23:50.000,0:23:52.000<br>
顯然左邊loss是比較高的，<br>
<br>
0:23:52.000,0:23:54.000<br>
右邊loss是比較低的，<br>
<br>
0:23:54.000,0:23:56.000<br>
那我們要找loss比較低的function，<br>
<br>
0:23:58.000,0:24:00.000<br>
所以你應該增加你的w值，<br>
<br>
0:24:00.000,0:24:02.000<br>
你應該增加w0值。<br>
<br>
0:24:04.000,0:24:06.000<br>
反之，如果今天算出來的斜率是正的，<br>
<br>
0:24:06.000,0:24:08.000<br>
代表跟這條虛線反向，<br>
<br>
0:24:08.000,0:24:08.960<br>
也就是右邊高左邊低的話，<br>
<br>
0:24:08.960,0:24:09.460<br>
也就是右邊高左邊低的話，<br>
<br>
0:24:09.460,0:24:10.000<br>
也就是右邊高左邊低的話，<br>
<br>
0:24:10.000,0:24:12.000<br>
也就是右邊高左邊低的話，<br>
<br>
0:24:12.000,0:24:14.000<br>
那我們顯然應該減少w的值。<br>
<br>
0:24:14.000,0:24:15.000<br>
把我們的參數往左邊移動，<br>
<br>
0:24:15.000,0:24:16.000<br>
把我們的參數往左邊移動，<br>
<br>
0:24:16.000,0:24:18.000<br>
把我們的參數減小。<br>
<br>
0:24:18.000,0:24:18.500<br>
或者是，<br>
<br>
0:24:18.500,0:24:20.000<br>
假如你對微分也不熟切線也不熟的話，<br>
<br>
0:24:20.000,0:24:22.000<br>
假如你對微分也不熟切線也不熟的話，<br>
<br>
0:24:22.000,0:24:22.920<br>
那你就想成是，<br>
<br>
0:24:22.920,0:24:24.000<br>
有一個人，<br>
<br>
0:24:24.000,0:24:26.000<br>
站在w0這個點。<br>
<br>
0:24:26.000,0:24:27.920<br>
他往前後各窺視了一下，<br>
<br>
0:24:28.000,0:24:30.000<br>
他往前後各窺視了一下，<br>
<br>
0:24:30.000,0:24:32.000<br>
看一下他往左邊走一步，<br>
<br>
0:24:32.000,0:24:34.000<br>
loss會減少，<br>
<br>
0:24:34.000,0:24:36.000<br>
往左邊走一步loss會減少，<br>
<br>
0:24:36.000,0:24:36.880<br>
還是往右邊走一步，<br>
<br>
0:24:36.880,0:24:38.000<br>
loss會減少。<br>
<br>
0:24:38.000,0:24:40.000<br>
如果往右邊走一步loss會減少的話，<br>
<br>
0:24:40.000,0:24:42.000<br>
那他就會往右邊走一步。<br>
<br>
0:24:42.000,0:24:43.100<br>
總之在這個例子裏面，<br>
<br>
0:24:43.100,0:24:44.000<br>
我們的參數是增加的，<br>
<br>
0:24:44.000,0:24:46.760<br>
我們的參數是會增加的，<br>
<br>
0:24:46.760,0:24:47.600<br>
會往右邊移動。<br>
<br>
0:24:48.000,0:24:48.500<br>
那怎麼增加呢?<br>
<br>
0:24:50.000,0:24:52.000<br>
應該要增加多少呢?<br>
<br>
0:24:52.000,0:24:53.080<br>
這邊的增加量，<br>
<br>
0:24:53.080,0:24:54.000<br>
我們寫成<br>
<br>
0:24:56.000,0:24:58.000<br>
有關gradient descent 的theory，<br>
<br>
0:24:58.000,0:25:00.000<br>
我們留到下次再講，<br>
<br>
0:25:00.000,0:25:02.000<br>
我們今天就講一下它的操作是甚麼樣子。<br>
<br>
0:25:04.000,0:25:06.000<br>
如果我們往右邊踏一步的話，<br>
<br>
0:25:06.000,0:25:08.000<br>
應該要踏多少呢?<br>
<br>
0:25:08.000,0:25:10.000<br>
這個踏一步的step size，<br>
<br>
0:25:10.000,0:25:12.000<br>
取決於兩件事。<br>
<br>
0:25:12.000,0:25:14.000<br>
第一件事情是，<br>
<br>
0:25:14.000,0:25:16.000<br>
現在的微分值有多大。<br>
<br>
0:25:16.000,0:25:18.000<br>
現在的dL/dw有多大<br>
<br>
0:25:18.000,0:25:20.000<br>
現在的dL/dw有多大<br>
<br>
0:25:20.000,0:25:22.000<br>
如果微分值越大，<br>
<br>
0:25:22.000,0:25:24.000<br>
代表現在在一個越陡峭的地方，<br>
<br>
0:25:24.000,0:25:26.000<br>
那它的移動的距離就越大，<br>
<br>
0:25:26.000,0:25:28.000<br>
反之就越小。<br>
<br>
0:25:28.000,0:25:30.000<br>
反之就越小。<br>
<br>
0:25:30.000,0:25:32.000<br>
那還取決於另外一件事情，<br>
<br>
0:25:32.000,0:25:32.900<br>
這個另外一件事情，<br>
<br>
0:25:32.900,0:25:34.000<br>
是一個常數項，<br>
<br>
0:25:34.000,0:25:36.000<br>
是一個常數項。<br>
<br>
0:25:36.000,0:25:38.000<br>
這個常數項這個η，<br>
<br>
0:25:38.000,0:25:40.000<br>
我們把它叫做learning rate。<br>
<br>
0:25:40.000,0:25:42.000<br>
這個learning rate決定說，<br>
<br>
0:25:42.000,0:25:44.000<br>
我們今天踏一步，<br>
<br>
0:25:44.000,0:25:46.000<br>
不只是取決於我們現在微分值算出來有多大，<br>
<br>
0:25:46.000,0:25:48.000<br>
不只是取決於我們現在微分值算出來有多大，<br>
<br>
0:25:48.000,0:25:50.000<br>
還取決於我們一個<br>
<br>
0:25:50.000,0:25:52.000<br>
事先就定好的數值。<br>
<br>
0:25:52.000,0:25:54.000<br>
這個learning rate是一個事先定好的數值。<br>
<br>
0:25:54.000,0:25:56.000<br>
如果這個事先定好的數值你給它定大一點的話，<br>
<br>
0:25:56.000,0:25:58.000<br>
如果這個事先定好的數值你給它定大一點的話，<br>
<br>
0:25:58.000,0:26:00.000<br>
那今天踏出一步的時候，<br>
<br>
0:26:00.000,0:26:02.000<br>
參數更新的幅度就比較大，<br>
<br>
0:26:02.000,0:26:04.000<br>
反之參數更新的幅度就比較小。<br>
<br>
0:26:04.000,0:26:06.000<br>
如果參數更新的幅度比較大的話，<br>
<br>
0:26:06.000,0:26:08.000<br>
你learning rate大一點的話，<br>
<br>
0:26:08.000,0:26:10.000<br>
那學習的效率，學習的速度就比較快。<br>
<br>
0:26:10.000,0:26:12.000<br>
那學習的效率，學習的速度就比較快。<br>
<br>
0:26:12.000,0:26:14.000<br>
所以這個參數η，<br>
<br>
0:26:14.000,0:26:16.000<br>
我們就稱之為learning rate。<br>
<br>
0:26:16.000,0:26:18.000<br>
所以，<br>
<br>
0:26:18.000,0:26:20.000<br>
現在我們已經算出，<br>
<br>
0:26:20.000,0:26:22.000<br>
在w0這個地方，<br>
<br>
0:26:22.000,0:26:24.000<br>
我們應該把參數更新η乘上dL/dw<br>
<br>
0:26:24.000,0:26:26.000<br>
我們應該把參數更新η乘上dL/dw<br>
<br>
0:26:26.000,0:26:28.000<br>
我們應該把參數更新η乘上dL/dw<br>
<br>
0:26:28.000,0:26:30.000<br>
我們應該把參數更新η乘上dL/dw<br>
<br>
0:26:30.000,0:26:32.000<br>
所以你就把原來的參數w0減掉η乘以dL/dw<br>
<br>
0:26:32.000,0:26:34.000<br>
所以你就把原來的參數w0減掉η乘以dL/dw<br>
<br>
0:26:34.000,0:26:36.000<br>
所以你就把原來的參數w0減掉η乘以dL/dw<br>
<br>
0:26:36.000,0:26:38.000<br>
所以你就把原來的參數w0減掉η乘以dL/dw<br>
<br>
0:26:38.000,0:26:40.000<br>
這邊會有一項減的，<br>
<br>
0:26:42.000,0:26:44.000<br>
因為如果我們這個微分算出來是負的的話，<br>
<br>
0:26:46.000,0:26:48.000<br>
要增加這個w的值，<br>
<br>
0:26:48.000,0:26:48.840<br>
如果算出來是正的的話，<br>
<br>
0:26:48.840,0:26:50.000<br>
要減少w的值。<br>
<br>
0:26:52.000,0:26:54.000<br>
所以這一項微分值，跟我們的增加減少是反向的，<br>
<br>
0:26:54.000,0:26:56.000<br>
所以我們前面需要乘以一個負號。<br>
<br>
0:26:58.000,0:27:00.000<br>
那我們把w0更新以後，<br>
<br>
0:27:00.000,0:27:02.000<br>
變成w1，<br>
<br>
0:27:02.000,0:27:04.000<br>
接下來就是重複剛才看到的步驟，<br>
<br>
0:27:04.000,0:27:06.000<br>
接下來就是重複剛才看到的步驟，<br>
<br>
0:27:06.000,0:27:08.000<br>
重新去計算一次<br>
<br>
0:27:08.000,0:27:10.000<br>
在w=w1這個地方，<br>
<br>
0:27:10.000,0:27:12.000<br>
所算出來的微分值。<br>
<br>
0:27:14.000,0:27:16.000<br>
假設這個微分值算出來是這樣子的，<br>
<br>
0:27:16.000,0:27:18.000<br>
這個微分值仍然建議我們，<br>
<br>
0:27:18.000,0:27:20.000<br>
應該往右移動我們的參數，<br>
<br>
0:27:20.000,0:27:22.000<br>
只是現在移動的幅度，<br>
<br>
0:27:22.000,0:27:24.000<br>
可能是比較小的。<br>
<br>
0:27:24.000,0:27:26.000<br>
因為這個微分值，<br>
<br>
0:27:26.000,0:27:28.000<br>
相較於前面這一項，<br>
<br>
0:27:28.000,0:27:30.000<br>
是比較小的。<br>
<br>
0:27:30.000,0:27:32.000<br>
那你就把w1−ηdL/dw，<br>
<br>
0:27:32.000,0:27:34.000<br>
然後變成w2。<br>
<br>
0:27:36.000,0:27:38.000<br>
那這個步驟就反覆不斷地執行下去，<br>
<br>
0:27:38.000,0:27:40.000<br>
那這個步驟就反覆不斷地執行下去，<br>
<br>
0:27:42.000,0:27:44.000<br>
經過非常非常多的iteration後，<br>
<br>
0:27:44.000,0:27:46.000<br>
經過非常多次的參數更新以後，<br>
<br>
0:27:48.000,0:27:50.000<br>
假設經過t次的更新，<br>
<br>
0:27:50.000,0:27:52.000<br>
這個t是一個非常大的數字，<br>
<br>
0:27:52.000,0:27:54.000<br>
最後你會到一個local minimum的地方。<br>
<br>
0:27:54.000,0:27:56.000<br>
所謂local minimum的地方，<br>
<br>
0:27:56.000,0:27:58.000<br>
就是這個地方的微分是0，<br>
<br>
0:27:58.000,0:28:00.000<br>
就是這個地方的微分是0，<br>
<br>
0:28:00.000,0:28:02.000<br>
所以你接下來算出的微分都是0了，<br>
<br>
0:28:02.000,0:28:04.000<br>
所以你的參數接下來就會卡在這邊，<br>
<br>
0:28:04.000,0:28:06.000<br>
就沒有辦法再更新了。<br>
<br>
0:28:06.000,0:28:08.000<br>
這件事情你可能會覺得不太高興，<br>
<br>
0:28:10.000,0:28:12.000<br>
因為這邊其實有一個local minimum，<br>
<br>
0:28:14.000,0:28:16.960<br>
你找出來的跟gradient descent找出來的solution，<br>
<br>
0:28:16.960,0:28:18.000<br>
你找出來的參數，<br>
<br>
0:28:18.000,0:28:20.000<br>
它其實不是最佳解。<br>
<br>
0:28:20.000,0:28:23.000<br>
你只能找到 local minimum，<br>
<br>
0:28:23.000,0:28:24.000<br>
你沒有辦法找到global minimum。<br>
<br>
0:28:24.000,0:28:26.000<br>
但幸運的是，這件事情在regression上面，<br>
<br>
0:28:26.000,0:28:28.000<br>
不是一個問題。<br>
<br>
0:28:28.000,0:28:30.000<br>
因為在regression上面，<br>
<br>
0:28:30.000,0:28:32.000<br>
在linear regression上面，<br>
<br>
0:28:32.000,0:28:34.000<br>
它是沒有local minimum，<br>
<br>
0:28:34.000,0:28:36.000<br>
等下這種事情我們會再看到。<br>
<br>
0:28:38.000,0:28:40.000<br>
今天我們剛才討論的是，<br>
<br>
0:28:40.000,0:28:41.940<br>
只有一個參數的情形，<br>
<br>
0:28:42.000,0:28:44.000<br>
那如果是有兩個參數呢<br>
<br>
0:28:44.000,0:28:46.000<br>
那如果是有兩個參數呢<br>
<br>
0:28:46.000,0:28:48.000<br>
我們今天真正要處理的事情，<br>
<br>
0:28:48.000,0:28:50.000<br>
是有兩個參數的問題，<br>
<br>
0:28:50.000,0:28:52.000<br>
也就是w跟b。<br>
<br>
0:28:52.000,0:28:54.000<br>
其實有兩個參數，<br>
<br>
0:28:54.000,0:28:56.000<br>
從一個參數推廣到兩個參數，<br>
<br>
0:28:56.000,0:28:58.000<br>
其實是沒有任何不同的。<br>
<br>
0:28:58.000,0:29:00.000<br>
首先你就隨機選取兩個初始值，<br>
<br>
0:29:00.000,0:29:02.000<br>
首先你就隨機選取兩個初始值，<br>
<br>
0:29:02.000,0:29:04.000<br>
w0和b0，<br>
<br>
0:29:04.000,0:29:06.000<br>
接下來，<br>
<br>
0:29:06.000,0:29:08.000<br>
你就計算，<br>
<br>
0:29:08.000,0:29:10.000<br>
在w=w0，b=b0的時候<br>
<br>
0:29:12.000,0:29:14.000<br>
w對loss的偏微分，<br>
<br>
0:29:14.000,0:29:16.000<br>
你在計算w=w0，b=b0的時候，<br>
<br>
0:29:16.000,0:29:18.000<br>
你在計算w=w0，b=b0的時候，<br>
<br>
0:29:18.000,0:29:20.000<br>
b對L的偏微分。<br>
<br>
0:29:22.000,0:29:24.000<br>
接下來，你計算出這兩個偏微分之後，<br>
<br>
0:29:24.000,0:29:26.000<br>
你就分別去更新w0和b0這兩個參數，<br>
<br>
0:29:26.000,0:29:28.000<br>
你就分別去更新w0和b0這兩個參數，<br>
<br>
0:29:28.000,0:29:30.000<br>
你就把w0減掉η乘上w對L的偏微分，<br>
<br>
0:29:30.000,0:29:32.000<br>
你就把w0減掉η乘上w對L的偏微分，<br>
<br>
0:29:32.000,0:29:34.000<br>
你就把w0減掉η乘上w對L的偏微分，<br>
<br>
0:29:34.000,0:29:36.000<br>
得到w1。<br>
<br>
0:29:36.000,0:29:38.000<br>
你就把b0減掉η乘上b對L的偏微分，<br>
<br>
0:29:38.000,0:29:40.000<br>
你就把b0減掉η乘上b對L的偏微分，<br>
<br>
0:29:40.000,0:29:42.000<br>
你就把b0減掉η乘上b對L的偏微分，<br>
<br>
0:29:42.000,0:29:44.000<br>
你就得到b1。<br>
<br>
0:29:44.000,0:29:46.000<br>
這個步驟你就反覆的持續下去，<br>
<br>
0:29:46.000,0:29:48.000<br>
這個步驟你就反覆的持續下去，<br>
<br>
0:29:48.000,0:29:50.000<br>
接下來，<br>
<br>
0:29:50.000,0:29:52.000<br>
你算出b1和w1以後，<br>
<br>
0:29:52.000,0:29:54.000<br>
你就再計算一次w和L 的偏微分<br>
<br>
0:29:54.000,0:29:56.000<br>
只是現在是計算w=w1，b=b1的時候的偏微分，<br>
<br>
0:29:56.000,0:29:58.000<br>
只是現在是計算w=w1，b=b1的時候的偏微分，<br>
<br>
0:29:58.000,0:30:00.000<br>
所以這項偏微分跟這項偏微分的值不是一樣的，<br>
<br>
0:30:00.000,0:30:02.000<br>
這是在不同位置算出來的。<br>
<br>
0:30:02.000,0:30:04.000<br>
接下來你有了w1和b1以後，<br>
<br>
0:30:04.000,0:30:06.000<br>
接下來你有了w1和b1以後，<br>
<br>
0:30:06.000,0:30:10.000<br>
你就計算w1和b1在w=w1,b=b1的時候，<br>
<br>
0:30:10.000,0:30:12.000<br>
w對L的偏微分，<br>
<br>
0:30:12.000,0:30:14.000<br>
還有w1和b1在w=w1,b=b1的時候，<br>
<br>
0:30:14.000,0:30:16.000<br>
b對L的偏微分。<br>
<br>
0:30:18.000,0:30:20.000<br>
接下來你就更新參數，<br>
<br>
0:30:20.000,0:30:22.000<br>
你就把w1減掉η乘上算出來的微分值，<br>
<br>
0:30:22.000,0:30:24.000<br>
你就把w1減掉η乘上算出來的微分值，<br>
<br>
0:30:24.000,0:30:26.000<br>
你就得到w2。<br>
<br>
0:30:26.000,0:30:28.000<br>
你把b1減掉η乘上微分值就得到b2。<br>
<br>
0:30:28.000,0:30:30.000<br>
你把b1減掉η乘上微分值就得到b2。<br>
<br>
0:30:30.000,0:30:32.000<br>
你就反覆進行這個步驟，<br>
<br>
0:30:34.000,0:30:36.000<br>
最後你就可以找到一個loss相對比較小的w值跟b的值，<br>
<br>
0:30:36.000,0:30:38.000<br>
最後你就可以找到一個loss相對比較小的w值跟b的值，<br>
<br>
0:30:38.000,0:30:40.000<br>
這邊要補充說明的是，<br>
<br>
0:30:40.000,0:30:42.000<br>
所謂的gradient descent的gradient指的是甚麼呢?<br>
<br>
0:30:42.000,0:30:44.000<br>
所謂的gradient descent的gradient指的是甚麼呢?<br>
<br>
0:30:44.000,0:30:46.000<br>
其實gradient就是這個倒三角   ∇L<br>
<br>
0:30:46.000,0:30:48.000<br>
其實gradient就是這個倒三角   ∇L<br>
<br>
0:30:48.000,0:30:50.000<br>
我知道大家已經，<br>
<br>
0:30:50.000,0:30:52.000<br>
很久沒有學微積分了，<br>
<br>
0:30:52.000,0:30:54.000<br>
所以我猜你八成不記得∇L是甚麼。<br>
<br>
0:30:54.000,0:30:56.000<br>
這個∇L就是，<br>
<br>
0:30:58.000,0:31:00.000<br>
你把w對L的偏微分和b對L的偏微分排成一個vector，<br>
<br>
0:31:00.000,0:31:02.000<br>
你把w對L的偏微分和b對L的偏微分排成一個vector，<br>
<br>
0:31:02.000,0:31:04.000<br>
你把w對L的偏微分和b對L的偏微分排成一個vector，<br>
<br>
0:31:04.000,0:31:06.000<br>
你把w對L的偏微分和b對L的偏微分排成一個vector，<br>
<br>
0:31:06.000,0:31:08.000<br>
這一項就是gradient。<br>
<br>
0:31:08.000,0:31:10.000<br>
因為我們在整個process裡面，<br>
<br>
0:31:10.000,0:31:12.000<br>
我們要計算w對L的偏微分和b對L的偏微分，<br>
<br>
0:31:12.000,0:31:14.000<br>
我們要計算w對L的偏微分和b對L的偏微分，<br>
<br>
0:31:14.000,0:31:16.000<br>
這個就是gradient。<br>
<br>
0:31:16.000,0:31:18.000<br>
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。<br>
<br>
0:31:18.000,0:31:20.000<br>
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。<br>
<br>
0:31:20.000,0:31:22.000<br>
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。<br>
<br>
0:31:22.000,0:31:24.000<br>
只是想要讓大家知道說gradient指的就是這個東西。<br>
<br>
0:31:26.000,0:31:28.000<br>
那我們來visualize一下剛才做的事情。<br>
<br>
0:31:28.000,0:31:30.000<br>
那我們來visualize一下剛才做的事情。<br>
<br>
0:31:30.000,0:31:32.000<br>
剛才做的事情像是這樣 :<br>
<br>
0:31:32.000,0:31:34.000<br>
有兩個參數w和b，<br>
<br>
0:31:34.000,0:31:36.000<br>
有兩個參數w和b，<br>
<br>
0:31:36.000,0:31:38.000<br>
這兩個參數決定了一個function長甚麼樣。<br>
<br>
0:31:40.000,0:31:42.000<br>
那在這個圖上的顏色，<br>
<br>
0:31:42.000,0:31:44.000<br>
這個圖上的顏色代表loss function的數值。<br>
<br>
0:31:44.000,0:31:46.000<br>
這個圖上的顏色代表loss function的數值。<br>
<br>
0:31:46.000,0:31:48.000<br>
這個圖上的顏色代表loss function的數值。<br>
<br>
0:31:48.000,0:31:50.000<br>
越偏藍色代表loss越小，<br>
<br>
0:31:50.000,0:31:52.000<br>
那我們隨機選取一個初始值，<br>
<br>
0:31:54.000,0:31:56.000<br>
是在左下角紅色的點這個地方。<br>
<br>
0:31:56.000,0:31:56.660<br>
接下來，<br>
<br>
0:31:56.660,0:31:58.000<br>
你就去計算，<br>
<br>
0:31:58.000,0:32:00.000<br>
在紅色這個點，<br>
<br>
0:32:08.000,0:32:10.000<br>
b對loss的偏微分還有w對loss的偏微分<br>
<br>
0:32:12.000,0:32:14.000<br>
然後你就把參數更新，<br>
<br>
0:32:14.000,0:32:16.000<br>
這個η乘上b對loss的偏微分。<br>
<br>
0:32:16.000,0:32:20.000<br>
還有η乘上w對loss的偏微分。<br>
<br>
0:32:20.000,0:32:22.000<br>
還有η乘上w對loss的偏微分。<br>
<br>
0:32:24.000,0:32:26.000<br>
如果你對偏微分比較不熟的話，<br>
<br>
0:32:26.000,0:32:26.840<br>
其實這個方向，<br>
<br>
0:32:28.000,0:32:30.000<br>
這個gradient的方向，<br>
<br>
0:32:32.000,0:32:34.000<br>
其實就是等高線的法線方向。<br>
<br>
0:32:34.000,0:32:36.000<br>
其實就是等高線的法線方向。<br>
<br>
0:32:36.000,0:32:38.000<br>
其實就是等高線的法線方向。<br>
<br>
0:32:40.000,0:32:42.000<br>
那我們就可以更新這個參數，<br>
<br>
0:32:42.000,0:32:44.000<br>
從這個地方，<br>
<br>
0:32:44.000,0:32:46.000<br>
到這個地方。<br>
<br>
0:32:46.000,0:32:48.000<br>
接下來你就再計算一次偏微分，<br>
<br>
0:32:48.000,0:32:50.000<br>
這個偏微分告訴你說現在應該往這個方向，<br>
<br>
0:32:50.000,0:32:52.000<br>
這個偏微分告訴你說現在應該往這個方向，<br>
<br>
0:32:52.000,0:32:54.000<br>
更新你的參數。<br>
<br>
0:32:54.000,0:32:56.000<br>
你就把你的參數從這個地方移到這個地方。<br>
<br>
0:32:56.000,0:32:58.000<br>
接下來它再告訴你說，<br>
<br>
0:32:58.000,0:32:58.920<br>
應該這樣子走，<br>
<br>
0:32:58.920,0:33:00.000<br>
應該這樣子走，<br>
<br>
0:33:00.000,0:33:02.000<br>
然後你就把參數從這個地方<br>
<br>
0:33:02.000,0:33:04.000<br>
再更新到這個地方。<br>
<br>
0:33:04.000,0:33:06.000<br>
那gradient descent有一個讓人擔心的地方。<br>
<br>
0:33:08.000,0:33:10.000<br>
就是如果今天你的loss function 長的是這個樣子，<br>
<br>
0:33:10.000,0:33:12.000<br>
就是如果今天你的loss function 長的是這個樣子，<br>
<br>
0:33:12.000,0:33:14.000<br>
就是如果今天你的loss function 長的是這個樣子，<br>
<br>
0:33:14.000,0:33:16.000<br>
如果今天w和b對這個loss L，它看起來是這個樣子，<br>
<br>
0:33:16.000,0:33:18.000<br>
如果今天w和b對這個loss L，它看起來是這個樣子，<br>
<br>
0:33:18.000,0:33:20.000<br>
如果今天w和b對這個loss L，它看起來是這個樣子，<br>
<br>
0:33:20.000,0:33:22.000<br>
那你就麻煩了。<br>
<br>
0:33:22.000,0:33:24.000<br>
那你就麻煩了。<br>
<br>
0:33:24.000,0:33:26.000<br>
這個時候如果你的隨機取捨值是在這個地方，<br>
<br>
0:33:26.000,0:33:28.000<br>
這個時候如果你的隨機取捨值是在這個地方，<br>
<br>
0:33:28.000,0:33:30.000<br>
那按照gradient建議你的方向，<br>
<br>
0:33:30.000,0:33:32.000<br>
按照今天這個偏微分建議你的方向，<br>
<br>
0:33:32.000,0:33:33.800<br>
你走走走走走，<br>
<br>
0:33:34.000,0:33:36.000<br>
就找到這個function。<br>
<br>
0:33:38.000,0:33:40.000<br>
如果你隨機取捨的地方是在這個地方，<br>
<br>
0:33:40.000,0:33:42.000<br>
那根據gradient的方向，<br>
<br>
0:33:42.000,0:33:44.000<br>
你走走走就走到這個地方。<br>
<br>
0:33:44.000,0:33:44.840<br>
所以變成說這個方法你找到的結果，<br>
<br>
0:33:44.840,0:33:46.000<br>
所以變成說這個方法你找到的結果，<br>
<br>
0:33:46.000,0:33:48.000<br>
是看人品的。<br>
<br>
0:33:50.000,0:33:52.000<br>
這個讓人非常非常的擔心，<br>
<br>
0:33:52.000,0:33:54.000<br>
但是在linear regression裡面，<br>
<br>
0:33:54.000,0:33:56.000<br>
你不用太擔心。<br>
<br>
0:33:56.000,0:33:58.000<br>
為甚麼呢?<br>
<br>
0:33:58.000,0:34:00.000<br>
因為在linear regression裡面，<br>
<br>
0:34:00.000,0:34:02.000<br>
你的這個loss function L，<br>
<br>
0:34:02.000,0:34:04.000<br>
你的這個loss function L，<br>
<br>
0:34:04.000,0:34:06.000<br>
它是convex。<br>
<br>
0:34:06.000,0:34:08.000<br>
如果你定義你loss的方式跟我在前幾頁投影片講的是一樣的話，<br>
<br>
0:34:08.000,0:34:10.000<br>
如果你定義你loss的方式跟我在前幾頁投影片講的是一樣的話，<br>
<br>
0:34:10.000,0:34:12.000<br>
那一個loss是convex的。<br>
<br>
0:34:12.000,0:34:14.000<br>
那一個loss是convex的。<br>
<br>
0:34:14.000,0:34:16.000<br>
如果你不知道convex是甚麼的話，<br>
<br>
0:34:16.000,0:34:16.800<br>
換句話說，<br>
<br>
0:34:16.800,0:34:18.000<br>
它是沒有local的optimal的位置，<br>
<br>
0:34:18.000,0:34:20.000<br>
它是沒有local的optimal的位置，<br>
<br>
0:34:20.000,0:34:22.000<br>
或者是，<br>
<br>
0:34:22.000,0:34:24.000<br>
如果我們把圖畫出來的話，<br>
<br>
0:34:24.000,0:34:26.000<br>
它長的就是這樣子。<br>
<br>
0:34:26.000,0:34:28.000<br>
它的等高線就是一圈一圈橢圓形的。<br>
<br>
0:34:28.000,0:34:30.000<br>
它的等高線就是一圈一圈橢圓形的。<br>
<br>
0:34:30.000,0:34:32.000<br>
它的等高線就是一圈一圈橢圓形的。<br>
<br>
0:34:32.000,0:34:34.000<br>
所以它是沒有local optimal的地方，<br>
<br>
0:34:34.000,0:34:36.000<br>
所以你隨便選一個起始點，<br>
<br>
0:34:36.000,0:34:40.000<br>
根據gradient descent所幫你找出來的最佳的參數<br>
<br>
0:34:40.000,0:34:42.000<br>
根據gradient descent所幫你找出來的最佳的參數<br>
<br>
0:34:42.000,0:34:44.000<br>
根據gradient descent所幫你找出來的最佳的參數<br>
<br>
0:34:44.000,0:34:46.000<br>
你最後找出來的都會是同一組參數。<br>
<br>
0:34:52.000,0:34:54.000<br>
我們來看一下它的formulation。<br>
<br>
0:34:54.000,0:34:56.000<br>
其實這個式子是非常簡單的，<br>
<br>
0:34:56.000,0:34:58.000<br>
假如你要實際算一下w對L的偏微分和b對L的偏微分<br>
<br>
0:34:58.000,0:35:00.000<br>
假如你要實際算一下w對L的偏微分和b對L的偏微分<br>
<br>
0:35:00.000,0:35:02.000<br>
假如你要實際算一下w對L的偏微分和b對L的偏微分<br>
<br>
0:35:02.000,0:35:04.000<br>
這個式子長的是甚麼樣子呢?<br>
<br>
0:35:04.000,0:35:06.000<br>
這個式子長的是甚麼樣子呢?<br>
<br>
0:35:08.000,0:35:10.000<br>
這個L我們剛才已經看到了，<br>
<br>
0:35:10.000,0:35:12.000<br>
它是長這個樣子。<br>
<br>
0:35:12.000,0:35:14.000<br>
它是估測誤差的平方和。<br>
<br>
0:35:16.000,0:35:18.000<br>
如果我們把它對w做偏微分，<br>
<br>
0:35:18.000,0:35:20.000<br>
我們得到甚麼樣的式子呢?<br>
<br>
0:35:22.000,0:35:24.000<br>
這個其實非常簡單，<br>
<br>
0:35:24.000,0:35:26.000<br>
我相信有修過微積分的人都可以秒算。<br>
<br>
0:35:28.000,0:35:30.000<br>
你就把這個2移到左邊，<br>
<br>
0:35:32.000,0:35:34.000<br>
你要對w做偏微分，<br>
<br>
0:35:36.000,0:35:38.000<br>
你就先把括號裡面這一項先做偏微分，<br>
<br>
0:35:38.000,0:35:40.000<br>
你把2移到左邊，<br>
<br>
0:35:40.000,0:35:42.000<br>
你得到這樣子的結果。<br>
<br>
0:35:42.000,0:35:44.000<br>
接下來考慮括號裡面的部分，<br>
<br>
0:35:44.000,0:35:46.000<br>
括號裡面的部分，<br>
<br>
0:35:46.000,0:35:48.000<br>
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的<br>
<br>
0:35:48.000,0:35:50.000<br>
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的<br>
<br>
0:35:50.000,0:35:52.000<br>
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的<br>
<br>
0:35:52.000,0:35:54.000<br>
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的<br>
<br>
0:35:54.000,0:35:56.000<br>
只有負的 𝑤 ∙𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的<br>
<br>
0:35:56.000,0:35:58.000<br>
所以如果你把括號裡面的equation對w做偏微分的話，<br>
<br>
0:35:58.000,0:36:00.000<br>
所以如果你把括號裡面的equation對w做偏微分的話，<br>
<br>
0:36:00.000,0:36:02.000<br>
所以如果你把括號裡面的equation對w做偏微分的話，<br>
<br>
0:36:02.000,0:36:04.000<br>
你得到的值就是負的 𝑥上標𝑛 下標𝑐𝑝<br>
<br>
0:36:04.000,0:36:06.000<br>
所以partial w partial L，<br>
<br>
0:36:08.000,0:36:10.000<br>
w對L的偏微分它的式子就是長這樣<br>
<br>
0:36:10.000,0:36:12.000<br>
w對L的偏微分它的式子就是長這樣<br>
<br>
0:36:14.000,0:36:16.000<br>
如果你要算d對L的偏微分的話，<br>
<br>
0:36:16.000,0:36:16.980<br>
也非常簡單，<br>
<br>
0:36:18.000,0:36:20.000<br>
你就把2移到前面，<br>
<br>
0:36:20.000,0:36:22.000<br>
把2移到前面，<br>
<br>
0:36:22.000,0:36:24.000<br>
變成這個樣子。<br>
<br>
0:36:24.000,0:36:26.000<br>
然後你再把這個括號裡面的值，<br>
<br>
0:36:26.000,0:36:28.000<br>
對b做偏微分。<br>
<br>
0:36:28.000,0:36:30.000<br>
括號裡面只有負b這一項，<br>
<br>
0:36:30.000,0:36:32.000<br>
跟我們要做偏微分的這個b有關，<br>
<br>
0:36:32.000,0:36:34.000<br>
跟我們要做偏微分的這個b有關，<br>
<br>
0:36:34.000,0:36:36.000<br>
所以－b對b做偏微分得到的值，<br>
<br>
0:36:36.000,0:36:38.000<br>
是－1<br>
<br>
0:36:38.000,0:36:40.000<br>
然後就結束了。<br>
<br>
0:36:40.000,0:36:42.000<br>
所以有了gradient descent，<br>
<br>
0:36:42.000,0:36:43.580<br>
你就知道說怎麼算偏微分，<br>
<br>
0:36:44.000,0:36:46.000<br>
那你就可以找一個最佳的function。<br>
<br>
0:36:46.000,0:36:48.000<br>
那你就可以找一個最佳的function。<br>
<br>
0:38:26.000,0:38:28.000<br>
那結果怎麼樣呢?<br>
<br>
0:38:28.000,0:38:30.000<br>
我們的model長這樣，<br>
<br>
0:38:30.000,0:38:32.000<br>
然後費盡一番功夫以後，<br>
<br>
0:38:34.000,0:38:36.000<br>
你找出來的最好的b跟w，<br>
<br>
0:38:36.000,0:38:38.000<br>
根據training data分別是b= －188.4，w=2.7<br>
<br>
0:38:38.000,0:38:40.000<br>
根據training data分別是b= －188.4，w=2.7<br>
<br>
0:38:40.000,0:38:42.000<br>
根據training data分別是b= －188.4，w=2.7<br>
<br>
0:38:42.000,0:38:46.000<br>
如果你把這一個function: y=b+w×Xcp<br>
<br>
0:38:46.000,0:38:48.000<br>
如果你把這一個function: y=b+w×Xcp<br>
<br>
0:38:48.000,0:38:50.000<br>
把它的b跟w值畫到圖上的話，<br>
<br>
0:38:50.000,0:38:52.000<br>
它長的是這個樣子。<br>
<br>
0:38:52.000,0:38:54.000<br>
這一條紅色的線，<br>
<br>
0:38:54.000,0:38:56.000<br>
那妳可以計算一下，<br>
<br>
0:38:56.000,0:38:58.000<br>
你會發現說這一條紅色的線，<br>
<br>
0:39:00.000,0:39:02.000<br>
沒有辦法完全正確的評定，<br>
<br>
0:39:02.000,0:39:04.000<br>
所有的寶可夢的進化後的CP值。<br>
<br>
0:39:06.000,0:39:08.000<br>
如果你想要知道說他做的有多不好的話，<br>
<br>
0:39:08.000,0:39:10.000<br>
或者是多好的話，<br>
<br>
0:39:10.000,0:39:12.000<br>
你可以看一下，<br>
<br>
0:39:12.000,0:39:14.000<br>
你可以計算一下你的error。<br>
<br>
0:39:14.000,0:39:16.000<br>
你的error就是，<br>
<br>
0:39:16.000,0:39:18.000<br>
你計算一下每一個藍色的點跟這個紅色的點之間的距離，<br>
<br>
0:39:18.000,0:39:20.000<br>
你計算一下每一個藍色的點跟這個紅色的點之間的距離，<br>
<br>
0:39:20.000,0:39:22.000<br>
第一個藍色的點跟這個紅色的線的距離，<br>
<br>
0:39:22.000,0:39:24.000<br>
第一個藍色的點跟這個紅色的線的距離，<br>
<br>
0:39:24.000,0:39:26.780<br>
是e1<br>
<br>
0:39:26.780,0:39:28.000<br>
第二個藍色的點跟紅色的線的距離是e2<br>
<br>
0:39:28.000,0:39:30.000<br>
以此類推，<br>
<br>
0:39:30.000,0:39:32.000<br>
所以有e1到e10。<br>
<br>
0:39:32.000,0:39:34.000<br>
那平均的training data的error，<br>
<br>
0:39:34.000,0:39:35.600<br>
就是summation e1到e10。<br>
<br>
0:39:36.000,0:39:38.000<br>
這邊算出來是31.9<br>
<br>
0:39:38.000,0:39:40.000<br>
這邊算出來是31.9<br>
<br>
0:39:42.000,0:39:44.000<br>
但是這個並不是我們真正關心的，<br>
<br>
0:39:44.000,0:39:46.000<br>
因為你真正關心的是，<br>
<br>
0:39:46.000,0:39:48.000<br>
generalization的case。<br>
<br>
0:39:48.000,0:39:50.000<br>
也就是說，<br>
<br>
0:39:50.000,0:39:52.000<br>
假設你今天抓到一隻新的寶可夢以後，<br>
<br>
0:39:54.000,0:39:56.000<br>
如果使用你現在的model去預測的話，<br>
<br>
0:39:58.000,0:40:00.000<br>
那做出來你估測的誤差到底有多少。<br>
<br>
0:40:00.000,0:40:02.000<br>
那做出來你估測的誤差到底有多少。<br>
<br>
0:40:02.000,0:40:04.000<br>
所以真正關心的是，那些你沒有看過的新的data，<br>
<br>
0:40:04.000,0:40:06.000<br>
這邊我們叫做testing data，<br>
<br>
0:40:06.000,0:40:08.000<br>
這邊我們叫做testing data，<br>
<br>
0:40:08.000,0:40:09.960<br>
它的誤差是多少。<br>
<br>
0:40:10.000,0:40:12.000<br>
所以這邊又抓了另外10隻寶可夢，<br>
<br>
0:40:12.000,0:40:14.000<br>
所以這邊又抓了另外10隻寶可夢，<br>
<br>
0:40:14.000,0:40:16.000<br>
當作testing data。<br>
<br>
0:40:16.000,0:40:16.500<br>
這10隻寶可夢跟之前拿來做訓練的10隻，<br>
<br>
0:40:16.500,0:40:18.000<br>
不是同樣的10隻。<br>
<br>
0:40:22.000,0:40:24.000<br>
其實這新抓的10隻跟剛才看到的10隻的分布，<br>
<br>
0:40:24.000,0:40:26.000<br>
其實這新抓的10隻跟剛才看到的10隻的分布，<br>
<br>
0:40:26.000,0:40:28.000<br>
其實是還滿像的<br>
<br>
0:40:28.000,0:40:30.000<br>
它們就是這個圖上的10個點。<br>
<br>
0:40:30.000,0:40:30.920<br>
那你會發現說，<br>
<br>
0:40:30.920,0:40:32.000<br>
我們剛才在訓練資料上找出來這條紅色的線，<br>
<br>
0:40:32.000,0:40:34.000<br>
我們剛才在訓練資料上找出來這條紅色的線，<br>
<br>
0:40:34.000,0:40:36.000<br>
我們剛才在訓練資料上找出來這條紅色的線，<br>
<br>
0:40:36.000,0:40:38.000<br>
其實也可以大致上預測，<br>
<br>
0:40:38.000,0:40:40.000<br>
在我們沒有看過的寶可夢上，<br>
<br>
0:40:40.000,0:40:42.000<br>
它的進化後的CP值。<br>
<br>
0:40:44.000,0:40:46.000<br>
如果你想要量化它的錯誤的話，<br>
<br>
0:40:46.000,0:40:48.000<br>
那就計算一下它的錯誤。<br>
<br>
0:40:48.000,0:40:50.000<br>
它錯誤算出來是35.0。<br>
<br>
0:40:50.000,0:40:52.000<br>
它錯誤算出來是35.0。<br>
<br>
0:40:52.000,0:40:54.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:40:54.000,0:40:56.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:40:56.000,0:40:58.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:40:58.000,0:41:00.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:41:00.000,0:41:02.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:41:02.000,0:41:04.000<br>
這個值是比我們剛才在training data 上看到的error還要稍微大一點<br>
<br>
0:41:04.000,0:41:06.000<br>
因為可以想想看我們最好的function是在training data上找到的<br>
<br>
0:41:06.000,0:41:08.000<br>
所以在training data上面算出來的error，<br>
<br>
0:41:08.000,0:41:10.000<br>
本來就應該比testing data上面算出來的error還要稍微大一點<br>
<br>
0:41:10.000,0:41:12.000<br>
本來就應該比testing data上面算出來的error還要稍微大一點<br>
<br>
0:41:12.000,0:41:14.000<br>
本來就應該比testing data上面算出來的error還要稍微大一點<br>
<br>
0:41:14.000,0:41:16.000<br>
本來就應該比testing data上面算出來的error還要稍微大一點<br>
<br>
0:41:18.000,0:41:20.000<br>
有沒有辦法做得更好呢?<br>
<br>
0:41:22.000,0:41:24.000<br>
如果你想要做得更好的話，<br>
<br>
0:41:24.000,0:41:26.000<br>
接下來你要做的事情就是，<br>
<br>
0:41:26.000,0:41:28.000<br>
重新去設計你的model。<br>
<br>
0:41:28.000,0:41:30.000<br>
如果你觀察一下data你會發現說，<br>
<br>
0:41:34.000,0:41:36.000<br>
在原進化前的CP值特別大的地方，<br>
<br>
0:41:36.000,0:41:38.000<br>
還有進化前的CP值特別小的地方，<br>
<br>
0:41:38.000,0:41:40.000<br>
預測是比較不準的。<br>
<br>
0:41:42.000,0:41:44.000<br>
在這個地方和這個地方，預測是比較不準的。<br>
<br>
0:41:46.000,0:41:48.000<br>
那你可以想想看說<br>
<br>
0:41:50.000,0:41:52.000<br>
任天堂在做這個遊戲的時候，<br>
<br>
0:41:52.000,0:41:54.000<br>
它背後一定是有某一支程式，<br>
<br>
0:41:54.000,0:41:56.000<br>
去根據某一些hidden 的factor。<br>
<br>
0:41:56.000,0:41:58.000<br>
去根據某一些hidden 的factor。<br>
<br>
0:41:58.000,0:42:00.000<br>
去根據某一些hidden 的factor。<br>
<br>
0:42:00.000,0:42:02.000<br>
比如說，去根據原來的CP值和其他的一些數值，<br>
<br>
0:42:02.000,0:42:04.000<br>
generate 進化以後的數值。<br>
<br>
0:42:04.000,0:42:06.000<br>
generate 進化以後的數值。<br>
<br>
0:42:08.000,0:42:10.000<br>
所以到底它的function長甚麼樣子?<br>
<br>
0:42:10.000,0:42:10.940<br>
從這個結果看來，<br>
<br>
0:42:10.940,0:42:12.000<br>
那個function可能不是這樣子一條直線，<br>
<br>
0:42:12.000,0:42:14.000<br>
那個function可能不是這樣子一條直線，<br>
<br>
0:42:14.000,0:42:16.000<br>
那個function可能不是這樣子一條直線，<br>
<br>
0:42:16.000,0:42:18.000<br>
它可能是稍微更複雜一點。<br>
<br>
0:42:18.000,0:42:20.000<br>
它可能是稍微更複雜一點。<br>
<br>
0:42:22.000,0:42:24.000<br>
所以我們需要有一個更複雜的model。<br>
<br>
0:42:24.000,0:42:24.880<br>
舉例來說，<br>
<br>
0:42:24.880,0:42:26.000<br>
我們這邊可能需要引入二次式。<br>
<br>
0:42:26.000,0:42:28.000<br>
我們這邊可能需要引入二次式。<br>
<br>
0:42:28.000,0:42:30.000<br>
我們這邊可能需要引入二次式。<br>
<br>
0:42:30.000,0:42:31.260<br>
我們今天，<br>
<br>
0:42:31.260,0:42:32.000<br>
可能需要引入(Xcp)²這一項<br>
<br>
0:42:32.000,0:42:34.000<br>
可能需要引入(Xcp)²這一項<br>
<br>
0:42:34.000,0:42:36.000<br>
我們重新設計了一個model，<br>
<br>
0:42:36.000,0:42:38.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:38.000,0:42:40.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:40.000,0:42:42.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:42.000,0:42:44.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:44.000,0:42:46.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:46.000,0:42:48.000<br>
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²<br>
<br>
0:42:48.000,0:42:50.000<br>
我們加了後面這一項<br>
<br>
0:42:50.000,0:42:52.000<br>
如果我們有了這個新的function，<br>
<br>
0:42:52.000,0:42:54.000<br>
你可以用我們剛才講得一模一樣的方式，<br>
<br>
0:42:54.000,0:42:56.000<br>
去define一個function 的好壞，<br>
<br>
0:42:56.000,0:42:58.000<br>
然後用gradient descent，<br>
<br>
0:42:58.000,0:43:00.000<br>
找出一個，<br>
<br>
0:43:00.000,0:43:02.000<br>
在你的function set裡面最好的function。<br>
<br>
0:43:02.000,0:43:04.000<br>
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3<br>
<br>
0:43:04.000,0:43:06.000<br>
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3<br>
<br>
0:43:06.000,0:43:08.000<br>
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3<br>
<br>
0:43:08.000,0:43:10.000<br>
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3<br>
<br>
0:43:10.000,0:43:12.000<br>
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3<br>
<br>
0:43:12.000,0:43:14.000<br>
如果我們把這個最好的function畫在圖上的話，<br>
<br>
0:43:14.000,0:43:16.000<br>
它長的是這個樣子。<br>
<br>
0:43:16.000,0:43:18.000<br>
它長的是這個樣子。<br>
<br>
0:43:18.000,0:43:18.800<br>
你就會發現說，<br>
<br>
0:43:18.800,0:43:20.000<br>
現在我們有了這條新的曲線，<br>
<br>
0:43:20.000,0:43:22.000<br>
現在我們有了這條新的曲線，<br>
<br>
0:43:22.000,0:43:24.000<br>
我們有了這個新的model，<br>
<br>
0:43:24.000,0:43:26.000<br>
它的預測在training data上面看起來是更準一點。<br>
<br>
0:43:26.000,0:43:28.000<br>
它的預測在training data上面看起來是更準一點。<br>
<br>
0:43:28.000,0:43:30.000<br>
它的預測在training data上面看起來是更準一點。<br>
<br>
0:43:30.000,0:43:32.000<br>
在training data上面你得到的average error現在是15.4。<br>
<br>
0:43:32.000,0:43:34.000<br>
在training data上面你得到的average error現在是15.4。<br>
<br>
0:43:34.000,0:43:34.500<br>
但我們真正關心的，<br>
<br>
0:43:36.000,0:43:38.000<br>
是testing data<br>
<br>
0:43:38.000,0:43:40.000<br>
那我們就把同樣的model<br>
<br>
0:43:40.000,0:43:42.000<br>
再apply到testing data上。<br>
<br>
0:43:44.000,0:43:46.000<br>
我們在testing data上apply同樣這條紅色的線，<br>
<br>
0:43:46.000,0:43:48.000<br>
然後去計算它的average error。<br>
<br>
0:43:48.000,0:43:50.000<br>
那我們現在得到的是18.4<br>
<br>
0:43:52.000,0:43:54.000<br>
在剛才如果我們沒有考慮(Xcp)²的時候<br>
<br>
0:43:54.000,0:43:56.000<br>
在剛才如果我們沒有考慮(Xcp)²的時候<br>
<br>
0:43:56.000,0:43:58.000<br>
算出來的average error是30左右<br>
<br>
0:43:58.000,0:44:00.000<br>
現在有考慮平方項得到的是18.4<br>
<br>
0:44:00.000,0:44:02.000<br>
現在有考慮平方項得到的是18.4<br>
<br>
0:44:02.000,0:44:04.000<br>
現在有考慮平方項得到的是18.4<br>
<br>
0:44:04.000,0:44:06.000<br>
那有沒有可能做得更好呢?<br>
<br>
0:44:06.000,0:44:08.000<br>
比如說我們可以考慮一個更複雜的model。<br>
<br>
0:44:08.000,0:44:10.980<br>
比如說我們可以考慮一個更複雜的model。<br>
<br>
0:44:10.980,0:44:12.000<br>
我們引入不只是(Xcp)²，<br>
<br>
0:44:12.000,0:44:14.000<br>
我們引入不只是(Xcp)²，<br>
<br>
0:44:14.000,0:44:16.000<br>
我們引入(Xcp)³。<br>
<br>
0:44:16.000,0:44:18.000<br>
我們引入(Xcp)³。<br>
<br>
0:44:18.000,0:44:20.000<br>
所以我們現在的model長的是這個樣子。<br>
<br>
0:44:22.000,0:44:22.800<br>
你就用一模一樣的方法，<br>
<br>
0:44:22.800,0:44:24.000<br>
你就可以根據你的training data，<br>
<br>
0:44:24.000,0:44:26.000<br>
找到在這一個 function set裡面，<br>
<br>
0:44:26.000,0:44:28.000<br>
在這個model 裡面最好的一個function。<br>
<br>
0:44:28.000,0:44:30.000<br>
在這個model 裡面最好的一個function。<br>
<br>
0:44:30.000,0:44:32.000<br>
那找出來是這樣 :<br>
<br>
0:44:32.000,0:44:32.500<br>
b=6.4，w1=0.66,<br>
<br>
0:44:32.500,0:44:34.000<br>
w2=4.3×10^-3 ，w3=-1.8×10^-6<br>
<br>
0:44:34.000,0:44:36.000<br>
w2=4.3×10^-3 ，w3=-1.8×10^-6<br>
<br>
0:44:36.000,0:44:38.000<br>
w2=4.3×10^-3 ，w3=-1.8×10^-6<br>
<br>
0:44:38.000,0:44:40.000<br>
所以你發現w3其實是它的值比較小<br>
<br>
0:44:42.000,0:44:44.000<br>
它可能是沒有太大的影響<br>
<br>
0:44:46.000,0:44:48.000<br>
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的<br>
<br>
0:44:48.000,0:44:50.000<br>
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的<br>
<br>
0:44:50.000,0:44:52.000<br>
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的<br>
<br>
0:44:52.000,0:44:54.000<br>
那做出來看起來像是這個樣子。<br>
<br>
0:44:56.000,0:44:58.000<br>
那這個時候average error算出來是15.3<br>
<br>
0:44:58.000,0:45:00.000<br>
如果你看testing data的話，<br>
<br>
0:45:00.000,0:45:02.000<br>
如果你看testing data的話，<br>
<br>
0:45:02.000,0:45:04.000<br>
testing data算出來的average error是18.1。<br>
<br>
0:45:04.000,0:45:06.640<br>
testing data算出來的average error是18.1。<br>
<br>
0:45:06.640,0:45:08.000<br>
跟剛剛二次的，<br>
<br>
0:45:08.000,0:45:10.000<br>
有考慮(Xcp)²的結果比起來是稍微好一點點。<br>
<br>
0:45:10.000,0:45:12.000<br>
有考慮(Xcp)²的結果比起來是稍微好一點點。<br>
<br>
0:45:12.000,0:45:14.000<br>
有考慮(Xcp)²的結果比起來是稍微好一點點。<br>
<br>
0:45:14.000,0:45:16.000<br>
剛才前一頁是18.3，有考慮三次項後是18.1<br>
<br>
0:45:16.000,0:45:18.000<br>
剛才前一頁是18.3，有考慮三次項後是18.1<br>
<br>
0:45:18.000,0:45:20.000<br>
是稍微好一點點。<br>
<br>
0:45:20.000,0:45:22.000<br>
那有沒有可能是更複雜的model呢?<br>
<br>
0:45:22.000,0:45:24.000<br>
那有沒有可能是更複雜的model呢?<br>
<br>
0:45:26.000,0:45:28.000<br>
或許在寶可夢的那個程式背後，<br>
<br>
0:45:28.000,0:45:30.000<br>
它產生進化後的CP值用的是一個更複雜的一個function<br>
<br>
0:45:30.000,0:45:32.000<br>
它產生進化後的CP值用的是一個更複雜的一個function<br>
<br>
0:45:32.000,0:45:34.000<br>
它產生進化後的CP值用的是一個更複雜的一個function<br>
<br>
0:45:34.000,0:45:34.720<br>
或許它不只考慮了三次，<br>
<br>
0:45:34.720,0:45:36.000<br>
或者它不只考慮了(Xcp)³<br>
<br>
0:45:36.000,0:45:38.000<br>
或者它不只考慮了(Xcp)³<br>
<br>
0:45:38.000,0:45:40.000<br>
或許它考慮的是四次方也說不定<br>
<br>
0:45:40.000,0:45:42.000<br>
或許它考慮的是四次方也說不定<br>
<br>
0:45:42.000,0:45:42.520<br>
那你就用同樣的方法，<br>
<br>
0:45:44.000,0:45:46.000<br>
再把這些參數 :b、w1、w2、w3和w4都找出來，<br>
<br>
0:45:46.000,0:45:48.000<br>
再把這些參數 :b、w1、w2、w3和w4都找出來，<br>
<br>
0:45:48.000,0:45:50.000<br>
那你得到的function長這個樣子。<br>
<br>
0:45:50.000,0:45:52.000<br>
那你得到的function長這個樣子。<br>
<br>
0:45:54.000,0:45:56.000<br>
你發現它在training data上它顯然可以做得更好。<br>
<br>
0:45:56.000,0:45:58.000<br>
在input的CP值比較小的這些寶可夢，<br>
<br>
0:45:58.000,0:46:00.000<br>
在input的CP值比較小的這些寶可夢，<br>
<br>
0:46:00.000,0:46:02.000<br>
在input的CP值比較小的這些寶可夢，<br>
<br>
0:46:02.000,0:46:04.000<br>
這些顯然是一些綠毛蟲之類的東西，<br>
<br>
0:46:04.000,0:46:06.000<br>
這些顯然是一些綠毛蟲之類的東西，<br>
<br>
0:46:06.000,0:46:08.000<br>
這些顯然是一些綠毛蟲之類的東西，<br>
<br>
0:46:08.000,0:46:10.000<br>
它在這邊是predict更準的。<br>
<br>
0:46:10.000,0:46:12.000<br>
它在這邊是predict更準的。<br>
<br>
0:46:12.000,0:46:14.000<br>
所以現在的average error是14.9，<br>
<br>
0:46:14.000,0:46:16.000<br>
所以現在的average error是14.9，<br>
<br>
0:46:16.000,0:46:18.000<br>
剛才三次的是15.3<br>
<br>
0:46:18.000,0:46:20.000<br>
剛才三次的是在training data上是15.3<br>
<br>
0:46:24.000,0:46:26.000<br>
現在四次的時候在training data上是14.9<br>
<br>
0:46:26.000,0:46:28.000<br>
但是我們真正關心的是testing<br>
<br>
0:46:30.000,0:46:32.000<br>
我們真正關心的是如果沒有看過的寶可夢，<br>
<br>
0:46:32.000,0:46:34.000<br>
我們能夠多精確的預測它進化後的CP值。<br>
<br>
0:46:34.000,0:46:36.000<br>
我們能夠多精確的預測它進化後的CP值。<br>
<br>
0:46:36.000,0:46:36.720<br>
所以，<br>
<br>
0:46:36.720,0:46:38.000<br>
我們發現說，<br>
<br>
0:46:40.000,0:46:42.000<br>
如果我們看沒有看過的寶可夢的話，<br>
<br>
0:46:42.000,0:46:44.000<br>
我們得到的average error是多少呢?<br>
<br>
0:46:46.000,0:46:48.000<br>
我們得到的average error其實是28.8<br>
<br>
0:46:48.000,0:46:50.000<br>
前一頁做出來已經是18.3了<br>
<br>
0:46:50.700,0:46:52.000<br>
就我們用三次的時候，<br>
<br>
0:46:52.000,0:46:54.000<br>
在testing data上面做出來已經是18.3了<br>
<br>
0:46:56.000,0:46:58.000<br>
但是我們換了一個更複雜的model以後，<br>
<br>
0:46:58.000,0:47:00.660<br>
做出來是28.8<br>
<br>
0:47:00.660,0:47:02.000<br>
結果竟然變得更糟了!<br>
<br>
0:47:02.000,0:47:04.000<br>
結果竟然變得更糟了!<br>
<br>
0:47:04.000,0:47:06.000<br>
我們換了一個更複雜的model，<br>
<br>
0:47:06.000,0:47:08.000<br>
在training data上給我們比較好的結果<br>
<br>
0:47:08.000,0:47:10.000<br>
但在testing data上，<br>
<br>
0:47:10.000,0:47:12.000<br>
看起來結果是更糟的。<br>
<br>
0:47:12.000,0:47:14.000<br>
那如果換再更複雜的model會怎樣呢?<br>
<br>
0:47:14.000,0:47:16.000<br>
有沒有可能是五次式，<br>
<br>
0:47:16.000,0:47:18.000<br>
有沒有可能是五次式，<br>
<br>
0:47:18.000,0:47:20.000<br>
有沒有可能它背後的程式是如此的複雜，<br>
<br>
0:47:22.000,0:47:24.000<br>
原來的CP值的一次、兩次、三次、四次到五次<br>
<br>
0:47:24.000,0:47:26.000<br>
原來的CP值的一次、兩次、三次、四次到五次<br>
<br>
0:47:26.000,0:47:28.000<br>
那這個時候，<br>
<br>
0:47:28.000,0:47:30.000<br>
我們把最好的function找出來，<br>
<br>
0:47:34.000,0:47:36.000<br>
你會發現它最好的function在training data上長得像是這樣子。<br>
<br>
0:47:36.000,0:47:38.000<br>
你會發現它最好的function在training data上長得像是這樣子。<br>
<br>
0:47:38.000,0:47:40.000<br>
你會發現它最好的function在training data上長得像是這樣子。<br>
<br>
0:47:40.000,0:47:42.000<br>
這個是一個合理的結果嗎?<br>
<br>
0:47:42.000,0:47:44.000<br>
你會發現說，<br>
<br>
0:47:44.000,0:47:46.000<br>
在原來的CP值是500左右，<br>
<br>
0:47:46.000,0:47:48.000<br>
在原來的CP值是500左右，<br>
<br>
0:47:48.000,0:47:50.000<br>
500左右可能就是伊布之類的東西。<br>
<br>
0:47:52.000,0:47:54.000<br>
在原來的CP值是500左右的寶可夢，<br>
<br>
0:47:54.000,0:47:56.000<br>
根據你現在的model預測出來，<br>
<br>
0:47:56.000,0:47:58.000<br>
它的CP值竟然是負的。<br>
<br>
0:48:00.000,0:48:02.000<br>
但是在training data上面，<br>
<br>
0:48:02.000,0:48:04.000<br>
但是在training data上面，<br>
<br>
0:48:06.000,0:48:09.000<br>
我們可以算出來的error是12.8，<br>
<br>
0:48:09.000,0:48:10.000<br>
比我們剛才用四次式，<br>
<br>
0:48:10.000,0:48:12.000<br>
得到的結果又再更好一些。<br>
<br>
0:48:14.000,0:48:16.000<br>
那在testing的結果上是怎樣呢<br>
<br>
0:48:18.000,0:48:20.000<br>
如果我們把這個我們找出來的function，<br>
<br>
0:48:20.000,0:48:22.000<br>
apply到新的寶可夢上面，<br>
<br>
0:48:22.000,0:48:24.000<br>
你會發現結果怎麼爛掉了啊<br>
<br>
0:48:26.000,0:48:28.000<br>
至少這一隻大概是伊布吧<br>
<br>
0:48:28.000,0:48:30.000<br>
這隻伊布，<br>
<br>
0:48:30.000,0:48:32.000<br>
它預測出來進化後的CP值，<br>
<br>
0:48:32.000,0:48:34.000<br>
它預測出來進化後的CP值，<br>
<br>
0:48:34.000,0:48:36.000<br>
是非常的不准。<br>
<br>
0:48:36.000,0:48:38.000<br>
照理說有1000多，<br>
<br>
0:48:38.000,0:48:40.000<br>
但是你的model卻給它一個負的預測值。<br>
<br>
0:48:40.000,0:48:42.000<br>
但是你的model卻給它一個負的預測值。<br>
<br>
0:48:42.000,0:48:44.000<br>
所以算出來的average error非常大，<br>
<br>
0:48:44.000,0:48:46.000<br>
有200多。<br>
<br>
0:48:46.000,0:48:48.000<br>
所以當我們換了一個更複雜的model，<br>
<br>
0:48:48.000,0:48:48.800<br>
考慮到五次的時候，<br>
<br>
0:48:50.000,0:48:52.000<br>
結果又更加糟糕了。<br>
<br>
0:48:56.000,0:48:58.000<br>
所以到目前為止，我們試了五個不同的model。<br>
<br>
0:48:58.000,0:49:00.000<br>
那這五個model，<br>
<br>
0:49:00.000,0:49:02.000<br>
如果你把他們分別的在training data上面的average error都畫出來的話，<br>
<br>
0:49:02.000,0:49:04.000<br>
如果你把他們分別的在training data上面的average error都畫出來的話，<br>
<br>
0:49:04.000,0:49:06.000<br>
如果你把他們分別的在training data上面的average error都畫出來的話，<br>
<br>
0:49:06.000,0:49:08.000<br>
如果你把他們分別的在training data上面的average error都畫出來的話，<br>
<br>
0:49:08.000,0:49:10.000<br>
你會得到這樣子的一張圖。<br>
<br>
0:49:12.000,0:49:14.000<br>
從高到低<br>
<br>
0:49:16.000,0:49:18.000<br>
也就是說，如果你考慮一個最簡單的model，<br>
<br>
0:49:18.000,0:49:20.000<br>
這個時候error是比較高的；<br>
<br>
0:49:20.000,0:49:21.520<br>
model稍微複雜一點，<br>
<br>
0:49:22.000,0:49:24.000<br>
error稍微下降；<br>
<br>
0:49:24.000,0:49:26.000<br>
然後model越複雜，<br>
<br>
0:49:26.000,0:49:28.000<br>
在這個training data上的error就會越來越小。<br>
<br>
0:49:28.000,0:49:30.000<br>
在這個training data上的error就會越來越小。<br>
<br>
0:49:30.000,0:49:32.000<br>
那為甚麼會這樣呢?<br>
<br>
0:49:32.000,0:49:32.880<br>
這件事情倒是非常的直覺，<br>
<br>
0:49:32.880,0:49:33.380<br>
非常容易解釋。<br>
<br>
0:49:36.000,0:49:38.000<br>
假設黃色的這個圈圈，<br>
<br>
0:49:38.000,0:49:40.000<br>
我們故意用一樣的顏色<br>
<br>
0:49:40.000,0:49:42.000<br>
黃色這個圈圈代表這一個式子，<br>
<br>
0:49:42.000,0:49:44.000<br>
黃色這個圈圈代表這一個式子，<br>
<br>
0:49:44.000,0:49:46.000<br>
有考慮三次的式子，<br>
<br>
0:49:46.000,0:49:48.000<br>
所形成的function space。<br>
<br>
0:49:50.000,0:49:52.000<br>
那四次的式子所形成的function space，<br>
<br>
0:49:52.000,0:49:54.000<br>
就是這個綠色的圈圈。<br>
<br>
0:49:54.000,0:49:56.000<br>
它是包含黃色的圈圈的，<br>
<br>
0:49:56.000,0:49:56.920<br>
這個事情很合理，<br>
<br>
0:49:56.920,0:49:58.000<br>
因為你只要把w4設為0，<br>
<br>
0:49:58.000,0:50:00.000<br>
因為你只要把w4設為0，<br>
<br>
0:50:00.000,0:50:02.000<br>
是四次的這個式子就可以變成三次的式子。<br>
<br>
0:50:02.000,0:50:04.000<br>
是四次的這個式子就可以變成三次的式子。<br>
<br>
0:50:06.000,0:50:08.000<br>
所以三次的式子都包含在這個四次的式子裡面，<br>
<br>
0:50:08.000,0:50:10.000<br>
所以三次的式子都包含在這個四次的式子裡面，<br>
<br>
0:50:10.000,0:50:12.000<br>
黃色的圈圈都包含在綠色的圈圈裏面。<br>
<br>
0:50:14.000,0:50:16.000<br>
那如果我們今天考慮更複雜的五次的式子的話，<br>
<br>
0:50:16.000,0:50:18.000<br>
它又可以包含所有四次的式子。<br>
<br>
0:50:18.000,0:50:20.000<br>
它又可以包含所有四次的式子。<br>
<br>
0:50:26.000,0:50:28.000<br>
所以今天如果你有一個越複雜的model，<br>
<br>
0:50:30.000,0:50:32.000<br>
它包含了越多的function的話，<br>
<br>
0:50:36.000,0:50:40.000<br>
那理論上你就可以找出一個function，<br>
<br>
0:50:40.000,0:50:42.000<br>
它可以讓你的error rate越來越低。<br>
<br>
0:50:42.000,0:50:44.000<br>
你的function如果越複雜，你的candidate如果越多，<br>
<br>
0:50:44.000,0:50:44.800<br>
你當然可以找到一個function，<br>
<br>
0:50:44.800,0:50:46.000<br>
讓你的error rate越來越低。<br>
<br>
0:50:46.000,0:50:48.000<br>
讓你的error rate越來越低。<br>
<br>
0:50:48.000,0:50:50.000<br>
當然這邊的前提就是，<br>
<br>
0:50:50.000,0:50:52.000<br>
你的gradient descent要能夠真正幫你找出best function的前提下，<br>
<br>
0:50:52.000,0:50:54.000<br>
你的gradient descent要能夠真正幫你找出best function的前提下，<br>
<br>
0:50:54.000,0:50:54.640<br>
你的function越複雜，<br>
<br>
0:50:54.640,0:50:56.000<br>
可以讓你的error rate在training function上越低。<br>
<br>
0:50:56.000,0:50:58.000<br>
可以讓你的error rate在training function上越低。<br>
<br>
0:50:58.000,0:51:00.000<br>
可以讓你的error rate在training function上越低。<br>
<br>
0:51:00.000,0:51:02.000<br>
但是在testing data上面，<br>
<br>
0:51:02.000,0:51:04.000<br>
看起來的結果是不一樣的。<br>
<br>
0:51:04.000,0:51:06.000<br>
在testing data上面看起來的結果是不一樣的。<br>
<br>
0:51:06.000,0:51:08.000<br>
在testing data上面看起來的結果是不一樣的。<br>
<br>
0:51:08.000,0:51:08.500<br>
在training data上，<br>
<br>
0:51:08.500,0:51:10.000<br>
你會發現說<br>
<br>
0:51:10.000,0:51:12.000<br>
model越來越複雜你的error越來越低；<br>
<br>
0:51:12.000,0:51:14.000<br>
model越來越複雜你的error越來越低；<br>
<br>
0:51:14.000,0:51:16.000<br>
但是在testing data上，<br>
<br>
0:51:16.000,0:51:19.040<br>
在到第三個式子為止，<br>
<br>
0:51:19.040,0:51:20.000<br>
你的error是有下降的。<br>
<br>
0:51:22.000,0:51:24.000<br>
但是到第四個和第五個的function的時候，<br>
<br>
0:51:24.000,0:51:26.000<br>
error就暴增。<br>
<br>
0:51:26.000,0:51:28.000<br>
然後把它的圖試著畫在左邊這邊。<br>
<br>
0:51:28.000,0:51:30.000<br>
然後把它的圖試著畫在左邊這邊。<br>
<br>
0:51:30.000,0:51:32.000<br>
藍色的是training data上對不同function的error，<br>
<br>
0:51:32.000,0:51:34.000<br>
藍色的是training data上對不同function的error，<br>
<br>
0:51:34.000,0:51:36.000<br>
橙色的是testing data上對不同function的error。<br>
<br>
0:51:36.000,0:51:38.000<br>
橙色的是testing data上對不同function的error。<br>
<br>
0:51:38.000,0:51:39.000<br>
你會發現說，<br>
<br>
0:51:39.000,0:51:40.000<br>
今天在五次的時候，<br>
<br>
0:51:40.000,0:51:42.000<br>
在testing上是爆炸的，<br>
<br>
0:51:42.000,0:51:44.000<br>
它就突破天際沒辦法畫在這張圖上。<br>
<br>
0:51:44.000,0:51:46.000<br>
它就突破天際沒辦法畫在這張圖上。<br>
<br>
0:51:46.000,0:51:48.000<br>
那所以我們今天，<br>
<br>
0:51:48.000,0:51:50.000<br>
得到一個觀察，<br>
<br>
0:51:50.000,0:51:52.000<br>
雖然說越複雜的model可以在training data上面給我們越好的結果，<br>
<br>
0:51:52.000,0:51:54.000<br>
雖然說越複雜的model可以在training data上面給我們越好的結果，<br>
<br>
0:51:54.000,0:51:56.000<br>
但這件事情也沒有甚麼，<br>
<br>
0:51:56.000,0:51:58.000<br>
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。<br>
<br>
0:51:58.000,0:52:00.000<br>
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。<br>
<br>
0:52:00.000,0:52:02.000<br>
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。<br>
<br>
0:52:02.000,0:52:04.000<br>
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。<br>
<br>
0:52:06.000,0:52:08.000<br>
這件事情就叫做overfitting。<br>
<br>
0:52:08.000,0:52:10.000<br>
就複雜的model在training data上有好的結果，<br>
<br>
0:52:10.000,0:52:12.000<br>
但在testing data上不一定有好的結果。<br>
<br>
0:52:14.000,0:52:16.000<br>
這件事情就叫做overfitting。<br>
<br>
0:52:16.000,0:52:18.000<br>
比如當我們用第四個和第五個式子的時候，<br>
<br>
0:52:20.000,0:52:22.000<br>
我們就發生overfitting的情形。<br>
<br>
0:52:24.000,0:52:26.000<br>
那為甚麼會有overfitting這個情形呢?<br>
<br>
0:52:26.000,0:52:28.000<br>
為甚麼更複雜的model它在training上面得到比較好的結果，<br>
<br>
0:52:28.000,0:52:30.000<br>
為甚麼更複雜的model它在training上面得到比較好的結果，<br>
<br>
0:52:30.000,0:52:32.000<br>
在testing上面不一定得到比較好的結果呢?<br>
<br>
0:52:32.000,0:52:34.000<br>
這個我們日後再解釋。<br>
<br>
0:52:34.000,0:52:36.000<br>
但是你其實是可以想到很多很直觀的，<br>
<br>
0:52:36.000,0:52:38.000<br>
在training data上面得到比較好的結果，<br>
<br>
0:52:38.000,0:52:40.000<br>
在training data上面得到比較好的結果，<br>
<br>
0:52:40.000,0:52:41.660<br>
在訓練的時候得到比較好的結果<br>
<br>
0:52:42.000,0:52:44.000<br>
但在測試的時候不一定會得到比較好的結果。<br>
<br>
0:52:44.000,0:52:44.840<br>
比如說，<br>
<br>
0:52:44.840,0:52:46.000<br>
你有沒有考過駕照?<br>
<br>
0:52:48.000,0:52:50.000<br>
考駕照不是都要去那個駕訓班嗎?<br>
<br>
0:52:52.000,0:52:54.000<br>
駕訓班不是都在那個場內練習嗎<br>
<br>
0:52:54.000,0:52:56.000<br>
你在場內練習不是都很順?<br>
<br>
0:52:56.000,0:52:56.600<br>
練習非常非常多次以後，<br>
<br>
0:52:56.600,0:52:58.000<br>
你就會得到很奇怪的技能。<br>
<br>
0:52:58.000,0:52:58.760<br>
你就學到說，<br>
<br>
0:52:58.760,0:53:00.000<br>
比如說，<br>
<br>
0:53:00.000,0:53:02.000<br>
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，<br>
<br>
0:53:02.000,0:53:04.000<br>
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，<br>
<br>
0:53:04.000,0:53:06.000<br>
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，<br>
<br>
0:53:06.000,0:53:08.000<br>
就把方向盤左轉半圈這樣子。<br>
<br>
0:53:08.000,0:53:10.000<br>
你就學到這種技能，<br>
<br>
0:53:10.000,0:53:12.000<br>
所以你在測試訓練的時候，<br>
<br>
0:53:12.000,0:53:14.000<br>
在駕訓班的時候你可以做得很好。<br>
<br>
0:53:14.000,0:53:16.000<br>
但在路上的時候你就做不好。<br>
<br>
0:53:16.000,0:53:18.000<br>
像我就不太會開車，<br>
<br>
0:53:18.000,0:53:20.000<br>
雖然我有駕照。<br>
<br>
0:53:20.000,0:53:22.000<br>
所以我都在等無人駕駛車出來。<br>
<br>
0:53:28.000,0:53:30.000<br>
所以overfitting是很有可能會發生的。<br>
<br>
0:53:30.000,0:53:32.000<br>
所以model不是越複雜越好，<br>
<br>
0:53:34.000,0:53:36.000<br>
我們必須選一個剛剛好，<br>
<br>
0:53:36.000,0:53:38.000<br>
沒有非常複雜也沒有很複雜的model，<br>
<br>
0:53:38.000,0:53:40.000<br>
沒有非常複雜也沒有很複雜的model，<br>
<br>
0:53:40.000,0:53:42.000<br>
你要選一個最適合的model。<br>
<br>
0:53:42.000,0:53:44.000<br>
比如說在這個case裡面，<br>
<br>
0:53:46.000,0:53:48.000<br>
當我們選一個三次式的時候，<br>
<br>
0:53:48.000,0:53:50.000<br>
在這個case裡面當我們選一個三次式的時候，<br>
<br>
0:53:52.000,0:53:54.000<br>
可以給我們最低的error。<br>
<br>
0:53:54.000,0:53:56.000<br>
所以如果今天可以選的話，<br>
<br>
0:53:56.000,0:53:58.000<br>
我們就應該選擇三次的式子來作為我們的model，<br>
<br>
0:53:58.000,0:54:00.000<br>
我們就應該選擇三次的式子來作為我們的model，<br>
<br>
0:54:00.000,0:54:02.000<br>
來作為我們的function set。<br>
<br>
0:54:04.000,0:54:06.000<br>
你以為這樣就結束了嗎?<br>
<br>
0:54:06.000,0:54:08.000<br>
其實還沒有。<br>
<br>
0:54:08.000,0:54:10.000<br>
剛才只收集了10隻寶可夢，其實太少了<br>
<br>
0:54:12.000,0:54:14.000<br>
當我們收集到60隻寶可夢的時候，<br>
<br>
0:54:14.000,0:54:16.000<br>
你會發現說剛才都是白忙一場。<br>
<br>
0:54:18.000,0:54:20.000<br>
你仔細想 : 當我們收集60隻寶可夢，<br>
<br>
0:54:22.000,0:54:24.000<br>
你把它的原來的CP值和進化後的CP值，<br>
<br>
0:54:24.000,0:54:26.000<br>
你把它的原來的CP值和進化後的CP值，<br>
<br>
0:54:26.000,0:54:28.000<br>
畫在這個圖上，<br>
<br>
0:54:28.000,0:54:30.000<br>
你會發現說他們中間有一個非常奇妙的關係。<br>
<br>
0:54:30.000,0:54:32.000<br>
你會發現說他們中間有一個非常奇妙的關係。<br>
<br>
0:54:32.000,0:54:34.000<br>
它顯然不是甚麼一次二次三次一百次式，<br>
<br>
0:54:34.000,0:54:36.000<br>
顯然都不是，<br>
<br>
0:54:38.000,0:54:40.000<br>
中間有另外一個力量，<br>
<br>
0:54:42.000,0:54:44.940<br>
這個力量不是CP值它在影響著進化後的數值，<br>
<br>
0:54:44.940,0:54:46.000<br>
到底是甚麼呢?<br>
<br>
0:54:46.000,0:54:48.000<br>
其實非常的直覺，<br>
<br>
0:54:48.000,0:54:50.000<br>
就是寶可夢的物種。<br>
<br>
0:54:52.000,0:54:54.000<br>
這邊我們把不同的物種用不同的顏色來表示，<br>
<br>
0:54:54.000,0:54:56.000<br>
這邊我們把不同的物種用不同的顏色來表示，<br>
<br>
0:55:00.000,0:55:02.000<br>
藍色是波波，<br>
<br>
0:55:02.000,0:55:04.000<br>
波波進化後是比比鳥，<br>
<br>
0:55:04.000,0:55:06.000<br>
比比鳥進化是大比鳥。<br>
<br>
0:55:16.000,0:55:18.000<br>
這個黃色的點是獨角蟲，<br>
<br>
0:55:18.000,0:55:20.000<br>
這個黃色的點是獨角蟲，<br>
<br>
0:55:20.000,0:55:22.000<br>
獨角蟲進化後是鐵殼蛹，<br>
<br>
0:55:22.000,0:55:24.000<br>
鐵殼蛹進化後是大針蜂。<br>
<br>
0:55:28.000,0:55:30.000<br>
然後綠色的是綠毛蟲，<br>
<br>
0:55:30.000,0:55:32.000<br>
綠毛蟲進化是鐵甲蛹，<br>
<br>
0:55:32.000,0:55:34.000<br>
鐵甲蛹進化是巴大蝴。<br>
<br>
0:55:34.000,0:55:36.000<br>
紅色的是伊布，<br>
<br>
0:55:36.000,0:55:38.000<br>
伊布可以進化成雷精靈、火精靈或水精靈等等<br>
<br>
0:55:40.000,0:55:42.000<br>
你可能說怎麼都只有這些路邊就可以見到的，<br>
<br>
0:55:42.000,0:55:44.000<br>
因為抓乘龍快龍是很麻煩的，<br>
<br>
0:55:44.000,0:55:45.100<br>
所以就只有這些而已。<br>
<br>
0:55:52.000,0:55:54.000<br>
所以剛才只考慮CP值這件事，<br>
<br>
0:55:54.000,0:55:56.000<br>
只考慮進化前的CP值顯然是不對的。<br>
<br>
0:55:56.000,0:55:58.000<br>
只考慮進化前的CP值顯然是不對的。<br>
<br>
0:56:02.000,0:56:04.000<br>
因為這個進化後的CP值受到物種的影響其實是很大的。<br>
<br>
0:56:04.000,0:56:06.000<br>
因為這個進化後的CP值受到物種的影響其實是很大的。<br>
<br>
0:56:06.000,0:56:08.000<br>
或者是比原來的CP值<br>
<br>
0:56:08.000,0:56:10.000<br>
產生非常關鍵性的影響。<br>
<br>
0:56:10.000,0:56:11.240<br>
所以我們在設計model的時候，<br>
<br>
0:56:14.000,0:56:16.000<br>
剛才那個model設計的是不好的。<br>
<br>
0:56:16.000,0:56:16.880<br>
剛才那個model就好像是，<br>
<br>
0:56:18.000,0:56:20.000<br>
你想要海底撈針，<br>
<br>
0:56:20.000,0:56:22.000<br>
從function set裡面撈出一個最好的model<br>
<br>
0:56:22.000,0:56:22.640<br>
那其實裡面model通通都不好，<br>
<br>
0:56:22.640,0:56:24.000<br>
所以針根本就不在海裡，<br>
<br>
0:56:26.000,0:56:28.000<br>
所以你要重新設計一下你的function set。<br>
<br>
0:56:30.000,0:56:32.000<br>
所以這邊就重新設計一下function set，<br>
<br>
0:56:32.000,0:56:34.000<br>
我們的function set， input x跟output y，<br>
<br>
0:56:34.000,0:56:36.000<br>
這個input寶可夢和output進化後的CP值有甚麼關係呢?<br>
<br>
0:56:36.000,0:56:38.000<br>
這個input寶可夢和output進化後的CP值有甚麼關係呢?<br>
<br>
0:56:38.000,0:56:40.000<br>
它的關係是這樣 :<br>
<br>
0:56:40.000,0:56:42.000<br>
如果今天輸入的寶可夢x，<br>
<br>
0:56:42.000,0:56:44.000<br>
如果今天輸入的寶可夢x，<br>
<br>
0:56:46.000,0:56:48.000<br>
它的物種是屬於波波的話，<br>
<br>
0:56:50.000,0:56:52.000<br>
這個Xs代表說這個input x 的物種，<br>
<br>
0:56:56.000,0:56:58.000<br>
那他的輸出y=b1+w1×Xcp。<br>
<br>
0:56:58.000,0:57:00.000<br>
那他的輸出y=b1+w1×Xcp。<br>
<br>
0:57:00.000,0:57:02.000<br>
那他的輸出y=b1+w1×Xcp。<br>
<br>
0:57:02.000,0:57:04.000<br>
那如果它是獨角蟲的話，<br>
<br>
0:57:06.000,0:57:08.000<br>
y=b2+w2×Xcp。<br>
<br>
0:57:08.000,0:57:10.000<br>
如果它是綠毛蟲的話，<br>
<br>
0:57:10.000,0:57:12.000<br>
就是b3+w3×Xcp。<br>
<br>
0:57:12.000,0:57:14.000<br>
如果它是伊布的話，<br>
<br>
0:57:14.000,0:57:15.600<br>
就用另外一個式子。<br>
<br>
0:57:16.000,0:57:18.000<br>
也就是不同的物種，我們就看它是哪一個物種，<br>
<br>
0:57:20.000,0:57:20.740<br>
我們就代不同的linear function，<br>
<br>
0:57:20.740,0:57:22.000<br>
我們就代不同的linear function，<br>
<br>
0:57:22.000,0:57:24.000<br>
然後得到不同的y作為最終的輸出。<br>
<br>
0:57:24.000,0:57:26.000<br>
然後得到不同的y作為最終的輸出。<br>
<br>
0:57:26.000,0:57:28.000<br>
你可能會問一個問題說，<br>
<br>
0:57:30.000,0:57:32.000<br>
你把if 放到整個function裡面，<br>
<br>
0:57:32.000,0:57:34.000<br>
這樣你不就不是一個linear model了嗎?<br>
<br>
0:57:36.000,0:57:38.000<br>
function裡面有if 你搞得定嗎?<br>
<br>
0:57:38.000,0:57:40.000<br>
你可以用微分來做嗎?<br>
<br>
0:57:40.000,0:57:42.000<br>
你可以用剛才的gradient descent來算參數對loss function微分嗎?<br>
<br>
0:57:42.000,0:57:44.000<br>
你可以用剛才的gradient descent來算參數對loss function微分嗎?<br>
<br>
0:57:44.000,0:57:46.000<br>
其實是可以的。<br>
<br>
0:57:46.000,0:57:48.000<br>
這個式子你可以把它改寫成一個linear function。<br>
<br>
0:57:50.000,0:57:52.000<br>
寫起來就是這樣 :<br>
<br>
0:57:54.000,0:57:56.000<br>
這個有一點複雜但沒有關係。<br>
<br>
0:57:56.000,0:57:58.000<br>
我們先來觀察一下δ這個function。<br>
<br>
0:57:58.000,0:58:00.000<br>
如果你有修過信號的處理，我想應該知道δ這個function是指甚麼。<br>
<br>
0:58:02.000,0:58:04.000<br>
今天這個δ這個function的意思是說，<br>
<br>
0:58:04.000,0:58:06.000<br>
δ of Xs等於比比鳥的意思就是說，<br>
<br>
0:58:06.000,0:58:08.000<br>
δ of Xs等於比比鳥的意思就是說，<br>
<br>
0:58:10.000,0:58:12.000<br>
假如我們今天輸入的這隻寶可夢是比比鳥的話，<br>
<br>
0:58:12.000,0:58:14.000<br>
假如我們今天輸入的這隻寶可夢是比比鳥的話，<br>
<br>
0:58:14.000,0:58:16.000<br>
這個δ function它的output就是1。<br>
<br>
0:58:16.000,0:58:18.000<br>
這個δ function它的output就是1。<br>
<br>
0:58:18.000,0:58:20.000<br>
反之如果是其他種類的寶可夢的話，<br>
<br>
0:58:20.000,0:58:22.000<br>
它δ function的output就是0。<br>
<br>
0:58:24.000,0:58:26.000<br>
所以我們可以把剛才那個有 if的式子，<br>
<br>
0:58:26.000,0:58:28.000<br>
寫成像這邊這個樣子。<br>
<br>
0:58:28.000,0:58:30.000<br>
寫成像這邊這個樣子。<br>
<br>
0:58:30.000,0:58:30.740<br>
你的寶可夢進化後的CP值，<br>
<br>
0:58:32.000,0:58:34.000<br>
等於b1×δ(比比鳥)這樣子，<br>
<br>
0:58:34.000,0:58:36.000<br>
等於b1×δ(比比鳥)這樣子，<br>
<br>
0:58:36.000,0:58:38.000<br>
等於b1×δ(比比鳥)這樣子，<br>
<br>
0:58:38.000,0:58:40.000<br>
等於b1×δ(比比鳥)這樣子，<br>
<br>
0:58:40.000,0:58:42.000<br>
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，<br>
<br>
0:58:42.000,0:58:44.000<br>
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，<br>
<br>
0:58:44.000,0:58:46.000<br>
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，<br>
<br>
0:58:46.000,0:58:48.000<br>
加上b2×δ(獨角蟲)，<br>
<br>
0:58:48.000,0:58:50.000<br>
加上b2×δ(獨角蟲)，<br>
<br>
0:58:50.000,0:58:52.000<br>
加上w2×δ(獨角蟲)，<br>
<br>
0:58:52.000,0:58:54.000<br>
再乘上它的CP值，<br>
<br>
0:58:54.000,0:58:56.000<br>
然後接下來考慮綠毛蟲，<br>
<br>
0:58:58.000,0:59:00.000<br>
然後接下來考慮伊布。<br>
<br>
0:59:02.000,0:59:03.780<br>
你可能會想說這個跟剛剛那個式子哪裡一樣了呢?<br>
<br>
0:59:06.000,0:59:08.000<br>
你想想看，假如我們今天輸入的那一隻神奇寶貝，<br>
<br>
0:59:10.000,0:59:12.000<br>
假如我們今天輸入的那一隻寶可夢，<br>
<br>
0:59:12.000,0:59:14.000<br>
是比比鳥的話，<br>
<br>
0:59:14.000,0:59:16.000<br>
假如Xs等於比比鳥的話，<br>
<br>
0:59:18.000,0:59:20.000<br>
意味著這兩個function會是1，<br>
<br>
0:59:22.000,0:59:24.000<br>
這兩個δ function如果input是比比鳥的話就是1，<br>
<br>
0:59:24.000,0:59:26.000<br>
其他δ function就是0。<br>
<br>
0:59:26.000,0:59:28.760<br>
其他δ function就是0。<br>
<br>
0:59:28.760,0:59:30.000<br>
那乘上0的項，<br>
<br>
0:59:32.000,0:59:34.000<br>
乘上0的項就當作沒看到，<br>
<br>
0:59:36.000,0:59:40.000<br>
其實就變成y=b1+w1×Xcp<br>
<br>
0:59:42.000,0:59:44.000<br>
所以對其他種類的寶可夢來說也是一樣。<br>
<br>
0:59:46.000,0:59:48.000<br>
所以當我們設計這個function的時候，<br>
<br>
0:59:50.000,0:59:52.000<br>
我們就可以做到我們剛才在前一頁design的那一個有if的function。<br>
<br>
0:59:52.000,0:59:54.000<br>
我們就可以做到我們剛才在前一頁design的那一個有if的function。<br>
<br>
0:59:54.000,0:59:54.500<br>
那事實上這一個function，<br>
<br>
0:59:54.500,0:59:56.000<br>
它就是一個linear function。<br>
<br>
0:59:56.000,0:59:58.000<br>
它就是一個linear function。<br>
<br>
0:59:58.000,1:00:00.000<br>
這個function就是一個linear function。<br>
<br>
1:00:00.000,1:00:02.000<br>
怎麼說呢?<br>
<br>
1:00:02.000,1:00:04.000<br>
前面這個b1 w1到b4 w4就是我們的參數，<br>
<br>
1:00:04.000,1:00:06.000<br>
前面這個b1 w1到b4 w4就是我們的參數，<br>
<br>
1:00:06.000,1:00:08.000<br>
而後面這一項δ或者是δ乘以Xcp<br>
<br>
1:00:08.000,1:00:10.000<br>
而後面這一項δ或者是δ乘以Xcp<br>
<br>
1:00:10.000,1:00:12.000<br>
而後面這一項δ或者是δ乘以Xcp<br>
<br>
1:00:12.000,1:00:14.000<br>
不同的δ，<br>
<br>
1:00:14.000,1:00:16.000<br>
跟不同的δ乘以Xcp，<br>
<br>
1:00:16.000,1:00:18.000<br>
就是後面這個Xi這一項feature。<br>
<br>
1:00:18.000,1:00:20.000<br>
就是後面這個Xi這一項feature。<br>
<br>
1:00:20.000,1:00:22.000<br>
這個藍色框框裡面的這些，<br>
<br>
1:00:22.000,1:00:24.000<br>
其實就是feature。<br>
<br>
1:00:24.000,1:00:26.000<br>
所以這個東西它也是linear model。<br>
<br>
1:00:26.000,1:00:28.000<br>
所以這個東西它也是linear model。<br>
<br>
1:00:28.000,1:00:30.000<br>
那有了這些以後，<br>
<br>
1:00:30.000,1:00:32.000<br>
我們做出來的結果怎麼樣呢?<br>
<br>
1:00:36.000,1:00:38.000<br>
這個是在training data 上的結果，<br>
<br>
1:00:40.000,1:00:42.000<br>
在training data 上面，<br>
<br>
1:00:42.000,1:00:44.000<br>
我們知道不同種類的寶可夢，<br>
<br>
1:00:44.000,1:00:46.000<br>
它用的參數就不一樣。<br>
<br>
1:00:48.000,1:00:50.000<br>
所以不同種類的寶可夢，<br>
<br>
1:00:54.000,1:00:56.000<br>
它的線是不一樣的，<br>
<br>
1:00:58.000,1:01:00.000<br>
它的model的那條line是不一樣的。<br>
<br>
1:01:02.000,1:01:04.000<br>
藍色這條線是比比鳥的線，<br>
<br>
1:01:04.000,1:01:06.000<br>
綠色這條線是綠毛蟲的線，<br>
<br>
1:01:08.000,1:01:10.000<br>
黃色獨角蟲的線跟綠毛蟲的線其實是重疊的，<br>
<br>
1:01:10.000,1:01:12.000<br>
黃色獨角蟲的線跟綠毛蟲的線其實是重疊的，<br>
<br>
1:01:12.000,1:01:14.000<br>
紅色這條線是伊布的線。<br>
<br>
1:01:14.000,1:01:16.000<br>
所以就發現說，<br>
<br>
1:01:16.000,1:01:18.000<br>
當我們分不同種類的寶可夢來考慮的時候，<br>
<br>
1:01:20.000,1:01:22.000<br>
我們的model在training data上面可以得到更低的error。<br>
<br>
1:01:22.000,1:01:24.000<br>
我們的model在training data上面可以得到更低的error。<br>
<br>
1:01:24.000,1:01:26.000<br>
你發現說現在這幾條線，<br>
<br>
1:01:26.000,1:01:28.000<br>
是把training data fit得更好，<br>
<br>
1:01:28.000,1:01:30.000<br>
是把training data 解釋得更好，<br>
<br>
1:01:30.000,1:01:32.000<br>
如果說我們這麼做有考慮到寶可夢的種類的時候，<br>
<br>
1:01:32.000,1:01:34.000<br>
如果說我們這麼做有考慮到寶可夢的種類的時候，<br>
<br>
1:01:36.000,1:01:38.000<br>
我們得到的average error是3.8，在training data上。<br>
<br>
1:01:38.000,1:01:40.000<br>
但我們真正在意的是，<br>
<br>
1:01:40.000,1:01:42.000<br>
它能不能夠預測新看到的寶可夢，<br>
<br>
1:01:44.000,1:01:46.000<br>
也就是testing data上面的結果。<br>
<br>
1:01:46.000,1:01:46.820<br>
那在testing data上面，<br>
<br>
1:01:46.820,1:01:48.000<br>
它的結果是這個樣子：<br>
<br>
1:01:50.000,1:01:52.000<br>
一樣是這三條線，<br>
<br>
1:01:54.000,1:01:56.000<br>
發現說它也把在testing data上面的那些寶可夢fit得很好。<br>
<br>
1:01:56.000,1:01:58.000<br>
發現說它也把在testing data上面的那些寶可夢fit得很好。<br>
<br>
1:01:58.000,1:02:00.000<br>
然後它的average error是14.3<br>
<br>
1:02:00.000,1:02:02.000<br>
然後它的average error是14.3<br>
<br>
1:02:02.000,1:02:04.000<br>
這比我們剛才可以做好的18點多還要更好。<br>
<br>
1:02:04.000,1:02:06.000<br>
這比我們剛才可以做好的18點多還要更好。<br>
<br>
1:02:06.000,1:02:08.000<br>
但是如果你再觀察這個圖的話，<br>
<br>
1:02:10.000,1:02:12.000<br>
感覺應該是還有一些東西是沒有做好的。<br>
<br>
1:02:12.000,1:02:14.000<br>
感覺應該是還有一些東西是沒有做好的。<br>
<br>
1:02:14.000,1:02:16.000<br>
感覺應該是還有一些東西是沒有做好的。<br>
<br>
1:02:18.000,1:02:20.000<br>
我仔細想想看，我覺得伊布這邊應該就沒救了。<br>
<br>
1:02:20.000,1:02:22.000<br>
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。<br>
<br>
1:02:22.000,1:02:24.000<br>
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。<br>
<br>
1:02:24.000,1:02:26.000<br>
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。<br>
<br>
1:02:26.000,1:02:28.000<br>
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。<br>
<br>
1:02:28.000,1:02:30.000<br>
所以如果你沒有考慮這個factor的話，應該就沒救了。<br>
<br>
1:02:34.000,1:02:36.000<br>
但是我覺得這邊有一些還沒有fit很好的地方，<br>
<br>
1:02:38.000,1:02:40.000<br>
有一些值還是略高或略低於這條直線。<br>
<br>
1:02:42.000,1:02:44.000<br>
所以這個地方搞不好還是有辦法解釋的。<br>
<br>
1:02:44.000,1:02:46.000<br>
當然有一個可能是，<br>
<br>
1:02:46.000,1:02:48.000<br>
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化<br>
<br>
1:02:48.000,1:02:50.000<br>
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化<br>
<br>
1:02:50.000,1:02:52.000<br>
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化<br>
<br>
1:02:54.000,1:02:56.000<br>
這個difference其實來自於random的數值，<br>
<br>
1:02:58.000,1:03:00.000<br>
就是每次那個寶可夢的程式產生進化後的CP值的時候，<br>
<br>
1:03:00.000,1:03:02.000<br>
就是每次那個寶可夢的程式產生進化後的CP值的時候，<br>
<br>
1:03:02.000,1:03:04.000<br>
它其實有加一個random的參數。<br>
<br>
1:03:04.000,1:03:06.000<br>
但也有可能是其實不是random的參數，<br>
<br>
1:03:06.000,1:03:08.000<br>
它還有其他的東西在影響著寶可夢進化後的CP值。<br>
<br>
1:03:08.000,1:03:10.000<br>
它還有其他的東西在影響著寶可夢進化後的CP值。<br>
<br>
1:03:10.000,1:03:12.000<br>
它還有其他的東西在影響著寶可夢進化後的CP值。<br>
<br>
1:03:12.000,1:03:14.000<br>
它還有其他的東西在影響著寶可夢進化後的CP值。<br>
<br>
1:03:14.000,1:03:16.000<br>
有什麼其他可能的參數呢?<br>
<br>
1:03:16.000,1:03:18.000<br>
比如說，<br>
<br>
1:03:20.000,1:03:22.000<br>
會不會進化後的CP值是跟weight有關係的?<br>
<br>
1:03:22.000,1:03:24.000<br>
會不會進化後的CP值是跟weight有關係的?<br>
<br>
1:03:28.000,1:03:30.000<br>
會不會進化後的CP值是跟它的高度有關係的?<br>
<br>
1:03:30.000,1:03:32.000<br>
會不會進化後的CP值是跟它的高度有關係的?<br>
<br>
1:03:34.000,1:03:36.000<br>
會不會進化後的CP值是跟它的HP有關係的?<br>
<br>
1:03:36.000,1:03:38.000<br>
會不會進化後的CP值是跟它的HP有關係的?<br>
<br>
1:03:38.000,1:03:40.000<br>
其實我們不知道，<br>
<br>
1:03:40.000,1:03:42.000<br>
我又不是大木博士我怎麼會知道這些事情，<br>
<br>
1:03:44.000,1:03:46.000<br>
所以如果你有domain knowledge的話，<br>
<br>
1:03:46.000,1:03:48.000<br>
你就可能可以知道說<br>
<br>
1:03:48.000,1:03:50.000<br>
你應該把甚麼樣的東西<br>
<br>
1:03:50.000,1:03:52.000<br>
加到你的model裡面去。<br>
<br>
1:03:52.000,1:03:54.000<br>
但是我又沒有domain knowledge，<br>
<br>
1:03:54.000,1:03:56.000<br>
那怎麼辦呢?<br>
<br>
1:03:56.000,1:03:58.000<br>
沒關係，有一招就是把你所有想到的東西，<br>
<br>
1:03:58.000,1:04:00.000<br>
通通塞進去，<br>
<br>
1:04:00.000,1:04:02.000<br>
我們來弄一個最複雜的function，然後看看會怎樣?<br>
<br>
1:04:02.000,1:04:04.000<br>
這個function我寫成這樣 :<br>
<br>
1:04:04.000,1:04:06.000<br>
如果它是比比鳥的話，<br>
<br>
1:04:08.000,1:04:10.000<br>
它的CP值我們就先計算一個y'，<br>
<br>
1:04:10.000,1:04:12.000<br>
它的CP值我們就先計算一個y'，<br>
<br>
1:04:12.000,1:04:14.000<br>
這個y'不是最後這個y，<br>
<br>
1:04:14.000,1:04:16.000<br>
這個y'還要做別的處理才能夠變成y。<br>
<br>
1:04:20.000,1:04:22.000<br>
我們就說，如果這個是比比鳥的話，<br>
<br>
1:04:22.000,1:04:24.000<br>
這其實不是比比鳥，這應該是波波，<br>
<br>
1:04:24.000,1:04:26.000<br>
因為比比鳥是進化後的。<br>
<br>
1:04:26.000,1:04:28.000<br>
這隻應該是波波。<br>
<br>
1:04:28.000,1:04:30.000<br>
那y'=b1+w1×Xcp+W5×(Xcp)²<br>
<br>
1:04:30.000,1:04:32.000<br>
那y'=b1+w1×Xcp+W5×(Xcp)²<br>
<br>
1:04:32.000,1:04:34.000<br>
那y'=b1+w1×Xcp+W5×(Xcp)²<br>
<br>
1:04:34.000,1:04:36.000<br>
我們就是不只要考慮CP值，<br>
<br>
1:04:36.000,1:04:38.000<br>
也要考慮CP值的平方。<br>
<br>
1:04:40.000,1:04:42.000<br>
如果是綠毛蟲，用另外一個式子。<br>
<br>
1:04:42.000,1:04:44.000<br>
如果是獨角蟲，用另外一個式子。<br>
<br>
1:04:44.000,1:04:46.000<br>
如果是伊布，用另外一個式子。<br>
<br>
1:04:48.000,1:04:50.000<br>
最好我們再把y'做其他的處理，<br>
<br>
1:04:50.000,1:04:52.000<br>
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10<br>
<br>
1:04:52.000,1:04:54.000<br>
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10<br>
<br>
1:04:54.000,1:04:56.000<br>
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10<br>
<br>
1:04:56.000,1:04:58.000<br>
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10<br>
<br>
1:05:00.000,1:05:02.000<br>
再加上高度乘上w11，<br>
<br>
1:05:02.000,1:05:04.000<br>
再加上高度的平方乘上w12，<br>
<br>
1:05:04.000,1:05:06.000<br>
再加上它的weight乘上w13，<br>
<br>
1:05:06.000,1:05:08.000<br>
再加上weight平方乘上w14，<br>
<br>
1:05:08.000,1:05:10.000<br>
這些東西合起來，<br>
<br>
1:05:10.000,1:05:12.000<br>
才是最後output的y。<br>
<br>
1:05:12.000,1:05:14.000<br>
所以這整個式子裡面，<br>
<br>
1:05:14.000,1:05:16.000<br>
其實也沒有很多個參數，<br>
<br>
1:05:16.000,1:05:18.000<br>
就是14+4=18<br>
<br>
1:05:18.000,1:05:20.000<br>
跟你們作業比起來，幾百個參數比起來，<br>
<br>
1:05:20.000,1:05:22.000<br>
其實也不是一個太複雜的model。<br>
<br>
1:05:24.000,1:05:26.000<br>
那我們現在有個這麼複雜的function，<br>
<br>
1:05:26.000,1:05:28.000<br>
在training data上我們得到的error，<br>
<br>
1:05:28.000,1:05:30.000<br>
期望應該就是非常的低。<br>
<br>
1:05:32.000,1:05:34.000<br>
我們果然得到一個非常低的error，<br>
<br>
1:05:34.000,1:05:36.000<br>
這個function你可以把它寫成線性的式子，<br>
<br>
1:05:36.000,1:05:38.000<br>
就跟剛才一樣，<br>
<br>
1:05:38.000,1:05:40.000<br>
這邊我就不解釋了。<br>
<br>
1:05:40.000,1:05:42.000<br>
那這麼一個複雜的function，<br>
<br>
1:05:44.000,1:05:46.000<br>
理論上我們可以得到非常低的training error。<br>
<br>
1:05:46.000,1:05:48.000<br>
training error算出來是1.9，<br>
<br>
1:05:48.000,1:05:50.000<br>
那你可以期待你在testing set上，<br>
<br>
1:05:52.000,1:05:54.000<br>
也算出很低的training error嗎?<br>
<br>
1:05:54.000,1:05:56.000<br>
倒是不見得。這麼複雜的model，<br>
<br>
1:05:56.000,1:05:58.000<br>
很以可能會overfitting。<br>
<br>
1:05:58.000,1:05:59.060<br>
你很有可能會得到，<br>
<br>
1:06:00.000,1:06:02.000<br>
在testing data上得到很糟的數字。<br>
<br>
1:06:02.000,1:06:06.000<br>
我們今天得到的數值很糟是102.3這樣子，<br>
<br>
1:06:06.000,1:06:08.000<br>
結果壞掉了<br>
<br>
1:06:08.000,1:06:10.000<br>
怎麼辦呢?<br>
<br>
1:06:10.000,1:06:12.000<br>
如果你是大木博士的話，<br>
<br>
1:06:14.000,1:06:16.000<br>
你就可以刪掉一些你覺得沒有用的input，<br>
<br>
1:06:16.000,1:06:18.000<br>
你就可以刪掉一些你覺得沒有用的input，<br>
<br>
1:06:18.000,1:06:20.000<br>
然後就得到一個簡單的model，<br>
<br>
1:06:20.000,1:06:22.000<br>
避免overfitting的情形。<br>
<br>
1:06:22.000,1:06:24.000<br>
但是我不是大木博士，<br>
<br>
1:06:24.000,1:06:26.000<br>
所以我有用別的方法來處理這個問題。<br>
<br>
1:06:28.000,1:06:30.000<br>
這招叫做regularization。<br>
<br>
1:06:30.000,1:06:32.000<br>
regularization要做的事情是，<br>
<br>
1:06:32.000,1:06:34.000<br>
我們重新定義了step 2的時候，<br>
<br>
1:06:34.000,1:06:36.000<br>
我們重新定義了step 2的時候，<br>
<br>
1:06:36.000,1:06:38.000<br>
我們對一個function是好還是壞的定義。<br>
<br>
1:06:38.000,1:06:40.000<br>
我們對一個function是好還是壞的定義。<br>
<br>
1:06:40.000,1:06:42.000<br>
我們重新redefine我們的loss function。<br>
<br>
1:06:42.000,1:06:44.000<br>
我們重新redefine我們的loss function。<br>
<br>
1:06:44.000,1:06:46.000<br>
然後，我們重新redefine我們的loss function，<br>
<br>
1:06:48.000,1:06:50.000<br>
把一些knowledge放進去，<br>
<br>
1:06:50.000,1:06:52.000<br>
讓我們可以找到比較好的function。<br>
<br>
1:06:52.000,1:06:54.000<br>
什麼意思呢?<br>
<br>
1:06:54.000,1:06:56.000<br>
假設我們的model in general寫成這樣 :<br>
<br>
1:06:56.000,1:06:58.000<br>
y=b+∑WiXi<br>
<br>
1:06:58.000,1:07:00.000<br>
我們原來的loss function，<br>
<br>
1:07:00.000,1:07:02.000<br>
我們原來的loss function，<br>
<br>
1:07:02.000,1:07:04.000<br>
它只考慮了error這件事。<br>
<br>
1:07:04.000,1:07:06.000<br>
原來的loss function只考慮了prediction的結果減掉正確答案的平方，<br>
<br>
1:07:06.000,1:07:08.000<br>
原來的loss function只考慮了prediction的結果減掉正確答案的平方，<br>
<br>
1:07:08.000,1:07:10.000<br>
原來的loss function只考慮了prediction的結果減掉正確答案的平方，<br>
<br>
1:07:10.000,1:07:12.000<br>
只考慮了prediction的error。<br>
<br>
1:07:12.000,1:07:14.000<br>
那regularization它就是加上一項額外的term，<br>
<br>
1:07:14.000,1:07:16.000<br>
那regularization它就是加上一項額外的term，<br>
<br>
1:07:16.000,1:07:18.000<br>
這一項額外的term是λ∑(Wi)²，<br>
<br>
1:07:18.000,1:07:20.000<br>
這一項額外的term是λ∑(Wi)²，<br>
<br>
1:07:20.000,1:07:22.000<br>
這一項額外的term是λ∑(Wi)²，<br>
<br>
1:07:22.000,1:07:24.000<br>
這一項額外的term是λ∑(Wi)²，<br>
<br>
1:07:24.000,1:07:26.000<br>
λ是一個常數，<br>
<br>
1:07:26.000,1:07:28.000<br>
這個是等一下我們要手調一下看要設多少。<br>
<br>
1:07:28.000,1:07:30.000<br>
那∑(Wi)²就是把這個model裡面所有的Wi，<br>
<br>
1:07:30.000,1:07:32.000<br>
那∑(Wi)²就是把這個model裡面所有的Wi，<br>
<br>
1:07:32.000,1:07:34.000<br>
那∑(Wi)²就是把這個model裡面所有的Wi，<br>
<br>
1:07:34.000,1:07:36.000<br>
都算一下平方以後加起來。<br>
<br>
1:07:36.000,1:07:38.000<br>
都算一下平方以後加起來。<br>
<br>
1:07:38.000,1:07:40.000<br>
那這個合起來才是我們的loss function。<br>
<br>
1:07:40.000,1:07:42.000<br>
那這個合起來才是我們的loss function。<br>
<br>
1:07:42.000,1:07:44.000<br>
前面這一項我們剛才解釋過，<br>
<br>
1:07:44.000,1:07:46.000<br>
所以我相信你是可以理解的。<br>
<br>
1:07:46.000,1:07:48.000<br>
error越小就代表當然是越好的function，<br>
<br>
1:07:48.000,1:07:50.000<br>
error越小就代表當然是越好的function，<br>
<br>
1:07:50.000,1:07:52.000<br>
但是，<br>
<br>
1:07:52.000,1:07:54.000<br>
為甚麼我們期待一個參數的值越小，<br>
<br>
1:07:54.000,1:07:56.000<br>
為甚麼我們期待一個參數的值越小，<br>
<br>
1:07:56.000,1:07:58.000<br>
為甚麼我們期待一個參數的值越小，<br>
<br>
1:07:58.000,1:08:00.000<br>
參數的值越接近0的function呢?<br>
<br>
1:08:02.000,1:08:04.000<br>
這件事情你就比較難想像。<br>
<br>
1:08:04.000,1:08:06.000<br>
為甚麼我們期待一個參數值接近0的function呢?<br>
<br>
1:08:08.000,1:08:10.000<br>
當我們加上這一項的時候，<br>
<br>
1:08:10.000,1:08:12.000<br>
我們就是預期說我們要找到的那個function，<br>
<br>
1:08:12.000,1:08:14.000<br>
它的那個參數越小越好。<br>
<br>
1:08:14.000,1:08:16.000<br>
它的那個參數越小越好。<br>
<br>
1:08:18.000,1:08:20.000<br>
當我們加上這一項的時候，<br>
<br>
1:08:22.000,1:08:24.000<br>
你知道參數值比較接近0的function，<br>
<br>
1:08:26.000,1:08:28.000<br>
它是比較平滑的。<br>
<br>
1:08:28.000,1:08:30.000<br>
所謂的比較平滑的意思是，<br>
<br>
1:08:30.000,1:08:32.000<br>
當今天的輸入有變化的時候，<br>
<br>
1:08:34.000,1:08:36.000<br>
output對輸入的變化是比較不敏感的。<br>
<br>
1:08:36.000,1:08:38.000<br>
output對輸入的變化是比較不敏感的。<br>
<br>
1:08:38.000,1:08:40.000<br>
為甚麼參數小就可以達到這個效果呢?<br>
<br>
1:08:40.000,1:08:40.500<br>
你可以想想看，<br>
<br>
1:08:40.500,1:08:42.000<br>
假設這個是我們的model，<br>
<br>
1:08:46.000,1:08:48.000<br>
現在input有一個變化，<br>
<br>
1:08:48.000,1:08:50.000<br>
比如說我們對某一個Xi加上ΔXi<br>
<br>
1:08:50.000,1:08:52.000<br>
比如說我們對某一個Xi加上ΔXi<br>
<br>
1:08:52.000,1:08:54.000<br>
比如說我們對某一個Xi加上ΔXi<br>
<br>
1:08:54.000,1:08:56.000<br>
比如說我們對某一個Xi加上ΔXi<br>
<br>
1:08:56.000,1:08:58.000<br>
這時候對輸出會有什麼變化呢?<br>
<br>
1:08:58.000,1:09:00.000<br>
這時候輸出的變化，<br>
<br>
1:09:00.000,1:09:02.000<br>
就是ΔXi乘上Wi<br>
<br>
1:09:02.000,1:09:04.000<br>
就是ΔXi乘上Wi<br>
<br>
1:09:04.000,1:09:06.000<br>
你的輸入變化ΔXi<br>
<br>
1:09:06.000,1:09:14.000<br>
輸出就是Wi乘上ΔXi<br>
<br>
1:09:14.500,1:09:16.000<br>
你會發現說如果今天你的Wi越小越接近0的話，<br>
<br>
1:09:16.000,1:09:18.000<br>
如果你的Wi越接近0的話，<br>
<br>
1:09:20.000,1:09:22.000<br>
它的變化就越小。<br>
<br>
1:09:22.000,1:09:24.000<br>
如果Wi越接近0的話，<br>
<br>
1:09:26.000,1:09:28.000<br>
輸出對輸入就越不sensitive。<br>
<br>
1:09:28.000,1:09:30.000<br>
輸出對輸入就越不sensitive。<br>
<br>
1:09:30.000,1:09:32.000<br>
所以今天Wi越接近0，<br>
<br>
1:09:32.000,1:09:34.000<br>
我們的function就是一個越平滑的function。<br>
<br>
1:09:34.000,1:09:36.000<br>
我們的function就是一個越平滑的function。<br>
<br>
1:09:36.000,1:09:38.000<br>
現在的問題就是，<br>
<br>
1:09:40.000,1:09:42.000<br>
為甚麼我們喜歡比較平滑的function?<br>
<br>
1:09:42.000,1:09:44.000<br>
這可以有不同的解釋，你可以這樣想，<br>
<br>
1:09:46.000,1:09:48.000<br>
如果我們今天有一個比較平滑的function的話，<br>
<br>
1:09:48.000,1:09:50.000<br>
如果我們今天有一個比較平滑的function的話，<br>
<br>
1:09:52.000,1:09:54.000<br>
那平滑的function對輸入是比較不敏感的。<br>
<br>
1:09:54.000,1:09:56.000<br>
所以今天如果我們的輸入，<br>
<br>
1:09:56.000,1:09:58.000<br>
被一些雜訊所干擾的話，<br>
<br>
1:09:58.000,1:10:00.000<br>
如果今天雜訊干擾的我們的輸入，<br>
<br>
1:10:00.000,1:10:02.000<br>
在我們測試的時候，<br>
<br>
1:10:02.000,1:10:04.000<br>
那一個比較平滑的function，<br>
<br>
1:10:04.000,1:10:05.080<br>
它會受到比較少的影響，<br>
<br>
1:10:08.000,1:10:10.000<br>
而給我們一個比較好的結果。<br>
<br>
1:10:10.000,1:10:12.000<br>
接下來我們就要來看看說，<br>
<br>
1:10:14.000,1:10:16.000<br>
如果我們加入了regularization的項，<br>
<br>
1:10:18.000,1:10:20.000<br>
對我們最終的結果會有甚麼樣的影響?<br>
<br>
1:10:22.000,1:10:24.000<br>
這個是實驗的結果。<br>
<br>
1:10:24.000,1:10:26.000<br>
我們就把λ從0、1、10一直調到100000<br>
<br>
1:10:26.000,1:10:28.000<br>
我們就把λ從0、1、10一直調到100000<br>
<br>
1:10:28.000,1:10:30.000<br>
我們就把λ從0、1、10一直調到100000<br>
<br>
1:10:30.000,1:10:32.000<br>
我們就把λ從0、1、10一直調到100000<br>
<br>
1:10:32.000,1:10:34.000<br>
所以λ值越大，<br>
<br>
1:10:34.000,1:10:36.000<br>
代表說今天我們的<br>
<br>
1:10:36.000,1:10:38.000<br>
代表說今天我們的<br>
<br>
1:10:38.000,1:10:40.000<br>
我們現在loss有兩項，<br>
<br>
1:10:40.000,1:10:42.000<br>
一項是考慮error，一項是考慮多smooth<br>
<br>
1:10:42.000,1:10:44.000<br>
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大<br>
<br>
1:10:44.000,1:10:46.000<br>
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大<br>
<br>
1:10:46.000,1:10:48.000<br>
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大<br>
<br>
1:10:48.000,1:10:50.000<br>
所以當λ值越大的時候，<br>
<br>
1:10:50.000,1:10:52.000<br>
我們找到的function就越平滑。<br>
<br>
1:10:54.000,1:10:56.000<br>
如果我們看看在training data上的error的話，<br>
<br>
1:10:56.000,1:10:58.000<br>
我們看看在training data上的error的話，<br>
<br>
1:10:58.000,1:11:00.000<br>
我們會發現說，<br>
<br>
1:11:00.000,1:11:02.000<br>
如果function越平滑，<br>
<br>
1:11:02.000,1:11:04.000<br>
我們在training data上得到的error其實是越大的。<br>
<br>
1:11:04.000,1:11:06.000<br>
我們在training data上得到的error其實是越大的。<br>
<br>
1:11:06.000,1:11:08.000<br>
但是這件事情是非常合理的，<br>
<br>
1:11:08.000,1:11:10.000<br>
因為當λ越大的時候，<br>
<br>
1:11:10.000,1:11:12.000<br>
我們就越傾向於考慮W本來的值，<br>
<br>
1:11:12.000,1:11:14.000<br>
我們就越傾向於考慮W本來的值，<br>
<br>
1:11:14.000,1:11:16.000<br>
我們就越傾向於考慮W本來的值，<br>
<br>
1:11:16.000,1:11:18.000<br>
我們就傾向考慮W的值而減少考慮我們的error。<br>
<br>
1:11:18.000,1:11:20.000<br>
我們就傾向考慮W的值而減少考慮我們的error。<br>
<br>
1:11:20.000,1:11:22.000<br>
所以今天如果λ越大的時候，<br>
<br>
1:11:22.000,1:11:24.000<br>
我們考慮error就愈少，<br>
<br>
1:11:24.000,1:11:26.000<br>
所以我們本來在training data上得到的error就越大。<br>
<br>
1:11:26.000,1:11:28.000<br>
所以我們本來在training data上得到的error就越大。<br>
<br>
1:11:28.000,1:11:30.000<br>
但是有趣的是，<br>
<br>
1:11:30.000,1:11:32.000<br>
雖然在training data上得到的error就越大，<br>
<br>
1:11:32.000,1:11:34.000<br>
但是在testing data上面得到的error可能是會比較小。<br>
<br>
1:11:34.000,1:11:36.000<br>
但是在testing data上面得到的error可能是會比較小。<br>
<br>
1:11:36.000,1:11:38.000<br>
比如說我們看這邊的例子，<br>
<br>
1:11:38.000,1:11:40.000<br>
原來λ=0，<br>
<br>
1:11:40.000,1:11:42.000<br>
就是沒有regularization的時候error是102<br>
<br>
1:11:42.000,1:11:44.000<br>
λ=1就變成68，<br>
<br>
1:11:44.000,1:11:46.000<br>
到10就變成25<br>
<br>
1:11:46.000,1:11:48.000<br>
到100就變成11.1<br>
<br>
1:11:48.000,1:11:50.000<br>
但是λ太大的時候，<br>
<br>
1:11:50.000,1:11:52.000<br>
到1000的時候，<br>
<br>
1:11:52.000,1:11:54.000<br>
error又變大變成12.8一直到26.8。<br>
<br>
1:11:54.000,1:11:56.000<br>
error又變大變成12.8一直到26.8。<br>
<br>
1:11:56.000,1:11:58.000<br>
那這個結果是合理，<br>
<br>
1:12:00.000,1:12:02.000<br>
我們比較喜歡比較平滑的function，<br>
<br>
1:12:02.000,1:12:04.000<br>
比較平滑的function它對noise比較不sensitive，<br>
<br>
1:12:04.000,1:12:06.000<br>
所以當我們增加λ的時候，<br>
<br>
1:12:06.000,1:12:08.000<br>
你的performance是越來越好，<br>
<br>
1:12:10.000,1:12:12.000<br>
但是我們又不喜歡太平滑的function，<br>
<br>
1:12:14.000,1:12:16.000<br>
因為最平滑的function是甚麼?<br>
<br>
1:12:16.000,1:12:18.000<br>
最平滑的function就是一條水平線啊，<br>
<br>
1:12:18.000,1:12:20.000<br>
一條水平線是最平滑的function。<br>
<br>
1:12:20.000,1:12:22.000<br>
如果你的function是一條水平線的話，<br>
<br>
1:12:22.000,1:12:24.000<br>
那它啥事都幹不成，<br>
<br>
1:12:24.000,1:12:26.000<br>
所以如果今天function太平滑的話，<br>
<br>
1:12:26.000,1:12:28.000<br>
你反而會在testing set上又得到糟糕的結果。<br>
<br>
1:12:28.000,1:12:30.000<br>
你反而會在testing set上又得到糟糕的結果。<br>
<br>
1:12:32.000,1:12:34.000<br>
所以現在的問題就是，<br>
<br>
1:12:36.000,1:12:38.000<br>
我們希望我們的model多smooth呢?<br>
<br>
1:12:38.000,1:12:40.000<br>
我們希望我們的model多smooth呢?<br>
<br>
1:12:42.000,1:12:44.000<br>
我們希望我們今天找到的function有多平滑呢?<br>
<br>
1:12:46.000,1:12:48.000<br>
這件事情就變成是你們要調λ來解決這件事情，<br>
<br>
1:12:48.000,1:12:50.000<br>
這件事情就變成是你們要調λ來解決這件事情，<br>
<br>
1:12:50.000,1:12:52.000<br>
你必須要調整λ來決定你的function的平滑程度。<br>
<br>
1:12:52.000,1:12:54.000<br>
你必須要調整λ來決定你的function的平滑程度。<br>
<br>
1:12:54.000,1:12:56.000<br>
比如說你可能調整一下參數以後發現說，<br>
<br>
1:12:58.000,1:13:00.000<br>
今天training都隨著λ增加而增加，<br>
<br>
1:13:00.000,1:13:02.000<br>
testing隨著λ先減少後增加。<br>
<br>
1:13:02.000,1:13:04.000<br>
testing隨著λ先減少後增加。<br>
<br>
1:13:04.000,1:13:06.000<br>
在這個地方有一個轉折點，<br>
<br>
1:13:06.000,1:13:08.000<br>
是可以讓我們的testing error最小，<br>
<br>
1:13:08.000,1:13:10.000<br>
是可以讓我們的testing error最小，<br>
<br>
1:13:10.000,1:13:12.000<br>
你就選λ=100來得到你的model。<br>
<br>
1:13:12.000,1:13:14.000<br>
你就選λ=100來得到你的model。<br>
<br>
1:13:14.000,1:13:16.000<br>
你就選λ=100來得到你的model。<br>
<br>
1:13:16.000,1:13:18.000<br>
這邊還有一個有趣的事實，<br>
<br>
1:13:18.000,1:13:20.000<br>
很多同學其實都知道regularization，<br>
<br>
1:13:22.000,1:13:22.660<br>
你有沒有發現，<br>
<br>
1:13:26.000,1:13:28.000<br>
這邊我沒有把b加進去，<br>
<br>
1:13:28.000,1:13:30.000<br>
我剛剛突然想到一件事情，<br>
<br>
1:13:30.000,1:13:32.000<br>
我其實在前面那個gradient descent的投影片裡面，<br>
<br>
1:13:32.000,1:13:34.000<br>
有一個地方寫錯了，<br>
<br>
1:13:34.000,1:13:36.000<br>
然後有同學提醒我，<br>
<br>
1:13:36.000,1:13:38.000<br>
以後你如果有發現我投影片有寫錯的話，<br>
<br>
1:13:40.000,1:13:42.000<br>
以後告訴我就把你的投影片寫在投影片這樣。<br>
<br>
1:13:46.000,1:13:48.000<br>
這邊你發現我沒有加上b，<br>
<br>
1:13:48.000,1:13:50.000<br>
為甚麼呢?<br>
<br>
1:13:50.000,1:13:52.000<br>
你覺得是我寫錯了的同學，<br>
<br>
1:13:52.000,1:13:54.000<br>
你覺得是我忘了加上去的同學舉手一下，<br>
<br>
1:13:58.000,1:14:00.000<br>
你覺得本來就不需要加上b的同學舉手一下<br>
<br>
1:14:02.000,1:14:04.000<br>
這邊我覺得我沒有寫錯。<br>
<br>
1:14:04.000,1:14:06.000<br>
事實上很多人可能不知道這件事，<br>
<br>
1:14:06.000,1:14:08.000<br>
在做regularization的時候，<br>
<br>
1:14:10.000,1:14:12.000<br>
其實是不需要考慮bias這一項的。<br>
<br>
1:14:12.000,1:14:14.000<br>
首先，如果你自己做實驗的話你會發現，<br>
<br>
1:14:14.000,1:14:16.000<br>
不考慮bias， performance會比較好。<br>
<br>
1:14:16.000,1:14:18.000<br>
再來為甚麼不考慮bias呢?<br>
<br>
1:14:18.000,1:14:20.000<br>
因為我們今天預期的是，<br>
<br>
1:14:22.000,1:14:24.000<br>
我們要找一個比較平滑的function。<br>
<br>
1:14:26.000,1:14:28.000<br>
你調整bias的這個b的大小，<br>
<br>
1:14:28.000,1:14:30.000<br>
跟一個function的平滑的程度是沒有關係的。<br>
<br>
1:14:30.000,1:14:32.000<br>
跟一個function的平滑的程度是沒有關係的。<br>
<br>
1:14:32.000,1:14:34.000<br>
調整bias值的大小時你只是把function上下移動而已，<br>
<br>
1:14:34.000,1:14:36.000<br>
調整bias值的大小時你只是把function上下移動而已，<br>
<br>
1:14:36.000,1:14:38.000<br>
對function的平滑程度是沒有關係的。<br>
<br>
1:14:38.000,1:14:40.000<br>
所以有趣的是，這很多人都不知道，<br>
<br>
1:14:40.000,1:14:42.000<br>
在做regularization的時候，<br>
<br>
1:14:42.000,1:14:44.000<br>
你是不用考慮bias的。<br>
<br>
1:14:46.000,1:14:48.000<br>
總之，<br>
<br>
1:14:48.000,1:14:50.000<br>
搞了半天以後，<br>
<br>
1:14:50.000,1:14:52.000<br>
我最後可以做到，<br>
<br>
1:14:52.000,1:14:54.000<br>
我們的testing error是11.1。<br>
<br>
1:14:56.000,1:14:58.000<br>
那在我們請助教公告作業之前，<br>
<br>
1:14:58.000,1:15:00.000<br>
我們就說一下今天的conclusion :<br>
<br>
1:15:02.000,1:15:04.000<br>
首先感謝大家來參加我對寶可夢研究的發表會，<br>
<br>
1:15:06.000,1:15:08.000<br>
那我今天得到的結論就是，<br>
<br>
1:15:10.000,1:15:12.000<br>
寶可夢進化後的CP值，<br>
<br>
1:15:12.000,1:15:14.000<br>
跟他進化前的CP值，<br>
<br>
1:15:14.000,1:15:16.000<br>
還有它是哪個物種，是非常有關係的。<br>
<br>
1:15:18.000,1:15:20.000<br>
知道這兩件事情幾乎可以決定進化後的CP值。<br>
<br>
1:15:20.000,1:15:20.780<br>
但是我認為，<br>
<br>
1:15:22.000,1:15:24.000<br>
可能應該還有其他的factors。<br>
<br>
1:15:24.000,1:15:26.000<br>
我們剛剛看到說我們加上其他甚麼高度啊體重啊HP以後，<br>
<br>
1:15:26.000,1:15:28.000<br>
我們剛剛看到說我們加上其他甚麼高度啊體重啊HP以後，<br>
<br>
1:15:28.000,1:15:30.000<br>
是有比較好的，如果我們加入regularization的話。<br>
<br>
1:15:30.000,1:15:32.000<br>
不過我data有點少，<br>
<br>
1:15:32.000,1:15:34.000<br>
所以我沒有那麼confident就是了。<br>
<br>
1:15:38.000,1:15:40.000<br>
然後再來呢就是，<br>
<br>
1:15:42.000,1:15:44.000<br>
我們今天講了gradient descent的作法，<br>
<br>
1:15:44.000,1:15:46.000<br>
就是告訴大家怎麼做<br>
<br>
1:15:48.000,1:15:50.000<br>
那我們以後會講它的原理還有技巧。<br>
<br>
1:15:56.000,1:15:58.000<br>
我們今天講了overfitting和regularization，<br>
<br>
1:15:58.000,1:16:00.000<br>
我們今天講了overfitting和regularization，<br>
<br>
1:16:00.000,1:16:02.000<br>
介紹一下表象上的現象，<br>
<br>
1:16:02.000,1:16:04.000<br>
未來會講更多它背後的理論。<br>
<br>
1:16:04.000,1:16:06.000<br>
再來最後我們有一個很重要的問題，<br>
<br>
1:16:06.000,1:16:08.000<br>
首先我覺得我這個結果應該還滿正確的，<br>
<br>
1:16:08.000,1:16:10.000<br>
因為你知道網路上有很多的CP的預測器，<br>
<br>
1:16:10.000,1:16:12.000<br>
因為你知道網路上有很多的CP的預測器，<br>
<br>
1:16:12.000,1:16:14.000<br>
那些CP的預測器你在輸入的時候，<br>
<br>
1:16:16.000,1:16:18.000<br>
你只要輸入你的寶可夢的物種和它現在的CP值，<br>
<br>
1:16:18.000,1:16:20.000<br>
你只要輸入你的寶可夢的物種和它現在的CP值，<br>
<br>
1:16:20.000,1:16:22.000<br>
它就可以告訴你進化以後的CP值。<br>
<br>
1:16:22.000,1:16:24.000<br>
它就可以告訴你進化以後的CP值。<br>
<br>
1:16:24.000,1:16:26.000<br>
所以我認為你要預測進化以後的CP值，<br>
<br>
1:16:26.000,1:16:28.000<br>
應該是要知道原來的CP值和它的物種，<br>
<br>
1:16:28.000,1:16:30.000<br>
就可以知道大部分。<br>
<br>
1:16:32.000,1:16:34.000<br>
不過我看那些預測器預測出來的誤差，<br>
<br>
1:16:34.000,1:16:36.000<br>
都是給你一個range，<br>
<br>
1:16:36.000,1:16:38.000<br>
它都沒有辦法給你一個更準確的預測。<br>
<br>
1:16:38.000,1:16:40.000<br>
如果考慮更多的factor更多的input，<br>
<br>
1:16:40.000,1:16:42.000<br>
比如說HP甚麼啊，<br>
<br>
1:16:42.000,1:16:44.000<br>
或許可以預測的更準就是了。<br>
<br>
1:16:44.000,1:16:46.000<br>
但最後的問題就是，<br>
<br>
1:16:46.000,1:16:48.000<br>
我們在testing data上面，<br>
<br>
1:16:48.000,1:16:50.000<br>
在我們testing的10隻寶可夢上，<br>
<br>
1:16:50.000,1:16:52.000<br>
我們得到的average error最後是11.1。<br>
<br>
1:16:52.000,1:16:54.000<br>
如果我把它做成一個系統，<br>
<br>
1:16:54.000,1:16:56.000<br>
放到網路上給大家使用的話，<br>
<br>
1:16:58.000,1:17:00.000<br>
你覺得如果我們看過沒有看到的data，<br>
<br>
1:17:04.000,1:17:06.000<br>
那我們得到的error會預期高過11.1還是低於11.1<br>
<br>
1:17:06.000,1:17:08.000<br>
那我們得到的error會預期高過11.1還是低於11.1<br>
<br>
1:17:10.000,1:17:12.000<br>
還是理論上期望值應該是一樣的。<br>
<br>
1:17:14.000,1:17:16.000<br>
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，<br>
<br>
1:17:16.000,1:17:18.000<br>
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，<br>
<br>
1:17:18.000,1:17:20.000<br>
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，<br>
<br>
1:17:20.000,1:17:22.000<br>
我們就假設使用者只能夠輸入那四種，<br>
<br>
1:17:22.000,1:17:24.000<br>
它不會輸入乘龍卡比這樣。<br>
<br>
1:17:24.000,1:17:26.000<br>
在這個情況下，<br>
<br>
1:17:28.000,1:17:30.000<br>
你覺得如果我們今天把這個系統放到線上，<br>
<br>
1:17:30.000,1:17:32.000<br>
給大家使用的話，<br>
<br>
1:17:32.000,1:17:34.000<br>
我們今天得到的CP值，<br>
<br>
1:17:36.000,1:17:38.000<br>
會比我今天在testing set上看到的高還是低還是一樣?<br>
<br>
1:17:40.000,1:17:42.000<br>
你覺得一樣的同學舉手一下<br>
<br>
1:17:44.000,1:17:46.000<br>
你覺得會比現在看到的低，舉手一下<br>
<br>
1:17:54.000,1:17:56.000<br>
你覺得會比我們今天看到的11.1還要高的舉手一下<br>
<br>
1:18:00.000,1:18:02.000<br>
我們之後會解釋，<br>
<br>
1:18:06.000,1:18:08.000<br>
我們今天其實用了testing set來選model<br>
<br>
1:18:08.000,1:18:10.000<br>
就我們今天得到的結果其實是<br>
<br>
1:18:10.000,1:18:12.000<br>
如果我們真的把系統放在線上的話<br>
<br>
1:18:14.000,1:18:16.000<br>
預期應該會得到比我們今天看到的11.1還要更高的error rate<br>
<br>
1:18:18.000,1:18:20.000<br>
這個時候我們需要validation觀念來解決這個問題<br>
<br>
1:18:22.000,1:18:24.000<br>
這個我們就下一堂課再講。<br>
<br>
1:18:24.000,1:18:26.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
1:18:26.000,1:18:28.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
1:18:28.000,1:18:30.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
