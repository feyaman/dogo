<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>ML-7</title></head>
<body><h1>ML Lecture 7: Backpropagation</h1>
<blockquote><p>臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 <a href='http://ai.ntu.edu.tw/'>http://ai.ntu.edu.tw</a></p>
</blockquote>
<h3>Gradient Descent</h3>
<ul>
<li>Backpropagation 就是 Gradient Descent，但 Backpropagation 是較有效率的演算法</li>
<li>詳見 ML 3-1 Gradient Descent 複習（<a href='https://youtu.be/yKKNr-QKz2Q' target='_blank' class='url'>https://youtu.be/yKKNr-QKz2Q</a>）</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_1.jpg" width="70%"></p>
<h3>Chain Rule</h3>
<ul>
<li><p><strong>Case 1</strong></p>
<blockquote><p>有兩個 function，<strong>y = g(x)</strong> 、 <strong>z = h(y)</strong>
=&gt; (output) x 的微小變化，會影響到 z 
=&gt; (微分) dz/dx = dz/dy * dy/dx </p>
</blockquote>
</li>
<li><p>Case 2</p>
<blockquote><p>有三個 function，<strong>x = g(s)</strong> 、 <strong>y = h(s)</strong>、 <strong>z = k(x,y)</strong></p>
<p>=&gt; (output) 改變 s，影響到 x 跟 y，最後影響 z</p>
<p>=&gt; (微分可拆成兩項) \frac{dz}{ds} = \frac{∂z}{∂x} \frac{dx}{ds} + \frac{∂z}{∂y} \frac{dy}{ds}</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_2.jpg" width="70%"></p>
<h2>Backpropagation</h2>
<h3>Loss function</h3>
<ul>
<li>Loss 來源：
給定一組 Neural Network 的參數 θ，將 training data, <strong>x<sup>n</sup></strong> 代到 Neural Network 裡面，會 output <strong>y<sup>n</sup></strong>，
而 <strong>y<sup>n</sup></strong> 和我們希望 output 的 <strong>\hat{y}^n</strong> 之間距離的 function 即為 <strong>C<sup>n</sup></strong></li>
<li>Total Loss：
(計算)  L(θ) = \sum\limits_{n=1}^N l^n(θ)
(微分) \frac{∂L(θ)}{∂w} = \sum\limits_{n=1}^N\frac{∂l^n(θ)}{∂w}</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_3.jpg" width="70%"></p>
<hr />
<h4>簡化問題：\frac{∂l}{∂w} = \frac{∂z}{∂w}\frac{∂l}{∂z}  </h4>
<p>=&gt; 某一筆 data 的 cost, <strong>l</strong> 對參數 w 的偏微分</p>
<p>(因為算出 \frac{∂l}{∂w} 後，再 \sum 所有 training data，就可以把L(θ) 對某一參數的偏微分算出來了)</p>
<h2><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_4.jpg" width="70%"></h2>
<h3>Forward Pass：計算 \frac{∂z}{∂w} 的過程</h3>
<ul>
<li><p>規律：z 對參數 w 偏微分的值，即為 w 前面所接 input 的值</p>
<blockquote><p>Ex：∂z / ∂w_1= x_1 、∂z / ∂w_2= x_2 </p>
</blockquote>
</li>
<li><p>結論：
要計算每個 weight, w 對 activation functon 的 input, z 的偏微分，
丟進 input，計算每個 neuron 的 output，就結束了。</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_5.jpg" width="70%"></p>
<hr />
<h3>Backward pass：計算 \frac{∂l}{∂z} 的過程</h3>
<p>假設：此處的 activation function 是 sigmoid function</p>
<p>\frac{∂l}{∂z} 可以拆成 \frac{∂a}{∂z}\times\frac{∂l}{∂a} </p>
<ol start='' >
<li>前項 (\frac{∂a}{∂z}) 就是 sigmoid function 的微分，σ\prime(z)，下圖中藍線所表示</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_7.jpg" width="70%"></p>
<ol start='2' >
<li><p>後項 (\frac{∂l}{∂a}) 可再拆成 \frac{∂z\prime}{∂a}\frac{∂l}{∂z\prime} + \frac{∂z"}{∂a}\frac{∂l}{∂z"} (假設 a 之後只有兩個 neuron 的情況)</p>
<p>\frac{∂z\prime}{∂a} 和 \frac{∂z"}{∂a} 即為 w_3 和 w_4，\frac{∂l}{∂z\prime} 和 \frac{∂l}{∂z"} 假設已知</p>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_8.jpg" width="70%"></p>
<ol start='3' >
<li><p>故可推得 \frac{∂l}{∂z} = σ\prime(z) [w_3\frac{∂l}{∂z\prime} + w_4\frac{∂l}{∂z"}]</p>
</li>
<li><p>求出 \frac{∂l}{∂z\prime} 及 \frac{∂l}{∂z"}，有兩種 case</p>
<ul>
<li><p><strong>Case 1. Output Layer</strong>：
紅色 neuron 後，即為 output layer (整個 neural network 的 output)</p>
<blockquote><p>\frac{∂l}{∂z\prime} = \frac{∂y_1}{∂z\prime} \frac{∂l}{∂y_1}</p>
<p>\frac{∂l}{∂z"} = \frac{∂y_2}{∂z} \frac{∂l}{∂y_2}</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_11.jpg" width="70%"></p>
<ul>
<li><p><strong>Case 2. Not Output Layer</strong>：
繼續往下層 layer 走，走到 output layer，再從 output layer 反算回來</p>
<blockquote><p>如下圖，從 z_5、z_6 的的偏微分算回 z_3、z_4 到 z_1、z_2</p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_16.jpg" width="70%"></p>
</li>

</ul>
</li>

</ol>
<hr />
<h3>Summarize</h3>
<ol start='' >
<li>Forward Pass，算出每個 activation function 的 output (即為它所連接的 weight 的 \frac{∂z}{∂w})</li>
<li>Backward Pass，將原來的 neural network 反向(如下圖)，每個三角形的 output 就是 \frac{∂l}{∂z}</li>
<li>最後，兩者相乘即得 \frac{∂l}{∂w}</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Backpropagation/Backpropagation_17.jpg" width="70%"></p>
</body>
</html>